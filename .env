PYTHONPATH="/home/hskim/work/llm-agent"

# LLM (text) server
MODEL_LLM=openai/gpt-oss-20b
LLM_PORT=8003
VLLM_CUDA_DEVICES=0,1
VLLM_CUDA_DEVICE=0
VLLM_CUDA_DEVICE1=1
VLLM_TP_SIZE=2
VLLM_MAX_MODEL_LEN=32768
VLLM_GPU_MEM_UTIL=0.6
VLLM_SHM_SIZE=8g
VLLM_TIMEOUT=600

# VLM (vision) server (optional, profile with-vlm)
MODEL_VLM=Qwen/Qwen3-VL-30B-A3B-Instruct
VLM_PORT=8004
VLM_CUDA_DEVICES=2
VLM_CUDA_DEVICE=2
VLM_TP_SIZE=1
VLM_MAX_MODEL_LEN=32768
VLM_GPU_MEM_UTIL=0.8
VLM_SHM_SIZE=8g

ES_DATA_PATH=/home/llm-share/es_data

API_PORT=8001
ES_PORT=8002
TEI_PORT=8080

# LLM Client settings (for backend code)
# Docker network: use service name + internal port (not host port)
# vllm service listens on internal port 8000
VLLM_BASE_URL=http://vllm:8000
VLLM_MODEL_NAME=${MODEL_LLM}
VLLM_TIMEOUT=600
VLLM_MAX_MODEL_LEN=32768

# VLM Client settings (for vision model)
# Docker network: vlm service listens on internal port 8000
VLM_CLIENT_BASE_URL=http://vlm:8000/v1
VLM_CLIENT_MODEL=${MODEL_VLM}
VLM_CLIENT_TIMEOUT=600
VLM_CLIENT_MAX_TOKENS=2048
VLM_CLIENT_TEMPERATURE=0.0

# Elasticsearch settings
# Docker network: use service name + internal port (9200)
SEARCH_ES_HOST=http://elasticsearch:9200
SEARCH_ES_ENV=dev
SEARCH_ES_INDEX_PREFIX=rag_chunks
SEARCH_ES_EMBEDDING_DIMS=768
SEARCH_BACKEND=es
