# =============================================================================
# RAG System Docker Compose
# Local development / PoC environment
# Services: FastAPI + Elasticsearch + vLLM + Ingestion volumes
# =============================================================================

services:
  # ---------------------------------------------------------------------------
  # FastAPI Backend API
  # ---------------------------------------------------------------------------
  api:
    container_name: rag-api
    build:
      context: .
      dockerfile: backend/Dockerfile
    env_file:
      - .env
    environment:
      - HF_HOME=/data/hf_cache
      - TRANSFORMERS_CACHE=/data/hf_cache
      # Inside the docker network, ES is reachable via service name + internal port.
      # (Using localhost here would point to the api container itself.)
      - SEARCH_ES_HOST=http://elasticsearch:9200

    volumes:
      # Code hot-reload (development): mount backend source into /app/backend
      - ./backend:/app/backend
      # Ingestion data (original pdf, pages, parsed.json)
      - ./data/ingestions:/data/ingestions
      # Vector store persistence (local-only; can be moved later)
      - ./data/vector_store:/data/vector_store
      # HuggingFace model cache (shared host cache)
      - /home/llm-share/hf:/data/hf_cache
    ports:
      - "${API_PORT}:8000"
    depends_on:
      elasticsearch:
        condition: service_healthy
      # vllm dependency removed - vllm is optional (with-llm profile)
    networks:
      - rag_net
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # Elasticsearch (Vector + BM25 search)
  # ---------------------------------------------------------------------------
  elasticsearch:
    container_name: rag-elasticsearch
    build:
      context: ./backend/elasticsearch
      dockerfile: Dockerfile
    environment:
      - discovery.type=single-node
      - ES_JAVA_OPTS=-Xms2g -Xmx2g
      - xpack.security.enabled=false
      - xpack.security.enrollment.enabled=false
      # Nori plugin (Korean analyzer) installed via custom Dockerfile
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      # ES data persistence (host path or named volume via ES_DATA_PATH env)
      - ${ES_DATA_PATH:?ES_DATA_PATH is required}:/usr/share/elasticsearch/data
    ports:
      - "${ES_PORT}:9200"
    networks:
      - rag_net
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:9200/_cluster/health | grep -q '\"status\":\"green\"\\|\"status\":\"yellow\"'"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # vLLM (OpenAI compatible LLM server)
  # ---------------------------------------------------------------------------
  vllm:
    container_name: rag-vllm
    image: vllm/vllm-openai:latest
    command: >
      --model ${MODEL_LLM:-${VLLM_MODEL_NAME:-gpt-oss-20b}}
      --port 8000
      --host 0.0.0.0
      --trust-remote-code
      --tensor-parallel-size ${VLLM_TP_SIZE:-2}
      --max-model-len ${VLLM_MAX_MODEL_LEN:-32768}
      --gpu-memory-utilization ${VLLM_GPU_MEM_UTIL:-0.9}
      --no-enable-prefix-caching
    environment:
      - HF_HOME=/data/hf_cache
      - HUGGINGFACE_HUB_TOKEN=${HUGGINGFACE_HUB_TOKEN:-}
      - CUDA_VISIBLE_DEVICES=${VLLM_CUDA_DEVICES:-0,1}
    volumes:
      - /home/llm-share/hf:/data/hf_cache
      - ./data/models:/data/models
    ports:
      - "${LLM_PORT:-${VLLM_PORT:-8003}}:8000"
    networks:
      - rag_net
    # GPU configuration (NVIDIA GPU)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["${VLLM_CUDA_DEVICE:-0}", "${VLLM_CUDA_DEVICE1:-1}"]
              capabilities: [gpu]
    shm_size: ${VLLM_SHM_SIZE:-8g}
    restart: unless-stopped
    profiles:
      - with-llm  # Only start with: docker compose --profile with-llm up

  # ---------------------------------------------------------------------------
  # VLM (Vision/Multimodal) vLLM server - Optional
  # ---------------------------------------------------------------------------
  vlm:
    container_name: rag-vlm
    image: vllm/vllm-openai:latest
    command: >
      --model ${MODEL_VLM:-${VLM_MODEL_NAME:-gpt-oss-20b}}
      --port 8000
      --host 0.0.0.0
      --trust-remote-code
      --tensor-parallel-size ${VLM_TP_SIZE:-2}
      --max-model-len ${VLM_MAX_MODEL_LEN:-32768}
      --gpu-memory-utilization ${VLM_GPU_MEM_UTIL:-0.9}
    environment:
      - HF_HOME=/data/hf_cache
      - HUGGINGFACE_HUB_TOKEN=${HUGGINGFACE_HUB_TOKEN:-}
      - CUDA_VISIBLE_DEVICES=${VLM_CUDA_DEVICES:-0}
    volumes:
      - /home/llm-share/hf:/data/hf_cache
      - ./data/models:/data/models
    ports:
      - "${VLM_PORT:-8004}:8000"
    networks:
      - rag_net
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["${VLM_CUDA_DEVICE:-0}", "${VLM_CUDA_DEVICE1:-1}"]
              capabilities: [gpu]
    shm_size: ${VLM_SHM_SIZE:-8g}
    restart: unless-stopped
    profiles:
      - with-vlm  # Only start with: docker compose --profile with-vlm up

  # ---------------------------------------------------------------------------
  # TEI (Text Embeddings Inference) - Optional
  # ---------------------------------------------------------------------------
  tei:
    container_name: rag-tei
    image: ghcr.io/huggingface/text-embeddings-inference:${TEI_VERSION:-cpu-1.8}
    command: >
      --model-id ${TEI_MODEL_ID:-intfloat/multilingual-e5-large}
      --port 80
      --max-concurrent-requests ${TEI_MAX_CONCURRENT:-512}
    environment:
      - HF_HOME=/data/hf_cache
      - HUGGINGFACE_HUB_TOKEN=${HUGGINGFACE_HUB_TOKEN:-}
    volumes:
      - /home/llm-share/hf:/data/hf_cache
    networks:
      - rag_net
    restart: unless-stopped
    profiles:
      - with-tei  # Only start with: docker compose --profile with-tei up

  # ---------------------------------------------------------------------------
  # MinIO (S3-compatible object storage for document images)
  # ---------------------------------------------------------------------------
  minio:
    container_name: rag-minio
    image: minio/minio:latest
    command: server /data --console-address ":9001"
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
    volumes:
      - ./data/minio:/data
    networks:
      - rag_net
    # Expose for local script access
    ports:
      - "127.0.0.1:9000:9000"
      - "127.0.0.1:9001:9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped

  minio-init:
    container_name: rag-minio-init
    image: minio/mc
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      /usr/bin/mc alias set local http://minio:9000 $${MINIO_ROOT_USER} $${MINIO_ROOT_PASSWORD} &&
      /usr/bin/mc mb -p local/$${MINIO_BUCKET} || true
      "
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
      - MINIO_BUCKET=${MINIO_BUCKET}
    networks:
      - rag_net

  # ---------------------------------------------------------------------------
  # Frontend (Vite dev server)
  # ---------------------------------------------------------------------------
  frontend:
    container_name: rag-frontend
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        # Empty VITE_API_BASE to use nginx proxy (/api/ -> api:8000)
        VITE_API_BASE: ""
        VITE_CHAT_PATH: ${VITE_CHAT_PATH:-/api/agent/run}
        VITE_INGESTIONS_BASE: ${VITE_INGESTIONS_BASE:-/data/ingestions}
        VITE_DEFAULT_INGESTION_RUN: ${VITE_DEFAULT_INGESTION_RUN:-}
    env_file:
      - .env
    volumes:
      # Ingestion data (shared with backend)
      - ./data/ingestions:/data/ingestions
    ports:
      - "${FRONTEND_PORT:-9097}:9097"
    depends_on:
      api:
        condition: service_started
    networks:
      - rag_net
    restart: unless-stopped

# =============================================================================
# Networks & Volumes
# =============================================================================

networks:
  rag_net:
    driver: bridge

volumes:
  es_data:
    driver: local
