# =============================================================================
# RAG System Docker Compose
# Local development / PoC environment
# Services: FastAPI + Elasticsearch + vLLM + Ingestion volumes
# =============================================================================

services:
  # ---------------------------------------------------------------------------
  # FastAPI Backend API
  # ---------------------------------------------------------------------------
  api:
    container_name: rag-api
    build:
      context: .
      dockerfile: backend/Dockerfile
    env_file:
      - .env
    environment:
      - HF_HOME=/data/hf_cache
      - TRANSFORMERS_CACHE=/data/hf_cache

    volumes:
      # Code hot-reload (development): mount backend source into /app/backend
      - ./backend:/app/backend
      # Ingestion data (original pdf, pages, parsed.json)
      - /home/llm-share/ingestions:/data/ingestions
      # Vector store persistence (local-only; can be moved later)
      - ./data/vector_store:/data/vector_store
      # HuggingFace model cache (shared host cache)
      - /home/llm-share/hf:/data/hf_cache
    ports:
      - "${API_PORT}:8000"
    depends_on:
      elasticsearch:
        condition: service_healthy
      # vllm dependency removed - vllm is optional (with-llm profile)
    networks:
      - rag_net
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # Elasticsearch (Vector + BM25 search)
  # ---------------------------------------------------------------------------
  elasticsearch:
    container_name: rag-elasticsearch
    image: docker.elastic.co/elasticsearch/elasticsearch:8.14.0
    environment:
      - discovery.type=single-node
      - ES_JAVA_OPTS=-Xms2g -Xmx2g
      - xpack.security.enabled=false
      - xpack.security.enrollment.enabled=false
      # For Korean analysis, consider building custom image with nori plugin
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      # ES data persistence (host path or named volume via ES_DATA_PATH env)
      - ${ES_DATA_PATH:?ES_DATA_PATH is required}:/usr/share/elasticsearch/data
    ports:
      - "${ES_PORT}:9200"
    networks:
      - rag_net
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:9200/_cluster/health | grep -q '\"status\":\"green\"\\|\"status\":\"yellow\"'"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # vLLM (OpenAI compatible LLM server)
  # ---------------------------------------------------------------------------
  vllm:
    container_name: rag-vllm
    image: vllm/vllm-openai:latest
    command: >
      --model ${MODEL_LLM:-${VLLM_MODEL_NAME:-gpt-oss-20b}}
      --port 8000
      --host 0.0.0.0
      --trust-remote-code
      --tensor-parallel-size ${VLLM_TP_SIZE:-2}
      --max-model-len ${VLLM_MAX_MODEL_LEN:-32768}
      --gpu-memory-utilization ${VLLM_GPU_MEM_UTIL:-0.9}
    environment:
      - HF_HOME=/data/hf_cache
      - HUGGINGFACE_HUB_TOKEN=${HUGGINGFACE_HUB_TOKEN:-}
      - CUDA_VISIBLE_DEVICES=${VLLM_CUDA_DEVICES:-0,1}
    volumes:
      - /home/llm-share/hf:/data/hf_cache
      - ./data/models:/data/models
    ports:
      - "${LLM_PORT:-${VLLM_PORT:-8003}}:8000"
    networks:
      - rag_net
    # GPU configuration (NVIDIA GPU)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["${VLLM_CUDA_DEVICE:-0}", "${VLLM_CUDA_DEVICE1:-1}"]
              capabilities: [gpu]
    shm_size: ${VLLM_SHM_SIZE:-8g}
    restart: unless-stopped
    profiles:
      - with-llm  # Only start with: docker compose --profile with-llm up

  # ---------------------------------------------------------------------------
  # VLM (Vision/Multimodal) vLLM server - Optional
  # ---------------------------------------------------------------------------
  vlm:
    container_name: rag-vlm
    image: vllm/vllm-openai:latest
    command: >
      --model ${MODEL_VLM:-${VLM_MODEL_NAME:-gpt-oss-20b}}
      --port 8000
      --host 0.0.0.0
      --trust-remote-code
      --tensor-parallel-size ${VLM_TP_SIZE:-2}
      --max-model-len ${VLM_MAX_MODEL_LEN:-32768}
      --gpu-memory-utilization ${VLM_GPU_MEM_UTIL:-0.9}
    environment:
      - HF_HOME=/data/hf_cache
      - HUGGINGFACE_HUB_TOKEN=${HUGGINGFACE_HUB_TOKEN:-}
      - CUDA_VISIBLE_DEVICES=${VLM_CUDA_DEVICES:-0}
    volumes:
      - /home/llm-share/hf:/data/hf_cache
      - ./data/models:/data/models
    ports:
      - "${VLM_PORT:-8004}:8000"
    networks:
      - rag_net
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["${VLM_CUDA_DEVICE:-0}"]
              capabilities: [gpu]
    shm_size: ${VLM_SHM_SIZE:-8g}
    restart: unless-stopped
    profiles:
      - with-vlm  # Only start with: docker compose --profile with-vlm up

  # ---------------------------------------------------------------------------
  # TEI (Text Embeddings Inference) - Optional
  # ---------------------------------------------------------------------------
  tei:
    container_name: rag-tei
    image: ghcr.io/huggingface/text-embeddings-inference:${TEI_VERSION:-cpu-1.8}
    command: >
      --model-id ${TEI_MODEL_ID:-intfloat/multilingual-e5-large}
      --port 80
      --max-concurrent-requests ${TEI_MAX_CONCURRENT:-512}
    environment:
      - HF_HOME=/data/hf_cache
      - HUGGINGFACE_HUB_TOKEN=${HUGGINGFACE_HUB_TOKEN:-}
    volumes:
      - /home/llm-share/hf:/data/hf_cache
    networks:
      - rag_net
    restart: unless-stopped
    profiles:
      - with-tei  # Only start with: docker compose --profile with-tei up

  # ---------------------------------------------------------------------------
  # Frontend (Vite dev server)
  # ---------------------------------------------------------------------------
  frontend:
    container_name: rag-frontend
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        # Empty VITE_API_BASE to use nginx proxy (/api/ -> api:8000)
        VITE_API_BASE: ""
        VITE_CHAT_PATH: ${VITE_CHAT_PATH:-/api/agent/run}
        VITE_INGESTIONS_BASE: ${VITE_INGESTIONS_BASE:-/data/ingestions}
        VITE_DEFAULT_INGESTION_RUN: ${VITE_DEFAULT_INGESTION_RUN:-}
    env_file:
      - .env
    volumes:
      # Ingestion data (shared with backend)
      - /home/llm-share/ingestions:/data/ingestions
    ports:
      - "${FRONTEND_PORT:-9097}:9097"
    depends_on:
      api:
        condition: service_started
    networks:
      - rag_net
    restart: unless-stopped

# =============================================================================
# Networks & Volumes
# =============================================================================

networks:
  rag_net:
    driver: bridge

volumes:
  es_data:
    driver: local
