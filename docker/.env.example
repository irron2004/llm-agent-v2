# =============================================================================
# RAG System Environment Configuration
# Copy this file to ../.env and customize for your environment
# =============================================================================

# =============================================================================
# Search Backend Configuration
# =============================================================================

# Backend type: local | es
SEARCH_BACKEND=local

# Local vector store path (for SEARCH_BACKEND=local)
SEARCH_LOCAL_INDEX_PATH=/data/vector_store

# Elasticsearch settings (for SEARCH_BACKEND=es)
SEARCH_ES_HOST=http://elasticsearch:9200
SEARCH_ES_USER=
SEARCH_ES_PASSWORD=

# ES Index Management (SU-2509)
# Index naming: {prefix}_{env}_v{version} (e.g., rag_chunks_dev_v1)
# Alias naming: {prefix}_{env}_current (e.g., rag_chunks_dev_current)
SEARCH_ES_ENV=dev
SEARCH_ES_INDEX_PREFIX=rag_chunks
SEARCH_ES_INDEX_VERSION=1
SEARCH_ES_EMBEDDING_DIMS=1024

# ES Docker settings
ES_PORT=9200
# ES data path (host path or named volume, default: es_data named volume)
ES_DATA_PATH=./data/elasticsearch

# =============================================================================
# RAG Pipeline Configuration
# =============================================================================

# Preprocessing
RAG_PREPROCESS_METHOD=normalize
RAG_PREPROCESS_VERSION=v1
RAG_PREPROCESS_LEVEL=L3

# Embedding
RAG_EMBEDDING_METHOD=koe5
RAG_EMBEDDING_VERSION=v1
RAG_EMBEDDING_DEVICE=cpu

# Retrieval
RAG_RETRIEVAL_METHOD=hybrid
RAG_RETRIEVAL_VERSION=v1
RAG_RETRIEVAL_TOP_K=10

# Hybrid search weights
RAG_HYBRID_DENSE_WEIGHT=0.7
RAG_HYBRID_SPARSE_WEIGHT=0.3
RAG_HYBRID_RRF_K=60

# Multi-query expansion
RAG_MULTI_QUERY_ENABLED=false
RAG_MULTI_QUERY_METHOD=llm
RAG_MULTI_QUERY_N=3
RAG_MULTI_QUERY_INCLUDE_ORIGINAL=true
RAG_MULTI_QUERY_PROMPT=general_mq_v1

# Reranking
RAG_RERANK_ENABLED=false
RAG_RERANK_METHOD=cross_encoder
RAG_RERANK_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2
RAG_RERANK_TOP_K=5

# =============================================================================
# vLLM Configuration
# =============================================================================

VLLM_BASE_URL=http://vllm:8000/v1
VLLM_MODEL_NAME=gpt-oss-20b
VLLM_TEMPERATURE=0.7
VLLM_MAX_TOKENS=2048
VLLM_TIMEOUT=60
VLLM_PORT=8002

# vLLM Server Settings (for Docker container)
VLLM_GPU_COUNT=1
VLLM_SHM_SIZE=8g

# =============================================================================
# TEI (Text Embeddings Inference) Configuration
# =============================================================================

TEI_ENDPOINT_URL=http://tei:80
TEI_TIMEOUT=30
TEI_PORT=8080
TEI_MODEL_ID=intfloat/multilingual-e5-large
TEI_MAX_CONCURRENT=512
TEI_VERSION=cpu-1.8

# =============================================================================
# FastAPI Configuration
# =============================================================================

API_PORT=8001
API_HOST=0.0.0.0
API_LOG_LEVEL=info
API_RELOAD=true

# =============================================================================
# Data Paths
# =============================================================================

INGESTION_ROOT=/data/ingestions

# =============================================================================
# HuggingFace Configuration
# =============================================================================

HUGGINGFACE_HUB_TOKEN=your-hf-token-here
HF_HOME=./data/hf_cache

# =============================================================================
# CUDA Configuration (for GPU environments)
# =============================================================================

CUDA_VISIBLE_DEVICES=0
