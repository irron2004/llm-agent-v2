# Embedding ì½”ë“œ ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ

llm-agent â†’ llm-agent-v2 ì„ë² ë”© ëª¨ë“ˆ ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œì…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

1. [ì „ì²´ êµ¬ì¡° ë¹„êµ](#ì „ì²´-êµ¬ì¡°-ë¹„êµ)
2. [ë§ˆì´ê·¸ë ˆì´ì…˜ ë§¤í•‘](#ë§ˆì´ê·¸ë ˆì´ì…˜-ë§¤í•‘)
3. [ê¶Œì¥ êµ¬ì¡°](#ê¶Œì¥-êµ¬ì¡°)
4. [ë‹¨ê³„ë³„ ë§ˆì´ê·¸ë ˆì´ì…˜](#ë‹¨ê³„ë³„-ë§ˆì´ê·¸ë ˆì´ì…˜)
5. [ì½”ë“œ ì˜ˆì‹œ](#ì½”ë“œ-ì˜ˆì‹œ)

## ğŸ—ï¸ ì „ì²´ êµ¬ì¡° ë¹„êµ

### llm-agent (ì›ë³¸)

```
core/embedding/
â”œâ”€â”€ embedders/
â”‚   â”œâ”€â”€ base.py                 # BaseEmbedder (encode ë©”ì†Œë“œ)
â”‚   â”œâ”€â”€ sentence.py             # SentenceTransformer (ì‹±ê¸€í†¤, L2 ì •ê·œí™”)
â”‚   â”œâ”€â”€ cache.py                # ë””ìŠ¤í¬ ìºì‹± ë˜í¼
â”‚   â””â”€â”€ create_embedder.py      # íŒ©í† ë¦¬ í•¨ìˆ˜
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ device.py               # GPU ìë™ ì„ íƒ (auto, round-robin)
â”‚   â”œâ”€â”€ normalize.py            # L2 ì •ê·œí™”
â”‚   â””â”€â”€ chunking.py             # í…ìŠ¤íŠ¸ ì²­í‚¹
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py             # EmbeddingSettings (Pydantic)
â”œâ”€â”€ adapters/
â”‚   â””â”€â”€ langchain.py            # LangChain ì–´ëŒ‘í„°
â”œâ”€â”€ indexing.py                 # FaissIndex
â””â”€â”€ cli.py                      # CLI ì¸í„°í˜ì´ìŠ¤
```

**íŠ¹ì§•**:
- ì‹±ê¸€í†¤ íŒ¨í„´ìœ¼ë¡œ ëª¨ë¸ ì¬ì‚¬ìš©
- ë””ìŠ¤í¬ ìºì‹± ì§€ì›
- GPU ìë™ ì„ íƒ (ì—¬ìœ  ë©”ëª¨ë¦¬ ê¸°ë°˜, round-robin)
- LangChain í†µí•©
- FAISS ì¸ë±ì‹±

### llm-agent-v2 (í˜„ì¬ - ìŠ¤ì¼ˆë ˆí†¤)

```
backend/llm_infrastructure/embedding/
â”œâ”€â”€ embedders/
â”‚   â”œâ”€â”€ sentence_transformer.py  # BGE, E5 ë“± (ë ˆì§€ìŠ¤íŠ¸ë¦¬ íŒ¨í„´)
â”‚   â””â”€â”€ tei_client.py            # TEI í´ë¼ì´ì–¸íŠ¸
â”œâ”€â”€ base.py                      # BaseEmbedder (embed, embed_batch)
â””â”€â”€ registry.py                  # EmbedderRegistry
```

**íŠ¹ì§•**:
- ë ˆì§€ìŠ¤íŠ¸ë¦¬ íŒ¨í„´ (preprocessingê³¼ ë™ì¼)
- ì—¬ëŸ¬ ëª¨ë¸ ì§€ì› (bge_base, bge_large, multilingual_e5)
- TEI í´ë¼ì´ì–¸íŠ¸ í¬í•¨
- ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì—†ìŒ (ì¶”ê°€ í•„ìš”)

## ğŸ¯ ë§ˆì´ê·¸ë ˆì´ì…˜ ë§¤í•‘

### í•µì‹¬ ëª¨ë“ˆ ë§¤í•‘

| llm-agent | llm-agent-v2 (ëª©í‘œ) | ì„¤ëª… |
|-----------|---------------------|------|
| `embedders/base.py` | `engines/sentence/base.py` | ì—”ì§„ ë‚´ë¶€ìš© BaseEmbedder |
| `embedders/sentence.py` | `engines/sentence/embedder.py` | SentenceTransformer ì—”ì§„ |
| `embedders/cache.py` | `engines/sentence/cache.py` | ë””ìŠ¤í¬ ìºì‹± |
| `embedders/create_embedder.py` | `engines/sentence/factory.py` | íŒ©í† ë¦¬ í•¨ìˆ˜ |
| `utils/device.py` | `engines/sentence/utils.py` | GPU ì„ íƒ ë¡œì§ |
| `utils/normalize.py` | `engines/sentence/utils.py` | L2 ì •ê·œí™” |
| `utils/chunking.py` | `engines/sentence/utils.py` | í…ìŠ¤íŠ¸ ì²­í‚¹ |
| `config/settings.py` | `backend/config/settings.py` | âœ… ì´ë¯¸ ì¡´ì¬ |
| `adapters/langchain.py` | `adapters/langchain.py` | LangChain ì–´ëŒ‘í„° |
| `indexing.py` | `indexing/faiss_index.py` | FAISS ì¸ë±ìŠ¤ (ì„ íƒ) |
| (ì—†ìŒ) | `base.py` | âœ… ë ˆì§€ìŠ¤íŠ¸ë¦¬ìš© BaseEmbedder |
| (ì—†ìŒ) | `adapters/sentence.py` | âœ… ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì–´ëŒ‘í„° |

### ë©”ì†Œë“œ ì¸í„°í˜ì´ìŠ¤ ì°¨ì´

| llm-agent | llm-agent-v2 | ë³€ê²½ í•„ìš” |
|-----------|--------------|-----------|
| `encode(texts)` â†’ ndarray | `embed(text)` â†’ ndarray | âœ… ë‹¨ì¼ í…ìŠ¤íŠ¸ìš© |
| `encode(texts)` â†’ ndarray | `embed_batch(texts)` â†’ ndarray | âœ… ë°°ì¹˜ìš© |
| `encode_query(text)` â†’ ndarray | `embed(text)` â†’ ndarray | âœ… í†µí•©ë¨ |

## ğŸ›ï¸ ê¶Œì¥ êµ¬ì¡°

preprocessingê³¼ ë™ì¼í•˜ê²Œ **ì—”ì§„-ì–´ëŒ‘í„° íŒ¨í„´** ì ìš©:

```
backend/llm_infrastructure/embedding/
â”œâ”€â”€ engines/                  # ğŸ”§ ì—”ì§„: ì‹¤ì œ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ sentence/             # SentenceTransformer ì—”ì§„
â”‚       â”œâ”€â”€ __init__.py       #    â†’ create_embedder ë“± ì¬export
â”‚       â”œâ”€â”€ base.py           #    â†’ BaseEmbedder (ì—”ì§„ìš©)
â”‚       â”œâ”€â”€ embedder.py       #    â†’ SentenceTransformer ë˜í¼
â”‚       â”œâ”€â”€ cache.py          #    â†’ ë””ìŠ¤í¬ ìºì‹±
â”‚       â”œâ”€â”€ utils.py          #    â†’ device, normalize, chunking
â”‚       â””â”€â”€ factory.py        #    â†’ create_embedder íŒ©í† ë¦¬
â”‚
â”œâ”€â”€ adapters/                 # ğŸ”Œ ì–´ëŒ‘í„°: ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì¸í„°í˜ì´ìŠ¤
â”‚   â”œâ”€â”€ __init__.py          #    â†’ ëª¨ë“  ì–´ëŒ‘í„° ì¬export
â”‚   â”œâ”€â”€ sentence.py           #    â†’ SentenceTransformer ì–´ëŒ‘í„°
â”‚   â”œâ”€â”€ tei.py                #    â†’ TEI ì–´ëŒ‘í„°
â”‚   â””â”€â”€ langchain.py          #    â†’ LangChain ì–´ëŒ‘í„° (ì„ íƒ)
â”‚
â”œâ”€â”€ indexing/                 # ğŸ“Š ì¸ë±ì‹± (ì„ íƒì‚¬í•­)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ faiss_index.py        #    â†’ FAISS IVF ë˜í¼
â”‚
â”œâ”€â”€ base.py                   # BaseEmbedder (ë ˆì§€ìŠ¤íŠ¸ë¦¬ìš©)
â””â”€â”€ registry.py               # EmbedderRegistry
```

**ì¤‘ìš”**: `engines/sentence/base.py`ëŠ” ì—”ì§„ ë‚´ë¶€ìš©, `embedding/base.py`ëŠ” ë ˆì§€ìŠ¤íŠ¸ë¦¬ìš©

### ì„¤ê³„ ì›ì¹™

1. **ì—”ì§„ (`*_engine/`)**: ìˆœìˆ˜ ì•Œê³ ë¦¬ì¦˜ ë¡œì§
   - SentenceTransformer ë˜í•‘
   - GPU ìë™ ì„ íƒ
   - ìºì‹±, ì •ê·œí™” ë“± ìœ í‹¸
   - ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì—†ì´ ë…ë¦½ì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥

2. **ì–´ëŒ‘í„° (`adapters/`)**: ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì—°ê²°
   - ì—”ì§„ì„ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ë“±ë¡
   - ì„¤ì • ì£¼ì…
   - ë©”íƒ€ë°ì´í„° ì²˜ë¦¬

3. **ì¸ë±ì‹± (`indexing/`)**: ë²¡í„° ì¸ë±ìŠ¤ (ì„ íƒ)
   - FAISS, Qdrant ë“±
   - ê²€ìƒ‰ì— ì‚¬ìš©

## ğŸ“¦ ë‹¨ê³„ë³„ ë§ˆì´ê·¸ë ˆì´ì…˜

### ğŸ“Œ ì£¼ì˜ì‚¬í•­ (ì‹œì‘ ì „ í•„ë…)

#### 1. ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì •ë¦¬
- âŒ `core.embedding...` â†’ âœ… `backend.llm_infrastructure.embedding...`
- ëª¨ë“  import ê²½ë¡œ ë³€ê²½
- ê° íŒ¨í‚¤ì§€ì— `__init__.py` ì¶”ê°€ í•„ìˆ˜

#### 2. GPU ì„ íƒ ë¡œì§ ì•ˆì •ì„±
- `auto` ì „ëµ: ë©”ëª¨ë¦¬ ì¡°íšŒ ì‹¤íŒ¨ ì‹œ CPU fallback ì¶”ê°€
- Docker/ì»¨í…Œì´ë„ˆ í™˜ê²½: `CUDA_VISIBLE_DEVICES` ê³ ë ¤
- ë©€í‹°í”„ë¡œì„¸ìŠ¤: round-robinì´ í”„ë¡œì„¸ìŠ¤ ê°„ ë™ê¸°í™” í•„ìš”

#### 3. Docker í™˜ê²½ ê³ ë ¤
- ìºì‹œ ê²½ë¡œ: ì“°ê¸° ê°€ëŠ¥í•œ ë³¼ë¥¨ ë§ˆìš´íŠ¸ í•„ìš”
- `.dockerignore`ì— ìºì‹œ ë””ë ‰í† ë¦¬ ì¶”ê°€
- í™˜ê²½ë³€ìˆ˜ë¡œ ëª¨ë“  ê²½ë¡œ ì œì–´ ê°€ëŠ¥í•˜ê²Œ

#### 4. ì˜ì¡´ì„±
```bash
# pyproject.toml or requirements.txt
sentence-transformers>=2.2.0
diskcache>=5.4.0
faiss-cpu>=1.7.0  # ë˜ëŠ” faiss-gpu
langchain>=0.1.0  # ì„ íƒì‚¬í•­
```

#### 5. ê¸°ì¡´ v2 ì„ë² ë”© ì½”ë“œ
- `embedders/sentence_transformer.py` â†’ **ë¬´ì‹œ** (ë®ì–´ì“°ê¸°)
- `embedders/tei_client.py` â†’ `adapters/tei.py`ë¡œ ì´ë™
- `base.py`, `registry.py` â†’ **ìœ ì§€**

---

### Step 1: ì—”ì§„ ìœ í‹¸ë¦¬í‹° ë§ˆì´ê·¸ë ˆì´ì…˜

**íŒŒì¼**: `backend/llm_infrastructure/embedding/engines/sentence/utils.py`

```python
"""Utility functions for SentenceTransformer engine."""

import torch
import numpy as np
from typing import List

# ==================== Device Selection ====================

_gpu_cycle = -1

def pick_device(strategy: str | None = None) -> str:
    """
    GPU ìë™ ì„ íƒ ì „ëµ (Docker/ì»¨í…Œì´ë„ˆ ì•ˆì „).

    Args:
        strategy: None/"auto" (ì—¬ìœ  ë©”ëª¨ë¦¬ ê¸°ì¤€)
                  "round-robin" (ìˆœí™˜)
                  "cuda:X" (ì§ì ‘ ì§€ì •)

    Returns:
        Device string (e.g., "cuda:0", "cpu")
    """
    if strategy is None or strategy == "auto":
        if torch.cuda.is_available():
            try:
                # ê°€ì¥ ì—¬ìœ  ìˆëŠ” GPU ì„ íƒ
                free_mem = [
                    torch.cuda.mem_get_info(i)[0]
                    for i in range(torch.cuda.device_count())
                ]
                best_gpu = int(max(range(len(free_mem)), key=free_mem.__getitem__))
                return f"cuda:{best_gpu}"
            except Exception:
                # ë©”ëª¨ë¦¬ ì¡°íšŒ ì‹¤íŒ¨ ì‹œ ì²« GPU ì‚¬ìš©
                return "cuda:0"
        return "cpu"

    if strategy == "round-robin":
        global _gpu_cycle
        if torch.cuda.is_available():
            _gpu_cycle = (_gpu_cycle + 1) % torch.cuda.device_count()
            return f"cuda:{_gpu_cycle}"
        return "cpu"

    # ì§ì ‘ ì§€ì • (CUDA_VISIBLE_DEVICES ê³ ë ¤)
    return strategy

# ==================== Normalization ====================

def l2_normalize(x: np.ndarray, axis: int = 1) -> np.ndarray:
    """L2 ì •ê·œí™”."""
    return x / np.linalg.norm(x, axis=axis, keepdims=True)

# ==================== Chunking ====================

def split_by_tokens(
    text: str,
    max_tokens: int = 512,
    overlap: int = 50
) -> List[str]:
    """
    í…ìŠ¤íŠ¸ë¥¼ í† í° ê¸¸ì´ ê¸°ì¤€ìœ¼ë¡œ ì²­í‚¹.

    Args:
        text: ì…ë ¥ í…ìŠ¤íŠ¸
        max_tokens: ìµœëŒ€ í† í° ìˆ˜
        overlap: ì˜¤ë²„ë© í† í° ìˆ˜

    Returns:
        ì²­í¬ ë¦¬ìŠ¤íŠ¸
    """
    words = text.split()
    if len(words) <= max_tokens:
        return [text]

    chunks = []
    start = 0
    step = max_tokens - overlap
    while start < len(words):
        end = min(start + max_tokens, len(words))
        chunks.append(" ".join(words[start:end]))
        start += step
    return chunks
```

### Step 2: ì—”ì§„ BaseEmbedder ë§ˆì´ê·¸ë ˆì´ì…˜

**íŒŒì¼**: `backend/llm_infrastructure/embedding/engines/sentence/base.py`

```python
"""Base embedder for SentenceTransformer engine (internal use)."""

from abc import ABC, abstractmethod
from typing import Iterable
import numpy as np

class BaseEmbedder(ABC):
    """
    ì—”ì§„ ë‚´ë¶€ìš© BaseEmbedder (llm-agent í˜¸í™˜).

    ì£¼ì˜: ë ˆì§€ìŠ¤íŠ¸ë¦¬ìš© BaseEmbedder(`embedding/base.py`)ì™€ ë‹¤ë¦„.
    """

    @abstractmethod
    def encode(self, texts: Iterable[str]) -> np.ndarray:
        """
        ë°°ì¹˜ ì„ë² ë”©.

        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì„ë² ë”© í–‰ë ¬ (n_texts, dimension)
        """
        ...

    def encode_query(self, text: str) -> np.ndarray:
        """
        ë‹¨ì¼ ì¿¼ë¦¬ ì„ë² ë”©.

        Args:
            text: ë‹¨ì¼ í…ìŠ¤íŠ¸

        Returns:
            ì„ë² ë”© ë²¡í„° (dimension,)
        """
        return self.encode([text])[0]
```

### Step 3: ìºì‹± ìœ í‹¸ë¦¬í‹° ë§ˆì´ê·¸ë ˆì´ì…˜

**íŒŒì¼**: `backend/llm_infrastructure/embedding/engines/sentence/cache.py`

```python
"""Disk caching for embeddings."""

import hashlib
import numpy as np
from diskcache import Cache
from typing import List

class CachedEmbedder:
    """
    ì„ë² ë”© ê²°ê³¼ë¥¼ ë””ìŠ¤í¬ì— ìºì‹±í•˜ëŠ” ë˜í¼.

    ì‚¬ìš©ë²•:
        embedder = SentenceTransformerEmbedder(...)
        cached = CachedEmbedder(embedder, cache_dir=".embed_cache")
        vecs = cached.encode(texts)
    """

    def __init__(self, inner, cache_dir: str = ".embed_cache"):
        """
        Args:
            inner: BaseEmbedder ì¸ìŠ¤í„´ìŠ¤
            cache_dir: ìºì‹œ ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.inner = inner
        self.cache = Cache(cache_dir)

    def _key(self, text: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„± (ëª¨ë¸ëª… + í…ìŠ¤íŠ¸)."""
        model_name = getattr(self.inner, "model_name", "unknown")
        h = hashlib.sha256()
        h.update((model_name + "::" + text).encode())
        return h.hexdigest()

    def encode(self, texts: List[str]) -> np.ndarray:
        """
        ë°°ì¹˜ ì„ë² ë”© (ìºì‹œ í™œìš©).

        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì„ë² ë”© í–‰ë ¬ (n_texts, dimension)
        """
        vecs = []
        to_compute = []
        indices = []

        # 1) ìºì‹œ í™•ì¸
        for i, t in enumerate(texts):
            key = self._key(t)
            if key in self.cache:
                vecs.append(self.cache[key])
            else:
                to_compute.append(t)
                indices.append(i)
                vecs.append(None)

        # 2) ìºì‹œ ë¯¸ìŠ¤ë§Œ ê³„ì‚°
        if to_compute:
            new_vecs = self.inner.encode(to_compute)
            for i, v in zip(indices, new_vecs):
                key = self._key(to_compute[indices.index(i)])
                self.cache[key] = v
                vecs[i] = v

        return np.vstack(vecs)

    def encode_query(self, text: str) -> np.ndarray:
        """ë‹¨ì¼ ì¿¼ë¦¬ ì„ë² ë”©."""
        return self.encode([text])[0]
```

### Step 4: SentenceTransformer ì—”ì§„ ë§ˆì´ê·¸ë ˆì´ì…˜

**íŒŒì¼**: `backend/llm_infrastructure/embedding/engines/sentence/embedder.py`

```python
"""SentenceTransformer embedding engine."""

from typing import Iterable
import numpy as np
from sentence_transformers import SentenceTransformer

from .base import BaseEmbedder
from .utils import pick_device, l2_normalize

class SentenceTransformerEmbedder(BaseEmbedder):
    """
    SentenceTransformer ì—”ì§„ (ì‹±ê¸€í†¤ íŒ¨í„´).

    íŠ¹ì§•:
    - ëª¨ë¸ëª… + ë””ë°”ì´ìŠ¤ ì¡°í•©ìœ¼ë¡œ ì‹±ê¸€í†¤ ê´€ë¦¬
    - L2 ì •ê·œí™” ìë™ ì ìš©
    - GPU ìë™ ì„ íƒ ì§€ì›
    """

    _instance_cache: dict[str, "SentenceTransformerEmbedder"] = {}

    def __new__(cls, model_name: str, device: str | None = None, **kwargs):
        """ì‹±ê¸€í†¤ íŒ¨í„´ìœ¼ë¡œ ëª¨ë¸ ì¬ì‚¬ìš©."""
        key = f"{model_name}@{device}"
        if key not in cls._instance_cache:
            cls._instance_cache[key] = super().__new__(cls)
        return cls._instance_cache[key]

    def __init__(self, model_name: str, device: str | None = None, **kwargs):
        """
        Args:
            model_name: HuggingFace ëª¨ë¸ ID
            device: ë””ë°”ì´ìŠ¤ ("auto", "round-robin", "cuda:X", "cpu")
            **kwargs: SentenceTransformer ì¶”ê°€ ì¸ì
        """
        if hasattr(self, "_init_done"):
            return  # ì‹±ê¸€í†¤ ì¬ì§„ì… ë°©ì§€

        self.model_name = model_name
        real_device = pick_device(device)
        self.device = real_device
        self.model = SentenceTransformer(
            model_name,
            device=real_device,
            trust_remote_code=True,
            **kwargs
        )
        self._init_done = True

        # E5 ê³„ì—´ ëª¨ë¸ prefix ì§€ì›
        self.uses_e5_prefix = "e5" in model_name.lower()

    def encode(
        self,
        texts: Iterable[str],
        show_progress_bar: bool = False
    ) -> np.ndarray:
        """
        ë°°ì¹˜ ì„ë² ë”© (L2 ì •ê·œí™” í¬í•¨).

        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            show_progress_bar: ì§„í–‰ë°” í‘œì‹œ ì—¬ë¶€

        Returns:
            L2 ì •ê·œí™”ëœ ì„ë² ë”© í–‰ë ¬
        """
        vecs = self.model.encode(
            list(texts),
            show_progress_bar=show_progress_bar
        )
        return l2_normalize(vecs)

    def encode_query(self, text: str) -> np.ndarray:
        """ë‹¨ì¼ ì¿¼ë¦¬ ì„ë² ë”©."""
        return self.encode([text])[0]

    def get_dimension(self) -> int:
        """ì„ë² ë”© ì°¨ì› ë°˜í™˜."""
        return self.model.get_sentence_embedding_dimension()
```

### Step 5: íŒ©í† ë¦¬ í•¨ìˆ˜ ë§ˆì´ê·¸ë ˆì´ì…˜

**íŒŒì¼**: `backend/llm_infrastructure/embedding/engines/sentence/factory.py`

```python
"""Factory function for creating embedders."""

from typing import Literal
from .embedder import SentenceTransformerEmbedder
from .cache import CachedEmbedder

_EmbedderType = Literal["sentence", "openai"]

def create_embedder(
    typ: _EmbedderType = "sentence",
    model_name: str = "nlpai-lab/KoE5",
    device: str | None = None,
    use_cache: bool = False,
    cache_dir: str = ".embed_cache",
    **kwargs,
):
    """
    ì„ë² ë” íŒ©í† ë¦¬ í•¨ìˆ˜.

    Args:
        typ: ì„ë² ë” íƒ€ì… ("sentence", "openai")
        model_name: ëª¨ë¸ëª…
        device: ë””ë°”ì´ìŠ¤
        use_cache: ë””ìŠ¤í¬ ìºì‹± ì‚¬ìš© ì—¬ë¶€
        cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬
        **kwargs: ì¶”ê°€ ì¸ì

    Returns:
        ì„ë² ë” ì¸ìŠ¤í„´ìŠ¤
    """
    if typ == "sentence":
        embedder = SentenceTransformerEmbedder(
            model_name,
            device=device,
            **kwargs
        )

        if use_cache:
            embedder = CachedEmbedder(embedder, cache_dir=cache_dir)

        return embedder

    raise ValueError(f"Unknown embedder type: {typ}")
```

### Step 6: ì—”ì§„ __init__.py

**íŒŒì¼**: `backend/llm_infrastructure/embedding/engines/sentence/__init__.py`

```python
"""SentenceTransformer embedding engine."""

from .embedder import SentenceTransformerEmbedder
from .cache import CachedEmbedder
from .factory import create_embedder
from .utils import pick_device, l2_normalize, split_by_tokens

__all__ = [
    "SentenceTransformerEmbedder",
    "CachedEmbedder",
    "create_embedder",
    "pick_device",
    "l2_normalize",
    "split_by_tokens",
]
```

### Step 7: engines/__init__.py ì¶”ê°€

**íŒŒì¼**: `backend/llm_infrastructure/embedding/engines/__init__.py`

```python
"""Embedding engines."""

# í•„ìš” ì‹œ ì—”ì§„ ì¬export
from .sentence import create_embedder

__all__ = ["create_embedder"]
```

### Step 8: ì–´ëŒ‘í„° ì‘ì„± (ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì—°ê²°)

**íŒŒì¼**: `backend/llm_infrastructure/embedding/adapters/sentence.py`

```python
"""SentenceTransformer adapter for registry."""

from typing import Any
import numpy as np
import numpy.typing as npt

from ..base import BaseEmbedder
from ..registry import register_embedder
from ..engines.sentence import create_embedder

@register_embedder("koe5", version="v1")
class KoE5Embedder(BaseEmbedder):
    """
    í•œêµ­ì–´ E5 ì„ë² ë” (KoE5).

    Config:
        device: str = "auto" - GPU ì„ íƒ ì „ëµ
        use_cache: bool = False - ë””ìŠ¤í¬ ìºì‹±
        cache_dir: str = ".embed_cache"
    """

    def __init__(self, **kwargs: Any) -> None:
        super().__init__(**kwargs)

        device = self.config.get("device", "auto")
        use_cache = self.config.get("use_cache", False)
        cache_dir = self.config.get("cache_dir", ".embed_cache")

        # ì—”ì§„ ìƒì„±
        self.engine = create_embedder(
            typ="sentence",
            model_name="nlpai-lab/KoE5",
            device=device,
            use_cache=use_cache,
            cache_dir=cache_dir,
        )

        self.dimension = self.engine.get_dimension()

    def embed(self, text: str) -> npt.NDArray[np.float32]:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©."""
        return self.engine.encode_query(text)

    def embed_batch(
        self,
        texts: list[str],
        batch_size: int = 32,
    ) -> npt.NDArray[np.float32]:
        """ë°°ì¹˜ ì„ë² ë”©."""
        # SentenceTransformerëŠ” ë‚´ë¶€ì ìœ¼ë¡œ ë°°ì¹˜ ì²˜ë¦¬
        return self.engine.encode(texts, show_progress_bar=len(texts) > 100)


@register_embedder("multilingual_e5", version="v1")
class MultilingualE5Embedder(BaseEmbedder):
    """Multilingual E5 ì„ë² ë”."""

    def __init__(self, **kwargs: Any) -> None:
        super().__init__(**kwargs)

        device = self.config.get("device", "auto")
        use_cache = self.config.get("use_cache", False)
        cache_dir = self.config.get("cache_dir", ".embed_cache")

        self.engine = create_embedder(
            typ="sentence",
            model_name="intfloat/multilingual-e5-large",
            device=device,
            use_cache=use_cache,
            cache_dir=cache_dir,
        )

        self.dimension = self.engine.get_dimension()

    def embed(self, text: str) -> npt.NDArray[np.float32]:
        return self.engine.encode_query(text)

    def embed_batch(
        self,
        texts: list[str],
        batch_size: int = 32,
    ) -> npt.NDArray[np.float32]:
        return self.engine.encode(texts, show_progress_bar=len(texts) > 100)
```

### Step 9: ì–´ëŒ‘í„° __init__.py

**íŒŒì¼**: `backend/llm_infrastructure/embedding/adapters/__init__.py`

```python
"""Embedding adapters for registry."""

# ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì–´ëŒ‘í„° ìë™ ë¡œë“œ
from . import sentence  # ìë™ ë“±ë¡
try:
    from . import langchain  # ì„ íƒì‚¬í•­
except ImportError:
    pass

__all__ = []
```

### Step 10: LangChain ì–´ëŒ‘í„° (ì„ íƒì‚¬í•­)

**íŒŒì¼**: `backend/llm_infrastructure/embedding/adapters/langchain.py`

```python
"""LangChain adapter for embedders."""

from langchain.embeddings.base import Embeddings
from ..base import BaseEmbedder

class LangChainEmbedderAdapter(Embeddings):
    """
    BaseEmbedder â†’ LangChain ì–´ëŒ‘í„°.

    ì‚¬ìš©ë²•:
        embedder = get_embedder("koe5", version="v1")
        lc_embedder = LangChainEmbedderAdapter(embedder)
        docs_vecs = lc_embedder.embed_documents(["text1", "text2"])
        query_vec = lc_embedder.embed_query("query")
    """

    def __init__(self, inner: BaseEmbedder):
        """
        Args:
            inner: BaseEmbedder ì¸ìŠ¤í„´ìŠ¤
        """
        self.inner = inner

    def embed_documents(self, texts: list[str]) -> list[list[float]]:
        """ë¬¸ì„œ ì„ë² ë”© (LangChain ì¸í„°í˜ì´ìŠ¤)."""
        vecs = self.inner.embed_batch(texts)
        return vecs.tolist()

    def embed_query(self, text: str) -> list[float]:
        """ì¿¼ë¦¬ ì„ë² ë”© (LangChain ì¸í„°í˜ì´ìŠ¤)."""
        vec = self.inner.embed(text)
        return vec.tolist()
```

### Step 11: FAISS ì¸ë±ì‹± (ì„ íƒì‚¬í•­)

**íŒŒì¼**: `backend/llm_infrastructure/embedding/indexing/faiss_index.py`

**ì£¼ì˜**: retrieval ëª¨ë“ˆê³¼ ê²½ë¡œ ì¶©ëŒ ì—†ë„ë¡ ëª…í™•íˆ ë¶„ë¦¬

```python
"""FAISS indexing for embeddings."""

import faiss
import numpy as np
from pathlib import Path

class FaissIndex:
    """
    FAISS IVF ì¸ë±ìŠ¤ ë˜í¼.

    ì‚¬ìš©ë²•:
        # ì¸ë±ìŠ¤ ìƒì„±
        idx = FaissIndex(dim=768, nlist=100, path="docs.ivf")
        idx.train_add(vecs, ids)
        idx.save()

        # ì¸ë±ìŠ¤ ë¡œë“œ ë° ê²€ìƒ‰
        idx = FaissIndex.load("docs.ivf")
        distances, indices = idx.search(query_vec, top_k=5)
    """

    def __init__(self, dim: int, nlist: int = 100, path: str | Path = "docs.ivf"):
        """
        Args:
            dim: ì„ë² ë”© ì°¨ì›
            nlist: IVF í´ëŸ¬ìŠ¤í„° ìˆ˜
            path: ì¸ë±ìŠ¤ ì €ì¥ ê²½ë¡œ
        """
        self.dim = dim
        self.nlist = nlist
        self.path = Path(path)

        quantizer = faiss.IndexFlatL2(dim)
        self.index = faiss.IndexIVFFlat(quantizer, dim, nlist)
        self.index.nprobe = max(4, int(0.05 * nlist))

    def train_add(self, vecs: np.ndarray, ids: np.ndarray):
        """
        ì¸ë±ìŠ¤ í•™ìŠµ ë° ë²¡í„° ì¶”ê°€.

        Args:
            vecs: ì„ë² ë”© ë²¡í„° (n, dim)
            ids: ë¬¸ì„œ ID (n,)
        """
        if not self.index.is_trained:
            self.index.train(vecs)
        self.index.add_with_ids(vecs, ids)

    def save(self):
        """ì¸ë±ìŠ¤ ì €ì¥."""
        faiss.write_index(self.index, str(self.path))

    @classmethod
    def load(cls, path: str | Path):
        """
        ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ.

        Args:
            path: ì¸ë±ìŠ¤ íŒŒì¼ ê²½ë¡œ

        Returns:
            FaissIndex ì¸ìŠ¤í„´ìŠ¤
        """
        idx = faiss.read_index(str(path))
        obj = object.__new__(cls)
        obj.dim = idx.d
        obj.nlist = idx.nlist
        obj.path = Path(path)
        obj.index = idx
        return obj

    def search(self, query: np.ndarray, top_k: int = 5):
        """
        ê²€ìƒ‰ ìˆ˜í–‰.

        Args:
            query: ì¿¼ë¦¬ ë²¡í„° (dim,) or (1, dim)
            top_k: ìƒìœ„ Kê°œ

        Returns:
            distances: ê±°ë¦¬ ë°°ì—´ (top_k,)
            indices: ì¸ë±ìŠ¤ ë°°ì—´ (top_k,)
        """
        if query.ndim == 1:
            query = query.reshape(1, -1)
        D, I = self.index.search(query, top_k)
        return D[0], I[0]
```

## ğŸ’¡ ì½”ë“œ ì˜ˆì‹œ

### ì—”ì§„ ì§ì ‘ ì‚¬ìš© (í”„ë¡œí† íƒ€ì…)

```python
from backend.llm_infrastructure.embedding.engines.sentence import create_embedder

# 1. ê¸°ë³¸ ì‚¬ìš©
embedder = create_embedder(
    typ="sentence",
    model_name="nlpai-lab/KoE5",
    device="auto",  # GPU ìë™ ì„ íƒ
)

texts = ["ì•ˆë…•í•˜ì„¸ìš”", "ì„ë² ë”© í…ŒìŠ¤íŠ¸"]
vecs = embedder.encode(texts)  # (2, 1024)
print(vecs.shape, vecs.dtype)

# 2. ìºì‹± ì‚¬ìš© (Docker ë³¼ë¥¨ ë§ˆìš´íŠ¸ í•„ìš”)
cached_embedder = create_embedder(
    typ="sentence",
    model_name="nlpai-lab/KoE5",
    device="auto",
    use_cache=True,
    cache_dir="/app/cache/embeddings",  # ì“°ê¸° ê°€ëŠ¥í•œ ê²½ë¡œ
)

# ì²« í˜¸ì¶œ: ì‹¤ì œ ê³„ì‚° + ìºì‹±
vecs1 = cached_embedder.encode(["ì•ˆë…•í•˜ì„¸ìš”"])

# ë‘ ë²ˆì§¸ í˜¸ì¶œ: ìºì‹œì—ì„œ ë¡œë“œ (ë¹ ë¦„)
vecs2 = cached_embedder.encode(["ì•ˆë…•í•˜ì„¸ìš”"])
```

### ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì‚¬ìš© (í”„ë¡œë•ì…˜)

```python
from backend.llm_infrastructure.embedding.registry import get_embedder
from backend.config.settings import rag_settings

# 1. ì„¤ì • ê¸°ë°˜
embedder = get_embedder(
    rag_settings.embedding_method,
    version=rag_settings.embedding_version,
    device="auto",
    use_cache=True,
)

# 2. ì§ì ‘ ì§€ì •
embedder = get_embedder(
    "koe5",
    version="v1",
    device="cuda:0",
    use_cache=False,
)

# 3. ì‚¬ìš©
vec = embedder.embed("ë‹¨ì¼ í…ìŠ¤íŠ¸")  # (dim,)
vecs = embedder.embed_batch(["í…ìŠ¤íŠ¸1", "í…ìŠ¤íŠ¸2"])  # (2, dim)
```

### LangChain í†µí•©

```python
from backend.llm_infrastructure.embedding.registry import get_embedder
from backend.llm_infrastructure.embedding.adapters.langchain import LangChainEmbedderAdapter

# ì„ë² ë” ìƒì„±
embedder = get_embedder("koe5", version="v1")

# LangChain ì–´ëŒ‘í„°ë¡œ ê°ì‹¸ê¸°
lc_embedder = LangChainEmbedderAdapter(embedder)

# LangChainì—ì„œ ì‚¬ìš©
from langchain.vectorstores import FAISS

docs = ["ë¬¸ì„œ1", "ë¬¸ì„œ2", "ë¬¸ì„œ3"]
vectorstore = FAISS.from_texts(docs, lc_embedder)
results = vectorstore.similarity_search("ì¿¼ë¦¬", k=2)
```

### FAISS ì¸ë±ì‹±

```python
from backend.llm_infrastructure.embedding.engines.sentence import create_embedder
from backend.llm_infrastructure.embedding.indexing.faiss_index import FaissIndex
import numpy as np

# ì„ë² ë”© ìƒì„±
embedder = create_embedder("sentence", "nlpai-lab/KoE5")
docs = ["ë¬¸ì„œ1", "ë¬¸ì„œ2", "ë¬¸ì„œ3"]
vecs = embedder.encode(docs)

# ì¸ë±ìŠ¤ ìƒì„± ë° ì €ì¥
idx = FaissIndex(dim=1024, nlist=100, path="/app/data/docs.ivf")
ids = np.arange(len(docs))
idx.train_add(vecs, ids)
idx.save()

# ì¸ë±ìŠ¤ ë¡œë“œ ë° ê²€ìƒ‰
idx = FaissIndex.load("/app/data/docs.ivf")
query_vec = embedder.encode_query("ì¿¼ë¦¬")
distances, indices = idx.search(query_vec, top_k=2)
print(f"Top 2: {indices}, Distances: {distances}")
```

## ğŸ”„ í™˜ê²½ë³€ìˆ˜ ì„¤ì •

**.env íŒŒì¼**:

```bash
# Embedding ì„¤ì •
RAG_EMBEDDING_METHOD=koe5
RAG_EMBEDDING_VERSION=v1
EMBEDDING_DEVICE=auto          # auto, round-robin, cuda:0, cpu
EMBEDDING_USE_CACHE=false
EMBEDDING_CACHE_DIR=.embed_cache
```

**backend/config/settings.py** (ì—…ë°ì´íŠ¸ í•„ìš”):

```python
class RAGSettings(BaseSettings):
    # ... ê¸°ì¡´ ì„¤ì • ...

    # Embedding ì¶”ê°€
    embedding_method: str = Field("koe5", env="RAG_EMBEDDING_METHOD")
    embedding_version: str = Field("v1", env="RAG_EMBEDDING_VERSION")
    embedding_device: str = Field("auto", env="EMBEDDING_DEVICE")
    embedding_use_cache: bool = Field(False, env="EMBEDDING_USE_CACHE")
    embedding_cache_dir: str = Field(".embed_cache", env="EMBEDDING_CACHE_DIR")
```

## ğŸ§ª í…ŒìŠ¤íŠ¸

### ì—”ì§„ í…ŒìŠ¤íŠ¸ (ë‹¨ìœ„)

**íŒŒì¼**: `tests/embedding/test_sentence_engine.py`

```python
"""SentenceTransformer engine tests."""

import numpy as np
import pytest
from backend.llm_infrastructure.embedding.engines.sentence import create_embedder

def test_create_embedder():
    """íŒ©í† ë¦¬ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸."""
    embedder = create_embedder(
        typ="sentence",
        model_name="nlpai-lab/KoE5",
        device="cpu",  # CI í™˜ê²½ ê³ ë ¤
    )
    assert embedder is not None
    assert hasattr(embedder, "encode")

def test_encode():
    """ë°°ì¹˜ ì„ë² ë”© í…ŒìŠ¤íŠ¸."""
    embedder = create_embedder("sentence", "nlpai-lab/KoE5", device="cpu")
    texts = ["ì•ˆë…•í•˜ì„¸ìš”", "í…ŒìŠ¤íŠ¸"]
    vecs = embedder.encode(texts)

    assert vecs.shape[0] == 2
    assert vecs.shape[1] > 0  # ì°¨ì›
    assert np.allclose(np.linalg.norm(vecs, axis=1), 1.0)  # L2 ì •ê·œí™”

def test_encode_query():
    """ë‹¨ì¼ ì¿¼ë¦¬ ì„ë² ë”© í…ŒìŠ¤íŠ¸."""
    embedder = create_embedder("sentence", "nlpai-lab/KoE5", device="cpu")
    vec = embedder.encode_query("ì¿¼ë¦¬")

    assert vec.ndim == 1
    assert len(vec) > 0

def test_cache(tmp_path):
    """ìºì‹± í…ŒìŠ¤íŠ¸."""
    cache_dir = tmp_path / "cache"
    embedder = create_embedder(
        "sentence",
        "nlpai-lab/KoE5",
        device="cpu",
        use_cache=True,
        cache_dir=str(cache_dir),
    )

    text = "ìºì‹œ í…ŒìŠ¤íŠ¸"

    # ì²« í˜¸ì¶œ: ìºì‹œ ë¯¸ìŠ¤
    import time
    start = time.time()
    vec1 = embedder.encode([text])
    t1 = time.time() - start

    # ë‘ ë²ˆì§¸ í˜¸ì¶œ: ìºì‹œ íˆíŠ¸ (ë¹ ë¦„)
    start = time.time()
    vec2 = embedder.encode([text])
    t2 = time.time() - start

    assert np.allclose(vec1, vec2)
    assert t2 < t1  # ìºì‹œê°€ ë” ë¹ ë¦„

def test_gpu_selection():
    """GPU ì„ íƒ ë¡œì§ í…ŒìŠ¤íŠ¸."""
    from backend.llm_infrastructure.embedding.engines.sentence.utils import pick_device

    # CPU fallback
    device = pick_device("cpu")
    assert device == "cpu"

    # auto (ë©”ëª¨ë¦¬ ì¡°íšŒ ì‹¤íŒ¨ ì‹œ ì•ˆì „)
    device = pick_device("auto")
    assert device in ["cpu", "cuda:0"]
```

### ì–´ëŒ‘í„° í…ŒìŠ¤íŠ¸ (í†µí•©)

**íŒŒì¼**: `tests/embedding/test_adapters.py`

```python
"""Embedding adapter tests."""

import pytest
from backend.llm_infrastructure.embedding.registry import get_embedder

def test_registry_koe5():
    """KoE5 ì–´ëŒ‘í„° ë ˆì§€ìŠ¤íŠ¸ë¦¬ í…ŒìŠ¤íŠ¸."""
    embedder = get_embedder(
        "koe5",
        version="v1",
        device="cpu",
        use_cache=False,
    )

    vec = embedder.embed("í…ŒìŠ¤íŠ¸")
    assert vec.ndim == 1

    vecs = embedder.embed_batch(["í…ìŠ¤íŠ¸1", "í…ìŠ¤íŠ¸2"])
    assert vecs.shape[0] == 2

def test_registry_multilingual_e5():
    """Multilingual E5 ì–´ëŒ‘í„° í…ŒìŠ¤íŠ¸."""
    embedder = get_embedder(
        "multilingual_e5",
        version="v1",
        device="cpu",
    )

    vec = embedder.embed("test")
    assert vec.ndim == 1
```

### Docker í™˜ê²½ í…ŒìŠ¤íŠ¸

**docker-compose.test.yml**:

```yaml
version: '3.8'

services:
  embedding-test:
    build:
      context: .
      dockerfile: Dockerfile
    environment:
      - EMBEDDING_DEVICE=cpu
      - EMBEDDING_USE_CACHE=true
      - EMBEDDING_CACHE_DIR=/app/cache/embeddings
    volumes:
      - ./tests:/app/tests
      - embedding_cache:/app/cache/embeddings
    command: pytest tests/embedding/ -v

volumes:
  embedding_cache:
```

ì‹¤í–‰:
```bash
docker-compose -f docker-compose.test.yml up --abort-on-container-exit
```

## ğŸ³ Docker ì„¤ì •

### .dockerignore ì¶”ê°€

```
# .dockerignore
.embed_cache/
*.cache
*.ivf
*.index
__pycache__/
.pytest_cache/
```

### Dockerfile ì˜ˆì‹œ

```dockerfile
FROM python:3.10-slim

WORKDIR /app

# ì˜ì¡´ì„± ì„¤ì¹˜
COPY pyproject.toml .
RUN pip install -e .[dev]

# ìºì‹œ ë””ë ‰í† ë¦¬ ê¶Œí•œ
RUN mkdir -p /app/cache/embeddings && chmod 777 /app/cache/embeddings

COPY . .

CMD ["python", "-m", "uvicorn", "backend.api.main:app", "--host", "0.0.0.0"]
```

### docker-compose.yml ë³¼ë¥¨ ì„¤ì •

```yaml
services:
  backend:
    volumes:
      - embedding_cache:/app/cache/embeddings
      - faiss_indices:/app/data/indices
    environment:
      - EMBEDDING_CACHE_DIR=/app/cache/embeddings
      - CUDA_VISIBLE_DEVICES=0,1  # GPU ì œí•œ

volumes:
  embedding_cache:
  faiss_indices:
```

## âœ… ë§ˆì´ê·¸ë ˆì´ì…˜ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì½”ë“œ ë§ˆì´ê·¸ë ˆì´ì…˜
- [ ] **Step 1**: `engines/sentence/utils.py` ìƒì„± (device, normalize, chunking)
- [ ] **Step 2**: `engines/sentence/base.py` ìƒì„± (BaseEmbedder)
- [ ] **Step 3**: `engines/sentence/cache.py` ìƒì„± (ìºì‹±)
- [ ] **Step 4**: `engines/sentence/embedder.py` ìƒì„± (SentenceTransformer)
- [ ] **Step 5**: `engines/sentence/factory.py` ìƒì„± (íŒ©í† ë¦¬)
- [ ] **Step 6**: `engines/sentence/__init__.py` ìƒì„±
- [ ] **Step 7**: `engines/__init__.py` ìƒì„±
- [ ] **Step 8**: `adapters/sentence.py` ìƒì„± (ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì–´ëŒ‘í„°)
- [ ] **Step 9**: `adapters/__init__.py` ìƒì„±
- [ ] **Step 10**: `adapters/langchain.py` ìƒì„± (ì„ íƒ)
- [ ] **Step 11**: `indexing/faiss_index.py` ìƒì„± (ì„ íƒ)

### ì„¤ì • ë° í™˜ê²½
- [ ] **Step 12**: `backend/config/settings.py` ì—…ë°ì´íŠ¸
- [ ] **Step 13**: `.env` íŒŒì¼ ì—…ë°ì´íŠ¸
- [ ] **Step 14**: `pyproject.toml` ì˜ì¡´ì„± ì¶”ê°€
- [ ] **Step 15**: `.dockerignore` ì—…ë°ì´íŠ¸
- [ ] **Step 16**: Docker ë³¼ë¥¨ ì„¤ì •

### í…ŒìŠ¤íŠ¸ ë° ê²€ì¦
- [ ] **Step 17**: ì—”ì§„ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„±
- [ ] **Step 18**: ì–´ëŒ‘í„° í†µí•© í…ŒìŠ¤íŠ¸ ì‘ì„±
- [ ] **Step 19**: Docker í™˜ê²½ í…ŒìŠ¤íŠ¸
- [ ] **Step 20**: GPU ì„ íƒ ë¡œì§ ê²€ì¦
- [ ] **Step 21**: ìºì‹œ hit/miss ê²€ì¦

## ğŸ“ ë§ˆì´ê·¸ë ˆì´ì…˜ í›„ íŒŒì¼ êµ¬ì¡°

```
backend/llm_infrastructure/embedding/
â”œâ”€â”€ engines/                     # âœ… ì‹ ê·œ
â”‚   â”œâ”€â”€ __init__.py              # âœ… ì—”ì§„ ì¬export
â”‚   â””â”€â”€ sentence/                # âœ… SentenceTransformer ì—”ì§„
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ base.py              # âœ… llm-agentì˜ embedders/base.py
â”‚       â”œâ”€â”€ embedder.py          # âœ… llm-agentì˜ embedders/sentence.py
â”‚       â”œâ”€â”€ cache.py             # âœ… llm-agentì˜ embedders/cache.py
â”‚       â”œâ”€â”€ utils.py             # âœ… llm-agentì˜ utils/*
â”‚       â””â”€â”€ factory.py           # âœ… llm-agentì˜ create_embedder.py
â”œâ”€â”€ adapters/                    # âœ… ì‹ ê·œ
â”‚   â”œâ”€â”€ __init__.py              # âœ… ì–´ëŒ‘í„° ìë™ ë¡œë“œ
â”‚   â”œâ”€â”€ sentence.py              # âœ… ì‹ ê·œ (ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì–´ëŒ‘í„°)
â”‚   â”œâ”€â”€ tei.py                   # âœ… ê¸°ì¡´ embedders/tei_client.py ì´ë™
â”‚   â””â”€â”€ langchain.py             # âœ… llm-agentì˜ adapters/langchain.py
â”œâ”€â”€ indexing/                    # âœ… ì‹ ê·œ (ì„ íƒ)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ faiss_index.py           # âœ… llm-agentì˜ indexing.py
â”œâ”€â”€ base.py                      # âœ… ìœ ì§€ (ë ˆì§€ìŠ¤íŠ¸ë¦¬ìš©)
â””â”€â”€ registry.py                  # âœ… ìœ ì§€
```

**í•µì‹¬ ë³€ê²½**:
1. `sentence_engine/` â†’ `engines/sentence/` (ê³„ì¸µ ì¶”ê°€)
2. ì—”ì§„ ë‚´ë¶€ì— `base.py` ì¶”ê°€ (ì—”ì§„ìš© BaseEmbedder)
3. `adapters/__init__.py`ì—ì„œ ìë™ importë¡œ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë“±ë¡
4. TEI í´ë¼ì´ì–¸íŠ¸ë„ `adapters/tei.py`ë¡œ ì´ë™

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

1. âœ… ì´ ê°€ì´ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì½”ë“œ ë§ˆì´ê·¸ë ˆì´ì…˜
2. âœ… í…ŒìŠ¤íŠ¸ ì‘ì„± (ì—”ì§„, ì–´ëŒ‘í„° ê°ê°)
3. âœ… ë¬¸ì„œ ì—…ë°ì´íŠ¸ (`embedding/README.md` ì‘ì„±)
4. âœ… ì‹¤í—˜ ëŸ¬ë„ˆì— í†µí•©
5. âœ… Retrieval ëª¨ë“ˆ ë§ˆì´ê·¸ë ˆì´ì…˜ (ë‹¤ìŒ ë‹¨ê³„)

## ğŸ“š ê´€ë ¨ ë¬¸ì„œ

- [Preprocessing Guide](../backend/llm_infrastructure/preprocessing/README.md): ì—”ì§„-ì–´ëŒ‘í„° íŒ¨í„´ ì˜ˆì‹œ
- [í”„ë¡œì íŠ¸ README](../README.md): ì „ì²´ ì•„í‚¤í…ì²˜

## ğŸ“ ë¬¸ì˜

- ì„ë² ë”© ì—”ì§„ ê´€ë ¨: `sentence_engine/` ì½”ë“œ í™•ì¸
- ë ˆì§€ìŠ¤íŠ¸ë¦¬ í†µí•©: `adapters/`, `registry.py` í™•ì¸
- ì„¤ì • ì£¼ì…: `backend/config/settings.py` í™•ì¸
