# 2026-01-02 Code Review & TODO

> ê¸°ì¤€ ë¬¸ì„œ: `docs/2026-01-02_retrieval review.md`
>
> ëª©ì : ë¦¬íŠ¸ë¦¬ë²Œ í’ˆì§ˆ(Recall/Precision) ê°œì„  + ìš´ì˜ ì•ˆì •ì„± + â€œì„¤ì • ì£¼ë„(pluggable)â€ êµ¬ì¡°ë¡œ ì»´í¬ë„ŒíŠ¸ êµì²´(LLM/í† í¬ë‚˜ì´ì €/MQ/reranker/retriever/agent)ë¥¼ ì‰½ê²Œ ë§Œë“¤ê¸°.

---

## 0) í˜„ì¬ ìƒíƒœ ìš”ì•½(ì½”ë“œ ê¸°ì¤€)

- ì„¤ì • ë¡œë”©: `.env` + Pydantic Settings (`RAG_*`, `SEARCH_*`, `VLLM_*`, `TEI_*` ë“±) ê¸°ë°˜ (`backend/config/settings.py`).
- Search backend ë¶„ê¸°: `SEARCH_BACKEND=local|es`ì— ë”°ë¼ `SearchService(local index)` ë˜ëŠ” `EsSearchService(ES)`ê°€ startupì—ì„œ ì£¼ì…ë¨ (`backend/api/main.py`).
- LLM/embedding/reranker/query-expander/retrieverëŠ” ëª¨ë‘ ë ˆì§€ìŠ¤íŠ¸ë¦¬ íŒ¨í„´ì´ ì´ë¯¸ ì¡´ì¬(= í”ŒëŸ¬ê·¸ì¸ í™•ì¥ ê°€ëŠ¥)í•˜ë‚˜, **ì¼ë¶€ëŠ” ëŸ°íƒ€ì„ wiringì´ ê³ ì •/ë¶ˆì™„ì „**:
  - ê¸°ë³¸ LLM DIëŠ” í˜„ì¬ `vllm`ìœ¼ë¡œ ê³ ì • (`backend/api/dependencies.py`).
  - `RAG_RETRIEVAL_PRESET`ì€ ì¡´ì¬í•˜ì§€ë§Œ ì‹¤ì œë¡œ preset ì ìš© ë¡œì§ì´ ì—†ìŒ(ì½”ë“œ preset/YAML preset ë‘˜ ë‹¤ ìˆìœ¼ë‚˜ ëŸ°íƒ€ì„ ì‚¬ìš©ì´ ì œí•œì ).
  - Multi-query/rerankëŠ” local(SearchService)ì—ì„œë§Œ í†µí•©ë˜ì–´ ìˆê³ , ES(EsSearchService) ê²½ë¡œì—ëŠ” ì•„ì§ í†µí•©ë˜ì§€ ì•ŠìŒ.
  - LangGraph agentëŠ” ê·¸ë˜í”„ ë ˆë²¨ì—ì„œ MQ/ì¬ì‹œë„ë¥¼ ìˆ˜í–‰í•˜ë©°, ë‚´ë¶€ searchì˜ MQ/rerankëŠ” ê°•ì œë¡œ ë”(ì¤‘ë³µ ë°©ì§€ ëª©ì ).

### âœ… ì˜ ë˜ì–´ ìˆëŠ” ë¶€ë¶„

- **ë ˆì§€ìŠ¤íŠ¸ë¦¬(í”ŒëŸ¬ê·¸ì¸) íŒ¨í„´ ê¸°ë°˜ì´ íƒ„íƒ„í•¨:** LLM/Embedding/Retriever/Reranker/QueryExpander/Preprocessor ë“±ì´ â€œì´ë¦„+ë²„ì „ â†’ ì¸ìŠ¤í„´ìŠ¤â€ë¡œ êµì²´ ê°€ëŠ¥.
- **Pydantic Settingsë¡œ ì„¤ì • êµ¬ì¡°í™”:** ê¸°ë³¸ê°’ì´ ëª…í™•í•˜ê³ , `.env`/í™˜ê²½ë³€ìˆ˜ë¡œ ì¬ì •ì˜ ê°€ëŠ¥.
- **FastAPI DI + ìºì‹± íŒ¨í„´:** ë¬´ê±°ìš´ ê°ì²´ë¥¼ `Depends()`ë¡œ ì£¼ì…í•˜ê³  `@lru_cache`ë¡œ ì¬ì‚¬ìš©(ì„±ëŠ¥/ì¼ê´€ì„±/í…ŒìŠ¤íŠ¸ ìš©ì´ì„± ì¸¡ë©´ ì¥ì ).

### âš ï¸ ê°œì„ ì´ í•„ìš”í•œ ë¶€ë¶„

- **ì„¤ì • íŒŒì¼(YAML/JSON) ê¸°ë°˜ í”„ë¦¬ì…‹ ì ìš©ì´ ë¯¸ì™„ì„±:** `retrieval_preset`/`preset_loader.py`/`backend/config/presets/*.yaml`ì´ ìˆìœ¼ë‚˜ ì‹¤ì œ ëŸ°íƒ€ì„ wiringì— ì—°ê²°ë˜ì§€ ì•ŠìŒ.
- **ëŸ°íƒ€ì„ êµì²´(ë™ì  ì¬êµ¬ì„±) ì œì•½:** DIê°€ `@lru_cache` ê¸°ë°˜ì´ë¼ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ ì¤‘ ì„¤ì • ë³€ê²½ë§Œìœ¼ë¡œ êµì²´ê°€ ë˜ì§€ ì•ŠìŒ(ì‹¤í—˜/ë©€í‹°íŒŒì´í”„ë¼ì¸ ìš´ì˜ì— ë¶ˆë¦¬).
- **ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ ì¡°ë¦½ì´ í•˜ë“œì½”ë”©:** ë‹¨ê³„(ì „ì²˜ë¦¬â†’í™•ì¥â†’ê²€ìƒ‰â†’ë¨¸ì§€â†’ì¬ë­í‚¹)ê°€ ì½”ë“œ ë‚´ë¶€ `if/else`ë¡œ ê³ ì •ë˜ì–´ í™•ì¥/ì‚½ì…(ì˜ˆ: ì»¤ìŠ¤í…€ í† í¬ë‚˜ì´ì €/ê²Œì´íŒ…/ë‹¤ë‹¨ê³„ ê²€ìƒ‰)ì´ ì–´ë ¤ì›€. íŠ¹íˆ ES ê²½ë¡œëŠ” MQ/rerank ë¯¸í†µí•©.
- **í”„ë¦¬ì…‹ UX ë¶€ì¡±:** â€œpreset í•˜ë‚˜ë¡œ êµ¬ì„± ì „í™˜â€ì„ CLI/API/UIì—ì„œ ì‰½ê²Œ í•˜ì§€ ëª»í•¨.
- **Agent êµ¬ì„± ì„¤ì •í™” ë¶€ì¡±:** LangGraph agentì˜ toolchain/flow/ì •ì±…(ì‹œë„ íšŸìˆ˜, MQ/rerank í™œìš© ë°©ì‹ ë“±)ì´ ì½”ë“œì— ê³ ì •ë˜ì–´ ìš´ì˜/ì‹¤í—˜ ì „í™˜ ë¹„ìš©ì´ í¼.
- **ì‹¤í–‰ ë°©ì‹ì´ ë‹¤ë©´ì (FastAPI/CLI/ë…¸íŠ¸ë¶):** ì„¤ì • ê³µìœ /ì¬í˜„ì„±(â€œê°™ì€ presetìœ¼ë¡œ FastAPI/CLIê°€ ë™ì¼ ë™ì‘â€)ì„ ê°•í™”í•  í•„ìš”ê°€ ìˆìŒ.

---

## 1) TODO (ìš°ì„ ìˆœìœ„)

### P0 â€” ì‚¬ì‹¤ í™•ì¸/ì •í•©ì„±(ìµœìš°ì„ )

- [x] **í˜„ì¬ ìš´ì˜(ë˜ëŠ” dev) ES mapping/settings ìŠ¤ëƒ…ìƒ· í™•ë³´**
  - ì‚°ì¶œë¬¼:
    - `docs/es_mapping_snapshot_2026-01-02.json`
    - `docs/es_settings_snapshot_2026-01-02.json`
  - ìŠ¤ëƒ…ìƒ· ê¸°ë°˜ í•µì‹¬ í™•ì¸ì‚¬í•­:
    - `embedding.dims = 768` + `index_options.type = int8_hnsw`
    - `chunk_summary`/`doc_description`/`chunk_keywords`ëŠ” `text`ë¡œ ê²€ìƒ‰ ê°€ëŠ¥
    - ë‹¤ìˆ˜ ë¬¸ìì—´ í•„ë“œê°€ `text` + `.keyword` ë©€í‹°í•„ë“œ(ë™ì  ë§¤í•‘) í˜•íƒœ â†’ í•„í„°ëŠ” `.keyword` ì‚¬ìš© ê¶Œì¥
    - settingsì— analysisê°€ ì—†ì–´ì„œ Nori analyzer ë¯¸ì‚¬ìš©(í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ ë¶ˆê°€)
    - `chunk_keywords.text` ê°™ì€ ì„œë¸Œí•„ë“œëŠ” ì—†ìŒ(ì¿¼ë¦¬/ì½”ë“œì—ì„œ í•„ë“œëª… ì •í•©ì„± í•„ìš”)
- [x] **ì„ë² ë”© ì°¨ì› ë¶ˆì¼ì¹˜ ë°©ì§€ ê°€ë“œë ˆì¼ ì •ë¦¬** âœ… ì™„ë£Œ (2026-01-02)
  - ëª©í‘œ: "ì¸ë±ìŠ¤ ìƒì„±/ì¸ì œìŠ¤ì²œ/ì„œë¹™" ì „ êµ¬ê°„ì—ì„œ dims ì¼ì¹˜ê°€ ìë™ ê²€ì¦ë˜ë„ë¡ ì²´í¬ë¦¬ìŠ¤íŠ¸/ê²€ì¦ ë¡œì§ ì¶”ê°€.
  - ì²´í¬í¬ì¸íŠ¸: `SEARCH_ES_EMBEDDING_DIMS` â†” ì‹¤ì œ embedder dimension â†” ES mapping dims.
  - ì™„ë£Œ ë‚´ìš©:
    - ì„¤ì •/ë§¤í•‘ ê¸°ë³¸ê°’ í†µì¼ (1024 â†’ 768)
    - `EsIndexManager.create_index()`ì— `validate_dims` ê²€ì¦ ë¡œì§ ì¶”ê°€
    - ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±: `scripts/validate_embedding_dimensions.py`
    - í˜„ì¬ ìƒíƒœ ê²€ì¦: ëª¨ë“  ì°¨ì› 768ë¡œ ì¼ì¹˜ í™•ì¸
  - ì‚°ì¶œë¬¼: `docs/2026-01-02_es_guardrails_snapshot.md`, `docs/2026-01-02_es_guardrails_improvements.md`
- [X] **alias/ì¸ë±ìŠ¤ ë„¤ì´ë° ì‹¤íƒœ ì ê²€** âœ… ì™„ë£Œ (2026-01-02)
  - ëª©í‘œ: "`rag_chunks_{env}_current`ê°€ aliasì¸ì§€ ì‹¤ì œ indexì¸ì§€"ë¥¼ ëª…í™•íˆ í•˜ê³ , ë¡¤ë§ ì—…ë°ì´íŠ¸ ì „ëµì´ ì‹¤ì œë¡œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸.
  - rag_chunks_dev_currentë¥¼ ì¡°íšŒí•˜ê²Œ ì½”ë“œë¥¼ ì‘ì„±í•´ë†“ì•˜ìŒ. ë‚˜ì¤‘ì— dataì˜ ë²„ì „ì´ ë°”ë€Œë”ë¼ë„ ì½”ë“œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ê³ , ë°”ë€ ë°ì´í„°ë¥¼ ì¡°íšŒí•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê²ƒì´ ëª©í‘œ.
  - ì²´í¬í¬ì¸íŠ¸: `_cat/aliases`, `_cat/indices`, EsIndexManager/ingestê°€ ë™ì¼ ê·œì¹™ì„ ì“°ëŠ”ì§€.
  - ì™„ë£Œ ë‚´ìš©:
    - ì‹¤íƒœ í™•ì¸: `rag_chunks_dev_current`ê°€ **ì‹¤ì œ ì¸ë±ìŠ¤**ë¡œ ì¡´ì¬ (alias ì•„ë‹˜)
    - ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±: `scripts/migrate_to_alias_strategy.py`
    - Dry-run í…ŒìŠ¤íŠ¸ ì„±ê³µ (340K ë¬¸ì„œ, 5.5GB, dims=768)
    - ë¡¤ë§ ì—…ë°ì´íŠ¸ ì „ëµ ë¬¸ì„œí™”
  - **ë‹¤ìŒ ì‘ì—…(ì‹¤í–‰ ë³´ë¥˜)**: alias ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
    - ìˆœì„œ: `--dry-run` í™•ì¸ âœ… â†’ (í•„ìš” ì‹œ ë°±ì—…) â†’ ì‹¤ì œ ì‹¤í–‰ â†’ `_cat/aliases`ë¡œ ê²€ì¦
- [x] **ES hybrid ê²€ìƒ‰ì˜ í›„ë³´êµ°(candidates) ì œí•œ ì´ìŠˆ ì ê²€** âœ… ì™„ë£Œ (2026-01-02)
  - í˜„ êµ¬ì¡°ëŠ” `script_score`ê°€ í…ìŠ¤íŠ¸ ì¿¼ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í›„ë³´êµ°ì„ ì œí•œí•  ìˆ˜ ìˆìŒ(semantic-only recall ì €í•˜ ê°€ëŠ¥).
  - ëª©í‘œ: ê¸°ë³¸ hybrid ì „ëµì„ "RRF ë˜ëŠ” 2-stage(ë²¡í„° í›„ë³´ + BM25 í›„ë³´ union â†’ merge)"ë¡œ ì „í™˜í•˜ëŠ” ë°©ì•ˆ ê²€í† /PoC.
  - ì™„ë£Œ ë‚´ìš©:
    - **RRFë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ë³€ê²½**: `EsHybridRetriever.use_rrf = True` (ê¸°ì¡´ False)
    - script_score vs RRF ë¹„êµ ë¶„ì„ ë¬¸ì„œí™”
    - í›„ë³´êµ° ì œí•œ ì´ìŠˆ í•´ê²°: ë²¡í„° ê²€ìƒ‰ê³¼ BM25 ê²€ìƒ‰ì´ ë…ë¦½ ì‹¤í–‰
  - ì˜ˆìƒ íš¨ê³¼: Semantic recall í–¥ìƒ (íŠ¹íˆ í•œêµ­ì–´ í˜•íƒœì†Œ ë³€í˜•, ë‹¤êµ­ì–´ ì§ˆì˜)
  - ë¡¤ë°± ê°€ëŠ¥: í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ì½”ë“œì—ì„œ `use_rrf=False` ì„¤ì •

### P1 â€” ê²€ìƒ‰ í’ˆì§ˆ ê°œì„ (ë¦¬ì½œ/ì •ë°€ë„)

- [ ] **í•œêµ­ì–´ analyzer(Nori) í™œì„±í™” ê³„íš ìˆ˜ë¦½ + ë¦¬ì¸ë±ì‹± ì ˆì°¨ ë§ˆë ¨**
  - ì‘ì—…:
    - ES nori plugin ì¤€ë¹„(ë„ì»¤ ì´ë¯¸ì§€/ë°°í¬ í™˜ê²½ í¬í•¨).
    - `content/search_text/chunk_summary/chunk_keywords`ì— ëŒ€í•´ nori ì ìš©(ë˜ëŠ” `standard` + `nori` ë©€í‹°í•„ë“œ ë³‘í–‰) ì„¤ê³„.
    - alias ë¡¤ë§ ì—…ë°ì´íŠ¸ë¡œ ì‹ ê·œ ì¸ë±ìŠ¤ ìƒì„±(v2) â†’ ì¬ìƒ‰ì¸ â†’ alias ìŠ¤ìœ„ì¹˜.
- [ ] **í•„ë“œ boost/ë©€í‹°í•„ë“œ ì „ëµ ì¬ì •ì˜**
  - ëª©í‘œ: `search_text` ì›íˆ´ì„ ë„˜ì–´, `content`, `chunk_summary`, `chunk_keywords`, (í•„ìš”ì‹œ) `title`/`doc_description`ì˜ ì—­í• ê³¼ weightë¥¼ ëª…í™•íˆ.
  - ì²´í¬í¬ì¸íŠ¸: â€œì¿¼ë¦¬ì— í¬í•¨í•˜ëŠ” í•„ë“œëŠ” ë°˜ë“œì‹œ index=trueâ€ ë³´ì¥(ë¶ˆí•„ìš” í•„ë“œëŠ” ì¿¼ë¦¬ì—ì„œ ì œê±°).
- [ ] **Reranking(cross-encoder) ì ìš© ë²”ìœ„/ì •ì±… ì •ì˜**
  - ëª©í‘œ: ìƒìœ„ Nê°œ í›„ë³´(ì˜ˆ: 20~50)ë¥¼ rerank í›„ top_k ë°˜í™˜í•˜ëŠ” ì •ì±…ì„ ì„¤ì •ìœ¼ë¡œ ì œì–´.
  - ê³ ë ¤ì‚¬í•­: latency/ë¹„ìš©/ìºì‹± ì „ëµ, GPU/CPU ë°°ì¹˜ í™˜ê²½, ì¥ì•  ì‹œ graceful fallback.
- [ ] **Multi-Query Expansion(MQE) ì ìš© ë²”ìœ„/ì •ì±… ì •ì˜**
  - ëª©í‘œ: â€œì–¸ì œ MQEë¥¼ ì¼¤ì§€â€ë¥¼ ëª…í™•íˆ(í•­ìƒ onì´ ì•„ë‹ˆë¼ íŠ¸ë¦¬ê±°/ê²Œì´íŒ… ê¸°ë°˜ ê¶Œì¥).
  - ì˜ˆ: ì§§ì€ ì§ˆì˜/ëª¨í˜¸ ì§ˆì˜/ë¼ìš°íŒ… ê²°ê³¼(ts/setup/general) ê¸°ë°˜ìœ¼ë¡œ MQE on.
- [ ] **ES ë°±ì—”ë“œì—ë„ MQE/Rerank í†µí•©(ë˜ëŠ” ê³µí†µ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì¼ì›í™”)**
  - ëª©í‘œ: `SEARCH_BACKEND=es`ì—ì„œë„ â€œí™•ì¥â†’(ë³µìˆ˜ ê²€ìƒ‰)â†’ë¨¸ì§€â†’(ì¬ë­í‚¹)â€ì„ ì„¤ì •ìœ¼ë¡œ ì˜¨/ì˜¤í”„ ê°€ëŠ¥í•˜ê²Œ.
  - ì²´í¬í¬ì¸íŠ¸: LangGraph agentê°€ MQ/rerankë¥¼ ìì²´ ìˆ˜í–‰í•˜ëŠ” ê²½ìš°ì—ëŠ” ì¤‘ë³µ ì‹¤í–‰ì´ ë°œìƒí•˜ì§€ ì•Šë„ë¡ ì •ì±…/í”Œë˜ê·¸ ì •ë¦¬.

### P1 â€” ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì •ë°€ë„ í–¥ìƒ(í•„í„°ë§/íƒ€ê²ŒíŒ…)

- [ ] **`/search` APIì— ë©”íƒ€ë°ì´í„° í•„í„° íŒŒë¼ë¯¸í„° ì¶”ê°€**
  - ëŒ€ìƒ: `doc_type`, `device_name`, `tenant_id`, `project_id`, `lang` ë“±.
  - ëª©í‘œ: UI/í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì†ì‰½ê²Œ ë²”ìœ„ë¥¼ ì¢í˜€ precision ê°œì„ .
- [ ] **ì§ˆì˜ì—ì„œ ì—”í‹°í‹°(ì¥ë¹„ëª…/ë¬¸ì„œíƒ€ì…/ì•ŒëŒì½”ë“œ ë“±) ì¶”ì¶œ â†’ ìë™ í•„í„° ì˜µì…˜í™”**
  - ëª©í‘œ: ì‚¬ìš©ìê°€ í•„í„°ë¥¼ ì§ì ‘ ê³ ë¥´ì§€ ì•Šì•„ë„, ëª…ì‹œëœ ì—”í‹°í‹°ê°€ ìˆìœ¼ë©´ ìë™ ì ìš©(ì˜µíŠ¸ì•„ì›ƒ ê°€ëŠ¥).

### P2 â€” â€œì„¤ì • ì£¼ë„(pluggable)â€ êµ¬ì¡°ë¡œ ë¦¬íŒ©í„°ë§

- [ ] **YAML/JSON ì„¤ì • íŒŒì¼ ê¸°ë°˜ êµ¬ì„± ë„ì…(í”„ë¦¬ì…‹ íŒŒì¼ ì„ íƒ)**
  - ëª©í‘œ: `.env`ë§Œìœ¼ë¡œëŠ” ê´€ë¦¬ê°€ ì–´ë ¤ìš´ â€œì¡°í•©(embedding+retrieval+MQE+rerank+agent)â€ì„ ë‹¨ì¼ íŒŒì¼ë¡œ ì •ì˜/ì „í™˜.
  - ì œì•ˆ: `RAG_PRESET_FILE=backend/config/presets/retrieval_full_pipeline.yaml` ë˜ëŠ” `RAG_PRESET_NAME=full_pipeline` ê°™ì€ í˜•íƒœë¡œ ëŸ°íƒ€ì„ ì„ íƒ.
  - ì²´í¬í¬ì¸íŠ¸: Pydantic Settings â†’ preset overlay(ë®ì–´ì“°ê¸°) ìš°ì„ ìˆœìœ„ ê·œì¹™(ENV > preset > defaults) ëª…ë¬¸í™”.
- [ ] **Preset ë‹¨ì¼í™”(ì½”ë“œ preset vs YAML preset) ë° ì‹¤ì œ ëŸ°íƒ€ì„ ì ìš©**
  - ëª©í‘œ: `RAG_RETRIEVAL_PRESET` í•˜ë‚˜ë¡œ ì•„ë˜ê°€ ì¼ê´€ë˜ê²Œ ê²°ì •ë˜ë„ë¡:
    - retriever ì¢…ë¥˜(local/es), hybrid ë°©ì‹(script_score vs rrf), top_k, weights, MQE, rerank.
  - ì‚°ì¶œë¬¼: preset ëª©ë¡/ì„¤ëª… ì¡°íšŒ ì—”ë“œí¬ì¸íŠ¸(ë˜ëŠ” CLI) + â€œí˜„ì¬ í™œì„± presetâ€ introspection.
- [ ] **LLM/Embedder/Reranker/QueryExpander ì„ íƒì„ ì„¤ì •ìœ¼ë¡œ ì™„ì „ ì™¸ë¶€í™”**
  - ëª©í‘œ: `RAG_LLM_METHOD/VERSION`, `RAG_RERANK_METHOD`, `RAG_QUERY_EXPAND_METHOD` ë“±ìœ¼ë¡œ ëŸ°íƒ€ì„ ì„ íƒ.
  - ì²´í¬í¬ì¸íŠ¸: FastAPI DI(`backend/api/dependencies.py`)ê°€ â€œê³ ì • êµ¬í˜„â€ì´ ì•„ë‹ˆë¼ â€œì„¤ì • ê¸°ë°˜ íŒ©í† ë¦¬â€ê°€ ë˜ë„ë¡ ì •ë¦¬.
- [ ] **ë™ì  ì¬êµ¬ì„±(ì‹¤í—˜ìš©) ì§€ì› ì—¬ë¶€ ê²°ì • + êµ¬í˜„**
  - ëª©í‘œ: â€œì¬ì‹œì‘ ì—†ì´â€ ë˜ëŠ” â€œìš”ì²­ ë‹¨ìœ„ë¡œâ€ ì—¬ëŸ¬ íŒŒì´í”„ë¼ì¸ì„ ë¹„êµ ì‹¤í—˜í•  ìˆ˜ ìˆê²Œ í• ì§€ ê²°ì •.
  - ì˜µì…˜:
    - ìš´ì˜: ì¬ì‹œì‘ ì „ì œ(í˜„ êµ¬ì¡° ìœ ì§€, ë¬¸ì„œí™” ê°•í™”)
    - ì‹¤í—˜: provider íŒ¨í„´(í‚¤ ê¸°ë°˜ ìºì‹œ + reload) + ê´€ë¦¬ìš© ì—”ë“œí¬ì¸íŠ¸ë¡œ ìºì‹œ ë¬´íš¨í™”
- [ ] **í† í¬ë‚˜ì´ì €/ë¶„ì„ê¸° ì„¤ì • ê²½ë¡œ ì •ë¦¬**
  - local BM25 tokenizer ì£¼ì… ê²½ë¡œë¥¼ â€œì„¤ì • â†’ í† í¬ë‚˜ì´ì € íŒ©í† ë¦¬/ë ˆì§€ìŠ¤íŠ¸ë¦¬â€ë¡œ í†µì¼.
  - chunkingì˜ `split_by=token`ì´ ì‹¤ì œë¡œ ë™ì‘í•˜ë„ë¡ tokenizer ì „ë‹¬(í•„ìš” ì‹œ embedderì˜ tokenizer ì¬ì‚¬ìš©).
  - ESëŠ” analyzerê°€ í† í¬ë‚˜ì´ì € ì—­í• ì„ í•˜ë¯€ë¡œ, â€œì–¸ì–´/ë¶„ì„ê¸° í”„ë¡œíŒŒì¼â€ë¡œ ê´€ë¦¬(standard/nori/synonyms).
- [ ] **ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ DSL/ìŠ¤í… ê¸°ë°˜ ì¡°ë¦½(ì„ íƒì‚¬í•­)**
  - ëª©í‘œ: â€œë‹¨ê³„ ì¶”ê°€/ìˆœì„œ ë³€ê²½/ì¡°ê±´ë¶€ ì‹¤í–‰â€ì„ ì½”ë“œ ìˆ˜ì • ì—†ì´ presetìœ¼ë¡œ ì‹¤í—˜ ê°€ëŠ¥í•˜ê²Œ.
  - ì‚°ì¶œë¬¼: `pipeline.steps`(preprocess/expand/embed/retrieve/merge/rerank ë“±) ìŠ¤í‚¤ë§ˆ + ì‹¤í–‰ê¸° + ë¡œê¹…/í”„ë¡œíŒŒì¼ë§.
- [ ] **Agent( LangGraph )ì™€ Search pipelineì˜ ì—­í•  ë¶„ë¦¬/í†µí•© ì •ì±… ëª…ë¬¸í™”**
  - ëª©í‘œ: â€œMQEëŠ” ê·¸ë˜í”„ì—ì„œë§Œ í•œë‹¤/ì„œì¹˜ì—ì„œë§Œ í•œë‹¤/ë‘˜ ë‹¤ í•œë‹¤(ë¹„ê¶Œì¥)â€ ì¤‘ ìš´ì˜ í‘œì¤€ì„ ê²°ì •í•˜ê³  ì„¤ì •ìœ¼ë¡œ ì œì–´.
  - ì¶”ê°€: agentìš© top_k/ëª¨ë“œ/ë¼ìš°íŒ… í”„ë¡¬í”„íŠ¸ ë²„ì „ë„ ì„¤ì •í™”.
- [ ] **Agent preset/config ë„ì…(íˆ´/ì›Œí¬í”Œë¡œìš°/í”„ë¡¬í”„íŠ¸ ë²„ì „ ì™¸ë¶€í™”)**
  - ëª©í‘œ: â€œì—ì´ì „íŠ¸ í”Œë¡œìš°/íˆ´ì²´ì¸â€ì„ í”„ë¦¬ì…‹ìœ¼ë¡œ ì„ íƒ ê°€ëŠ¥í•˜ê²Œ í•˜ì—¬ ìš´ì˜/ì‹¤í—˜ ì „í™˜ ë¹„ìš©ì„ ë‚®ì¶¤.

### P2 â€” ìš´ì˜/ìœ ì§€ë³´ìˆ˜/í‰ê°€

- [ ] **ì¸ë±ìŠ¤ ìœ ì§€ë³´ìˆ˜ ëŸ°ë¶ ì •ë¦¬**
  - alias ë¡¤ë§ ì—…ë°ì´íŠ¸ ì ˆì°¨(ìƒì„±â†’ì¬ìƒ‰ì¸â†’alias switchâ†’ê²€ì¦â†’ë¡¤ë°±).
  - `_meta(pipeline)`ë¥¼ í™œìš©í•œ â€œì–´ë–¤ ì„¤ì •ìœ¼ë¡œ ë§Œë“  ì¸ë±ìŠ¤ì¸ì§€â€ ì¶”ì  í‘œì¤€í™”.
- [ ] **íšŒê·€ í‰ê°€ ì„¸íŠ¸(queries + expected evidence) êµ¬ì¶•**
  - ëª©í‘œ: analyzer ë³€ê²½, weights ë³€ê²½, MQE/rerank ë„ì… ì‹œ í’ˆì§ˆ íšŒê·€ë¥¼ ìë™ ê°ì§€.
  - ì‚°ì¶œë¬¼: ìµœì†Œ 30~100ê°œ ì§ˆì˜(ì„¤ì¹˜/TS/ì¼ë°˜) + ê¸°ëŒ€ top docs ë˜ëŠ” ìµœì†Œ í¬í•¨ ì¡°ê±´.
- [ ] **ì„±ëŠ¥/ë¹„ìš© ë²¤ì¹˜ë§ˆí¬**
  - MQE on/off, rerank on/off, RRF vs script_score ë“± ì¡°í•©ë³„ latency/throughput ì¸¡ì •.

---

## 2) ë¹ ë¥¸ ì²´í¬ë¦¬ìŠ¤íŠ¸(ì‹¤í–‰ ì „ í™•ì¸)

- [ ] ES ë²„ì „/í”ŒëŸ¬ê·¸ì¸(nori) ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
- [ ] í˜„ì¬ aliasê°€ ê°€ë¦¬í‚¤ëŠ” ì¸ë±ìŠ¤ì˜ mapping/settings ìŠ¤ëƒ…ìƒ· í™•ë³´
- [ ] ì„ë² ë”© ëª¨ë¸ ë³€ê²½ ì‹œ: dimension ë³€ê²½ â†’ ì‹ ê·œ ì¸ë±ìŠ¤(vNext) ìƒì„± í›„ reindexê°€ í•„ìš”í•œì§€ í™•ì¸
- [ ] MQE/rerank ë„ì… ì‹œ: p95 latency ëª©í‘œ/ë¦¬ì†ŒìŠ¤ ì˜ˆì‚°(GPU/CPU) ì •ì˜

---

## 3) ìƒì„¸ ì‹¤í–‰ ê³„íš (ë¦¬íŠ¸ë¦¬ë²Œ ì•„í‚¤í…ì²˜ ë¦¬ë·° ë°˜ì˜)

> **ê¸°ë°˜ ë¬¸ì„œ**: ë¦¬íŠ¸ë¦¬ë²Œ ì•„í‚¤í…ì²˜ ì§„ë‹¨ ë° ê°œì„  (8ê°€ì§€ ì´ìŠˆ)
> **ì‘ì„±ì¼**: 2026-01-02

### ğŸ“Š í˜„ì¬ ì‹œìŠ¤í…œ ì§„ë‹¨ ìš”ì•½

**ê²€ìƒ‰ êµ¬ì„±**:
- ë°±ì—”ë“œ: Elasticsearch 8.x
- ë°©ì‹: Hybrid Search (Dense kNN + BM25)
- ì„ë² ë”©: 768-dim (ì¶”ì •: BGE-base ë˜ëŠ” KoE5)
- ì¸ë±ìŠ¤/alias: `rag_chunks_dev_current` (alias ì—¬ë¶€ í™•ì¸ í•„ìš”; `_cat/aliases`ë¡œ ê²€ì¦)

**ì£¼ìš” í•„ë“œ**:
```
âœ… embedding (dense_vector, cosine, 768 dims, int8_hnsw)
âœ… search_text (text, standard analyzer) - ë³µí•© í•„ë“œ (content+title+tags)
âœ… content (text, standard analyzer)
âœ… chunk_summary (text) - ê²€ìƒ‰ ê°€ëŠ¥ (ìŠ¤ëƒ…ìƒ· ê¸°ì¤€)
âœ… doc_description (text) - ê²€ìƒ‰ ê°€ëŠ¥ (ìŠ¤ëƒ…ìƒ· ê¸°ì¤€)
âœ… chunk_keywords (text + keyword ë©€í‹°í•„ë“œ) - ê²€ìƒ‰ ê°€ëŠ¥ (ìŠ¤ëƒ…ìƒ· ê¸°ì¤€)
```

**ë¬¸ì œì **:
1. í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ ë¯¸ì ìš© (Nori analyzer ì£¼ì„ ì²˜ë¦¬)
2. ìš”ì•½/í‚¤ì›Œë“œ/ì„¤ëª… í•„ë“œ í™œìš©(ë¶€ìŠ¤íŠ¸/ë…¸ì´ì¦ˆ/í•„ë“œëª… ì •í•©ì„±) ì¬ì ê²€ í•„ìš”
3. Multi-Query Expansion ë¹„í™œì„±í™”
4. Reranking ë¹„í™œì„±í™”
5. ë™ì˜ì–´ ì‚¬ì „ ë¯¸êµ¬ì¶•
6. í•˜ì´ë¸Œë¦¬ë“œ ê°€ì¤‘ì¹˜ ê³ ì • (dynamic weighting ë¯¸ì ìš©)

---

### ğŸ”´ Phase 1: í•œêµ­ì–´ ì²˜ë¦¬ ë° ì¸ë±ì‹± ìµœì í™” (HIGH PRIORITY)

#### [TODO-1.1] Nori Analyzer í™œì„±í™” ë° ë¦¬ì¸ë±ì‹±

**ëª©í‘œ**: Recall +15% (í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„)

**ë°°ê²½**:
```
í˜„ì¬: "ì¥ë¹„ë¥¼ ê°€ë™í–ˆë‹¤" â‰  "ì¥ë¹„ ê°€ë™" (ë§¤ì¹­ ì‹¤íŒ¨)
ê°œì„  í›„: "ì¥ë¹„", "ê°€ë™" í˜•íƒœì†Œ ì¶”ì¶œ â†’ ë§¤ì¹­ ì„±ê³µ
```

**Step 1: Nori Plugin ì„¤ì¹˜ í™•ì¸**
```bash
# ES ì»¨í…Œì´ë„ˆì—ì„œ í”ŒëŸ¬ê·¸ì¸ í™•ì¸
docker exec -it <es-container> bin/elasticsearch-plugin list

# nori ë¯¸ì„¤ì¹˜ ì‹œ
docker exec -it <es-container> bin/elasticsearch-plugin install analysis-nori
```

- [ ] Docker Composeì— í”ŒëŸ¬ê·¸ì¸ ìë™ ì„¤ì¹˜ ì¶”ê°€
  ```yaml
  # docker-compose.yml
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    environment:
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms4g -Xmx4g"
    command: >
      sh -c "bin/elasticsearch-plugin install analysis-nori &&
             bin/elasticsearch"
  ```

**Step 2: ì¸ë±ìŠ¤ ë§¤í•‘ ìˆ˜ì •**

- [ ] `backend/llm_infrastructure/elasticsearch/mappings.py:187` ìˆ˜ì •
  ```python
  def get_index_settings(...):
      return {
          "analysis": {
              "analyzer": {
                  "nori_analyzer": {
                      "type": "custom",
                      "tokenizer": "nori_tokenizer",
                      "filter": [
                          "nori_readingform",  # í•œì â†’ í•œê¸€ ë³€í™˜
                          "lowercase",
                          "nori_part_of_speech",  # í’ˆì‚¬ í•„í„°
                      ],
                  }
              },
              "filter": {
                  "nori_part_of_speech": {
                      "type": "nori_part_of_speech",
                      "stoptags": ["E", "IC", "J", "MAG", "MM", "SP", "SSC", "SSO", "SC", "SE", "XPN", "XSA", "XSN", "XSV", "UNA", "NA", "VSV"]
                  }
              }
          }
      }
  ```

- [ ] í…ìŠ¤íŠ¸ í•„ë“œì— analyzer ì ìš© (mappings.py:44, 50)
  ```python
  "content": {
      "type": "text",
      "analyzer": "nori_analyzer",
  },
  "search_text": {
      "type": "text",
      "analyzer": "nori_analyzer",
  }
  ```

**Step 3: ë¦¬ì¸ë±ì‹± ì ˆì°¨**

- [ ] ë¦¬ì¸ë±ì‹± ìŠ¤í¬ë¦½íŠ¸ ì‘ì„± (`scripts/es_reindex_with_nori.py`)
  ```python
  # 1. ìƒˆ ì¸ë±ìŠ¤ ìƒì„± (v2)
  index_name = f"{prefix}_{env}_v2"
  es.indices.create(index=index_name, body={
      "settings": get_index_settings(...),  # with nori
      "mappings": get_rag_chunks_mapping(dims=768),
  })

  # 2. ë°ì´í„° ë³µì‚¬
  helpers.reindex(
      es,
      source_index=f"{prefix}_{env}_v1",
      target_index=index_name,
      chunk_size=500,
  )

  # 3. Alias ì „í™˜
  es.indices.update_aliases(body={
      "actions": [
          {"remove": {"index": f"{prefix}_{env}_v1", "alias": f"{prefix}_{env}_current"}},
          {"add": {"index": index_name, "alias": f"{prefix}_{env}_current"}},
      ]
  })
  ```

- [ ] ë¡¤ë°± ê³„íš ìˆ˜ë¦½
  ```bash
  # Aliasë¥¼ v1ë¡œ ë˜ëŒë¦¬ê¸°
  python scripts/es_index_manager.py rollback --from v2 --to v1
  ```

**Step 4: A/B í…ŒìŠ¤íŠ¸**

- [ ] í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì„¸íŠ¸ ì¤€ë¹„
  ```python
  test_cases = [
      # (ì¿¼ë¦¬, ì˜ˆìƒ ë§¤ì¹­ ë¬¸ì„œ í‚¤ì›Œë“œ)
      ("ì¥ë¹„ë¥¼ ê°€ë™í–ˆë‹¤", ["ì¥ë¹„ ê°€ë™", "ì¥ë¹„ ì‹œë™"]),
      ("ì„¼ì„œë¥¼ êµì²´í–ˆìŠµë‹ˆë‹¤", ["ì„¼ì„œ êµì²´", "ì„¼ì„œ ì¥ì°©"]),
      ("ì±”ë²„ ì²­ì†Œ ì ˆì°¨", ["ì±”ë²„ í´ë¦¬ë‹", "ì±”ë²„ë¥¼ ì²­ì†Œ"]),
      ("íŒí”„ê°€ ì‘ë™í•˜ì§€ ì•Šì•„ìš”", ["íŒí”„ ì‘ë™ ë¶ˆëŸ‰", "íŒí”„ ê³ ì¥"]),
  ]
  ```

- [ ] Recall@10 ë¹„êµ
  ```python
  for query, keywords in test_cases:
      # v1 (standard) ê²€ìƒ‰
      results_v1 = search_index_v1(query, top_k=10)
      recall_v1 = calculate_recall(results_v1, keywords)

      # v2 (nori) ê²€ìƒ‰
      results_v2 = search_index_v2(query, top_k=10)
      recall_v2 = calculate_recall(results_v2, keywords)

      print(f"{query}: v1={recall_v1:.2f}, v2={recall_v2:.2f}")
  ```

**ì˜ˆìƒ íš¨ê³¼**: Recall +15%, í•œêµ­ì–´ ë™ì‚¬/í˜•ìš©ì‚¬ í™œìš©í˜• ì¿¼ë¦¬ ëŒ€ì‘

**ì†Œìš” ì‹œê°„**: 2-3ì¼

**íŒŒì¼**:
- `docker-compose.yml`
- `backend/llm_infrastructure/elasticsearch/mappings.py:187`
- `scripts/es_reindex_with_nori.py` (ì‹ ê·œ)

---

#### [TODO-1.2] chunk_summary í•„ë“œ ê²€ìƒ‰ í™œì„±í™”

**ëª©í‘œ**: LLM ìƒì„± ìš”ì•½ì„ BM25 ê²€ìƒ‰ì— í™œìš©

**ë°°ê²½**:
- ìŠ¤ëƒ…ìƒ· ê¸°ì¤€ `chunk_summary`ëŠ” `text`ë¡œ ê²€ìƒ‰ ê°€ëŠ¥(â€œê²€ìƒ‰ ë¶ˆê°€â€ ìƒíƒœëŠ” ì•„ë‹˜)
- ë‹¤ë§Œ (1) í•œêµ­ì–´ ë¶„ì„ê¸° ì ìš© ì—¬ë¶€, (2) ì¿¼ë¦¬ ë¶€ìŠ¤íŠ¸/ë…¸ì´ì¦ˆ, (3) `search_text` í¬í•¨ ì—¬ë¶€ë¥¼ ì •ë¦¬í•  í•„ìš”ê°€ ìˆìŒ

**ì‘ì—…**:

1. **ë§¤í•‘ ìˆ˜ì •** (mappings.py:117)
   ```python
   "chunk_summary": {
       "type": "text",
       "index": True,
       "analyzer": "nori",  # (ë˜ëŠ” multi-fieldë¡œ nori ë³‘í–‰)
   },
   ```
   - [ ] ë§¤í•‘ ìˆ˜ì • (TODO-1.1ê³¼ í•¨ê»˜ v2 ì¸ë±ìŠ¤ì— ì ìš©)

2. **BM25 ì¿¼ë¦¬ì— í•„ë“œ ì¶”ê°€** (es_search.py:121)
   ```python
   text_fields=[
       "search_text^1.0",
       "chunk_summary^0.7",  # ì¶”ê°€
       "chunk_keywords^0.8",
   ]
   ```
   - [ ] EsSearchEngine ê¸°ë³¸ text_fields ìˆ˜ì •

3. **Boost íŠœë‹**
   - [ ] ì´ˆê¸°ê°’: `chunk_summary^0.5` (ë³´ìˆ˜ì  ì‹œì‘)
   - [ ] A/B í…ŒìŠ¤íŠ¸: 0.5 â†’ 0.7 â†’ 1.0
   - [ ] ë…¸ì´ì¦ˆ ë°œìƒ ì‹œ ê°€ì¤‘ì¹˜ í•˜í–¥ ë˜ëŠ” ë¹„í™œì„±í™”

**ì£¼ì˜ì‚¬í•­**:
- ìš”ì•½ í’ˆì§ˆ ê²€ì¦ í•„ìš” (LLM hallucination ê°€ëŠ¥ì„±)
- ë„ˆë¬´ ë†’ì€ boostëŠ” ì›ë³¸ content ì••ë„í•  ìˆ˜ ìˆìŒ

**ì†Œìš” ì‹œê°„**: 1ì¼ (TODO-1.1ê³¼ ë³‘í–‰)

**íŒŒì¼**:
- `backend/llm_infrastructure/elasticsearch/mappings.py:117`
- `backend/llm_infrastructure/retrieval/engines/es_search.py:121`

---

#### [TODO-1.3] ì„ë² ë”© ì°¨ì› ë¶ˆì¼ì¹˜ í•´ê²°

**ëª©í‘œ**: ì¸ë±ìŠ¤ ë§¤í•‘ â†” ì„ë² ë”© ëª¨ë¸ ì°¨ì› 100% ë™ê¸°í™”

**í˜„ì¬ ë¬¸ì œ**:
- `.env`: `SEARCH_ES_EMBEDDING_DIMS=768`
- `mappings.py` ê¸°ë³¸ê°’: `dims=1024`
- ë¶ˆì¼ì¹˜ ì‹œ ì¸ì œìŠ¤ì…˜ ì‹¤íŒ¨ ê°€ëŠ¥

**ê²€ì¦ ì ˆì°¨**:

1. **ì‹¤ì œ ì°¨ì› í™•ì¸**
   ```python
   from backend.services.embedding_service import EmbeddingService
   from backend.config.settings import rag_settings, search_settings

   print(f"ì„¤ì • method: {rag_settings.embedding_method}")
   print(f"í™˜ê²½ë³€ìˆ˜ dims: {search_settings.es_embedding_dims}")

   svc = EmbeddingService()
   actual_dim = svc.dimension()
   print(f"ì‹¤ì œ ì¶œë ¥ dims: {actual_dim}")
   ```
   - [ ] ì°¨ì› í™•ì¸ ë° ë¬¸ì„œí™”

2. **ES ë§¤í•‘ í™•ì¸**
   ```bash
   curl -X GET "http://localhost:8002/rag_chunks_dev_current/_mapping" | \
     jq '.[] | .mappings.properties.embedding.dims'
   ```
   - [ ] ê²°ê³¼ ì €ì¥: `docs/es_current_mapping_snapshot.json`

3. **ë¶ˆì¼ì¹˜ ì‹œ í•´ê²°**
   - [ ] **Option A**: ë§¤í•‘ ìˆ˜ì • (v2 ì¸ë±ìŠ¤ì—ì„œ 768 ì ìš©)
   - [ ] **Option B**: ì„ë² ë”© ëª¨ë¸ êµì²´ (1024-dim ëª¨ë¸ ì‚¬ìš©)
     - KoE5 large ë˜ëŠ” multilingual-e5-large

4. **ì¼ê´€ì„± ì²´í¬ ê°•í™”**
   ```python
   # backend/services/es_ingest_service.py:243
   def _validate_embedding_dimension(self, embeddings: np.ndarray):
       actual_dim = embeddings.shape[1]
       expected_dim = search_settings.es_embedding_dims

       if actual_dim != expected_dim:
           raise ValueError(
               f"âŒ Embedding dimension mismatch!\n"
               f"   Actual:   {actual_dim}\n"
               f"   Expected: {expected_dim}\n"
               f"   Fix: Check RAG_EMBEDDING_METHOD and SEARCH_ES_EMBEDDING_DIMS"
           )
   ```
   - [ ] ì¸ì œìŠ¤ì…˜ ì‹œì‘ ì‹œ ê²€ì¦ ì¶”ê°€
   - [ ] Health checkì— ì°¨ì› ê²€ì¦ ì¶”ê°€

**ì†Œìš” ì‹œê°„**: 0.5ì¼

**íŒŒì¼**:
- `.env:52`
- `backend/llm_infrastructure/elasticsearch/mappings.py:61`
- `backend/services/es_ingest_service.py:243`

---

### ğŸŸ¡ Phase 2: ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ (MEDIUM PRIORITY)

#### [TODO-2.1] Cross-Encoder Reranking í™œì„±í™”

**ëª©í‘œ**: Precision@5 +20%

**ì‘ì—…**:

1. **í™˜ê²½ë³€ìˆ˜ ì„¤ì •**
   ```bash
   # .env ì¶”ê°€
   RAG_RERANK_ENABLED=true
   RAG_RERANK_METHOD=cross_encoder
   RAG_RERANK_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2
   RAG_RERANK_TOP_K=5
   ```
   - [ ] `.env` ìˆ˜ì •

2. **ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸**
   ```python
   from backend.api.dependencies import get_reranker
   reranker = get_reranker()

   # ìƒ˜í”Œ ë¦¬ë­í‚¹
   query = "EFEM ì„¼ì„œ ì˜¤ë¥˜ í•´ê²°"
   results = [...] # ê²€ìƒ‰ ê²°ê³¼ (top 20)
   reranked = reranker.rerank(query, results, top_k=5)

   # ì¶”ë¡  ì†ë„ ì¸¡ì •
   import time
   start = time.time()
   reranker.rerank(query, results, top_k=5)
   print(f"Rerank time: {(time.time() - start)*1000:.1f}ms")
   ```
   - [ ] GPU/CPU í™˜ê²½ë³„ ì†ë„ ì¸¡ì •

3. **ES ê²½ë¡œì— reranking í†µí•©**
   - [ ] `backend/services/es_search_service.py` ìˆ˜ì • í•„ìš” í™•ì¸
   ```python
   def search(self, query: str, **kwargs):
       results = self.retriever.retrieve(...)

       # Reranking ì¶”ê°€
       if rag_settings.rerank_enabled:
           reranker = get_reranker()
           results = reranker.rerank(query, results, top_k=rag_settings.rerank_top_k)

       return results
   ```

4. **ì„±ëŠ¥ í‰ê°€**
   - [ ] Precision@5 ì¸¡ì •
   - [ ] MRR (Mean Reciprocal Rank) ì¸¡ì •
   - [ ] NDCG@5 ì¸¡ì •
   - [ ] P95 latency ì¸¡ì • (ëª©í‘œ: <500ms)

5. **ì¥ì•  ëŒ€ì‘**
   ```python
   try:
       results = reranker.rerank(query, results, top_k=5)
   except Exception as e:
       logger.warning(f"Reranking failed: {e}")
       results = results[:5]  # Fallback to top-5
   ```
   - [ ] Graceful degradation êµ¬í˜„

**ì†Œìš” ì‹œê°„**: 1-2ì¼

**íŒŒì¼**:
- `.env`
- `backend/services/es_search_service.py`

---

#### [TODO-2.2] Multi-Query Expansion ì¡°ê±´ë¶€ í™œì„±í™”

**ëª©í‘œ**: ëª¨í˜¸í•œ ì§ˆì˜ì˜ recall í–¥ìƒ (ë¹„ìš© ìµœì†Œí™”)

**ì „ëµ**: ëª¨ë“  ì§ˆì˜ì— MQE ì ìš© ì‹œ ì§€ì—°/ë¹„ìš© ì¦ê°€ â†’ ì„ íƒì  íŠ¸ë¦¬ê±°

**ì‘ì—…**:

1. **ì§ˆì˜ ë¶„ë¥˜ê¸° êµ¬í˜„**
   ```python
   # backend/services/query_analyzer.py (ì‹ ê·œ)
   import re

   def should_expand_query(query: str) -> bool:
       """MQE íŠ¸ë¦¬ê±° ì—¬ë¶€ íŒë‹¨"""
       tokens = query.split()

       # ì§§ì€ ì§ˆì˜ (< 3 ë‹¨ì–´)
       if len(tokens) < 3:
           return True

       # ì§ˆë¬¸í˜•
       if any(q in query for q in ['?', 'ì™œ', 'ì–´ë–»ê²Œ', 'ë¬´ì—‡', 'ì–¸ì œ']):
           return True

       # ì—ëŸ¬ ì½”ë“œë§Œ (ì˜ˆ: "EFEM-1234")
       if re.match(r'^[A-Z0-9\-]+$', query.strip()):
           return True

       return False
   ```
   - [ ] ë¶„ë¥˜ê¸° êµ¬í˜„ ë° ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

2. **SearchServiceì— ì¡°ê±´ë¶€ ë¡œì§ ì¶”ê°€**
   ```python
   # backend/services/search_service.py
   from backend.services.query_analyzer import should_expand_query

   def search(self, query: str, **kwargs):
       # ì¡°ê±´ë¶€ MQE
       if self.multi_query_enabled and should_expand_query(query):
           queries = self.query_expander.expand(query, n=2)
           logger.info(f"âœ“ MQE triggered: {len(queries)} queries")
       else:
           queries = [query]

       # ê²€ìƒ‰ ë° ë³‘í•©
       all_results = []
       for q in queries:
           results = self.retriever.retrieve(q, **kwargs)
           all_results.append((q, results))

       # RRFë¡œ ë³‘í•©
       final_results = self._merge_with_rrf(all_results)
       return final_results
   ```
   - [ ] ì¡°ê±´ë¶€ ë¡œì§ êµ¬í˜„

3. **ëª¨ë‹ˆí„°ë§**
   - [ ] MQE íŠ¸ë¦¬ê±° ë¹„ìœ¨ ë¡œê¹…
   - [ ] í™•ì¥ ì¿¼ë¦¬ë‹¹ ì§€ì—° ì‹œê°„ ì¸¡ì •
   - [ ] ì›”ë³„ LLM API ë¹„ìš© ì¶”ì 

**Alternative**: UIì— "í™•ì¥ ê²€ìƒ‰" í† ê¸€ ì œê³µ

**ì†Œìš” ì‹œê°„**: 2ì¼

**íŒŒì¼**:
- `backend/services/query_analyzer.py` (ì‹ ê·œ)
- `backend/services/search_service.py`

---

#### [TODO-2.3] ë™ì  í•˜ì´ë¸Œë¦¬ë“œ ê°€ì¤‘ì¹˜ ì¡°ì •

**ëª©í‘œ**: ì§ˆì˜ íƒ€ì…ë³„ ìµœì  dense/sparse ë¹„ìœ¨ ì ìš©

**í˜„ì¬ ë¬¸ì œ**: ëª¨ë“  ì§ˆì˜ì— `dense=0.7, sparse=0.3` ê³ ì •

**ì‘ì—…**:

1. **ê°€ì¤‘ì¹˜ ì „ëµ êµ¬í˜„**
   ```python
   # backend/services/query_analyzer.py
   def get_hybrid_weights(query: str) -> tuple[float, float]:
       """ì§ˆì˜ íŠ¹ì„±ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ ë°˜í™˜

       Returns:
           (dense_weight, sparse_weight)
       """
       tokens = query.split()

       # ì—ëŸ¬ ì½”ë“œ: BM25 ìš°ì„ 
       if re.search(r'\b[A-Z]{2,}-?\d{3,}\b', query):
           return (0.3, 0.7)  # BM25 ìš°ì„ 

       # ìì—°ì–´ ì§ˆë¬¸: Dense ìš°ì„ 
       elif len(tokens) > 5 and any(q in query for q in ['?', 'ì™œ', 'ì–´ë–»ê²Œ']):
           return (0.8, 0.2)  # ì˜ë¯¸ ê²€ìƒ‰ ìš°ì„ 

       # ê¸°ë³¸ê°’
       else:
           return (0.7, 0.3)
   ```
   - [ ] ì „ëµ êµ¬í˜„

2. **EsHybridRetrieverì— í†µí•©**
   ```python
   # backend/llm_infrastructure/retrieval/adapters/es_hybrid.py:143
   from backend.services.query_analyzer import get_hybrid_weights

   def retrieve(self, query: str, **kwargs):
       # ë™ì  ê°€ì¤‘ì¹˜
       dense_w, sparse_w = get_hybrid_weights(query)
       logger.debug(f"Dynamic weights: dense={dense_w}, sparse={sparse_w}")

       # ... (ê¸°ì¡´ ë¡œì§ì—ì„œ ê°€ì¤‘ì¹˜ë§Œ êµì²´)
       hits = self.es_engine.hybrid_search(
           ...,
           dense_weight=dense_w,
           sparse_weight=sparse_w,
       )
       return [hit.to_retrieval_result() for hit in hits]
   ```
   - [ ] ê°€ì¤‘ì¹˜ ì£¼ì…

3. **A/B í…ŒìŠ¤íŠ¸**
   ```python
   test_cases = [
       ("EFEM-1234 ì•ŒëŒ í•´ê²°", "BM25 ìš°ì„ "),  # â†’ 0.3, 0.7
       ("ì™œ ì˜¨ë„ê°€ ì•ˆ ì˜¬ë¼ê°€ë‚˜ìš”?", "Dense ìš°ì„ "),  # â†’ 0.8, 0.2
       ("ì„¼ì„œ êµì²´ ë°©ë²•", "ê¸°ë³¸ê°’"),  # â†’ 0.7, 0.3
   ]
   ```
   - [ ] NDCG@10 ë¹„êµ

**ì†Œìš” ì‹œê°„**: 2ì¼

**íŒŒì¼**:
- `backend/services/query_analyzer.py`
- `backend/llm_infrastructure/retrieval/adapters/es_hybrid.py`

---

### ğŸŸ¢ Phase 3: ê³ ê¸‰ ê¸°ëŠ¥ (LOW PRIORITY)

#### [TODO-3.1] ë©”íƒ€ë°ì´í„° í•„í„° UI/API í†µí•©

**ëª©í‘œ**: Precision í–¥ìƒ

**ì‘ì—…**:

1. **API íŒŒë¼ë¯¸í„° ì¶”ê°€**
   ```python
   # backend/api/routers/search.py
   @router.get("")
   async def search(
       q: str,
       doc_type: Optional[str] = Query(None, description="sop|maintenance|setup"),
       device_name: Optional[str] = Query(None, description="SUPRA|EFEM|..."),
       ...
   ):
       results = search_service.search(
           q,
           doc_type=doc_type,
           device_name=device_name,
       )
   ```
   - [ ] API ìˆ˜ì •

2. **í•„í„° ì˜µì…˜ ì§‘ê³„ API**
   ```python
   @router.get("/filters/device_names")
   async def get_device_names():
       # ES aggregation
       agg_result = es.search(
           index=index,
           body={"size": 0, "aggs": {"devices": {"terms": {"field": "device_name.keyword"}}}}
       )
       return [b["key"] for b in agg_result["aggregations"]["devices"]["buckets"]]
   ```
   - [ ] ì§‘ê³„ API ì¶”ê°€

3. **ìë™ í•„í„° ì¶”ì¶œ**
   ```python
   DEVICE_NAMES = ['SUPRA', 'EFEM', 'PRECIA']

   def extract_device_filter(query: str) -> str | None:
       for device in DEVICE_NAMES:
           if device.upper() in query.upper():
               return device
       return None
   ```
   - [ ] ìë™ ì¶”ì¶œ ë¡œì§ (ì„ íƒì )

**ì†Œìš” ì‹œê°„**: 2-3ì¼

---

#### [TODO-3.2] RRF vs Script Score ì‹¤í—˜

**ëª©í‘œ**: ê°€ì¤‘ì¹˜ íŠœë‹ ì—†ì´ í•˜ì´ë¸Œë¦¬ë“œ ê²°í•© ê°œì„ 

**ì‘ì—…**:

1. **RRF í™œì„±í™”**
   ```bash
   RAG_HYBRID_USE_RRF=true
   RAG_HYBRID_RRF_K=60
   ```

2. **ì„±ëŠ¥ ë¹„êµ**
   ```python
   configs = [
       {"method": "script_score", "dense": 0.7, "sparse": 0.3},
       {"method": "script_score", "dense": 0.5, "sparse": 0.5},
       {"method": "rrf", "rrf_k": 60},
   ]

   for config in configs:
       # NDCG, Precision, Recall ì¸¡ì •
       ...
   ```
   - [ ] í‰ê°€ ì‹¤í–‰

3. **ê²°ê³¼ ê¸°ë°˜ ì„ íƒ**
   - [ ] RRFê°€ ìš°ìˆ˜í•˜ë©´ ê¸°ë³¸ê°’ ë³€ê²½
   - [ ] ì•„ë‹ˆë©´ í˜„ì¬ ìœ ì§€

**ì†Œìš” ì‹œê°„**: 1ì¼

---

#### [TODO-3.3] ë™ì˜ì–´ ì‚¬ì „ êµ¬ì¶• (ë„ë©”ì¸ íŠ¹í™”)

**ëª©í‘œ**: ë°˜ë„ì²´ ì¥ë¹„ ìš©ì–´ ì •ê·œí™”

**ì‘ì—…**:

1. **ìš©ì–´ ìˆ˜ì§‘**
   ```
   # config/synonyms/semiconductor.txt
   EFEM, efem, Equipment Front End Module
   PM, pm, Preventive Maintenance, ì˜ˆë°© ì •ë¹„
   RF, rf, Radio Frequency
   ```
   - [ ] ìµœì†Œ 50ê°œ ìš©ì–´ ìˆ˜ì§‘

2. **Nori analyzerì— ì ìš©**
   ```python
   "filter": [
       "nori_readingform",
       "lowercase",
       {
           "type": "synonym",
           "synonyms_path": "config/synonyms/semiconductor.txt",
       }
   ]
   ```
   - [ ] ë§¤í•‘ ìˆ˜ì • ë° ë¦¬ì¸ë±ì‹±

**ì†Œìš” ì‹œê°„**: 2-3ì¼ (ìˆ˜ì§‘ ì‹œê°„ í¬í•¨)

---

### ğŸ“Š Timeline ë° ìš°ì„ ìˆœìœ„

#### Week 1-2: Critical Path
```
Day 1-2:  [TODO-1.3] ì„ë² ë”© ì°¨ì› ê²€ì¦
Day 3-6:  [TODO-1.1] Nori analyzer + [TODO-1.2] chunk_summary
Day 7-10: ë¦¬ì¸ë±ì‹± ë° A/B í…ŒìŠ¤íŠ¸
```

#### Week 3: Search Quality
```
Day 11-12: [TODO-2.1] Reranking
Day 13-15: [TODO-2.2] ì¡°ê±´ë¶€ MQE
```

#### Week 4: Advanced
```
Day 16-17: [TODO-2.3] ë™ì  ê°€ì¤‘ì¹˜
Day 18-20: [TODO-3.1] ë©”íƒ€ë°ì´í„° í•„í„°
Day 21:    [TODO-3.2] RRF ì‹¤í—˜
```

---

### ğŸ“ˆ ì˜ˆìƒ ì„±ëŠ¥ ê°œì„ 

| ì§€í‘œ | í˜„ì¬ | ëª©í‘œ | ì£¼ìš” ê°œì„  ì‚¬í•­ |
|------|------|------|----------------|
| Recall@10 | ê¸°ì¤€ | +15% | Nori analyzer |
| Precision@5 | ê¸°ì¤€ | +20% | Cross-encoder reranking |
| NDCG@10 | ê¸°ì¤€ | +10% | ì „ì²´ ê°œì„  |
| ì‘ë‹µ ì‹œê°„ (P95) | ? | <500ms | Reranking í¬í•¨ |
| ì œë¡œ ê²°ê³¼ ë¹„ìœ¨ | ? | -30% | MQE + ë™ì˜ì–´ |

---

---

## 4) Nori í™œì„±í™” ìƒì„¸ êµ¬í˜„ ê³„íš (2026-01-02 ì¶”ê°€)

> **ëª©í‘œ**: í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ìœ¼ë¡œ Recall +15% í–¥ìƒ
> **ì‘ì—…ì¼**: 2026-01-02
> **ìš°ì„ ìˆœìœ„**: P1 (High Priority)

### ğŸ“‹ í˜„ì¬ ìƒíƒœ ë¶„ì„

**ë°œê²¬ëœ ì‚¬ì‹¤**:
1. **Docker (docker-compose.yml:55)**:
   - Nori plugin ë¯¸ì„¤ì¹˜ (ì£¼ì„ë§Œ ì¡´ì¬: `# For Korean analysis, consider building custom image with nori plugin`)
   - ES 8.14.0 ì´ë¯¸ì§€ ì‚¬ìš© ì¤‘

2. **Mappings (backend/llm_infrastructure/elasticsearch/mappings.py)**:
   - Nori analyzer ì„¤ì • ì£¼ì„ ì²˜ë¦¬ (line 191-199)
   - í…ìŠ¤íŠ¸ í•„ë“œë“¤ì´ ëª¨ë‘ `standard` analyzer ì‚¬ìš©:
     - `content` (line 44-48)
     - `search_text` (line 50-55)
   - `chunk_summary`: index=Trueì´ì§€ë§Œ analyzer ë¯¸ì§€ì • (ê¸°ë³¸ standard ì ìš©)
   - `chunk_keywords.text`: standard analyzer

3. **ì¸ë±ìŠ¤ ê´€ë¦¬**:
   - âœ… EsIndexManagerê°€ alias ì „ëµ ì™„ë²½ ì§€ì›
   - âœ… validate_dims ê¸°ëŠ¥ êµ¬í˜„ë¨
   - âœ… ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ ì¡´ì¬: `scripts/migrate_to_alias_strategy.py`

4. **í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰**:
   - âœ… RRF ê¸°ë³¸ í™œì„±í™” (`use_rrf=True`, es_hybrid.py:65)
   - âœ… script_score í›„ë³´êµ° ì œí•œ ì´ìŠˆ í•´ê²°ë¨

### ğŸ¯ êµ¬í˜„ ê³„íš (4ë‹¨ê³„)

---

#### **Phase 1: Nori Plugin ì„¤ì¹˜ (Docker)**

**íŒŒì¼**: `docker-compose.yml`

**í˜„ì¬ (line 47-76)**:
```yaml
elasticsearch:
  container_name: rag-elasticsearch
  image: docker.elastic.co/elasticsearch/elasticsearch:8.14.0
  environment:
    - discovery.type=single-node
    - ES_JAVA_OPTS=-Xms2g -Xmx2g
    - xpack.security.enabled=false
    - xpack.security.enrollment.enabled=false
    # For Korean analysis, consider building custom image with nori plugin
```

**ë³€ê²½ í›„**:
```yaml
elasticsearch:
  container_name: rag-elasticsearch
  image: docker.elastic.co/elasticsearch/elasticsearch:8.14.0
  # Nori plugin ìë™ ì„¤ì¹˜
  entrypoint: >
    sh -c "
    if ! bin/elasticsearch-plugin list | grep -q analysis-nori; then
      echo 'Installing analysis-nori plugin...';
      bin/elasticsearch-plugin install --batch analysis-nori;
    fi &&
    /usr/local/bin/docker-entrypoint.sh
    "
  environment:
    - discovery.type=single-node
    - ES_JAVA_OPTS=-Xms2g -Xmx2g
    - xpack.security.enabled=false
    - xpack.security.enrollment.enabled=false
```

**ê²€ì¦**:
```bash
docker exec -it rag-elasticsearch bin/elasticsearch-plugin list
# ì¶œë ¥: analysis-nori
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] docker-compose.yml ìˆ˜ì •
- [ ] ES ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘: `docker compose down elasticsearch && docker compose up -d elasticsearch`
- [ ] Nori plugin ì„¤ì¹˜ í™•ì¸
- [ ] ES health check í†µê³¼ í™•ì¸

---

#### **Phase 2: Nori Analyzer ì„¤ì • (Mappings)**

**íŒŒì¼**: `backend/llm_infrastructure/elasticsearch/mappings.py`

**Step 2-1: get_index_settings() ìˆ˜ì • (line 173-200)**

**í˜„ì¬**:
```python
def get_index_settings(
    number_of_shards: int = 1,
    number_of_replicas: int = 0,
) -> dict[str, Any]:
    return {
        "number_of_shards": number_of_shards,
        "number_of_replicas": number_of_replicas,
        "refresh_interval": "1s",
        # Nori analyzer ì£¼ì„ ì²˜ë¦¬ë¨
    }
```

**ë³€ê²½ í›„**:
```python
def get_index_settings(
    number_of_shards: int = 1,
    number_of_replicas: int = 0,
    enable_nori: bool = True,
) -> dict[str, Any]:
    """Get index settings.

    Args:
        number_of_shards: Number of primary shards (default: 1 for dev)
        number_of_replicas: Number of replica shards (default: 0 for dev)
        enable_nori: Enable Korean (Nori) analyzer (default: True)

    Returns:
        Elasticsearch index settings
    """
    settings = {
        "number_of_shards": number_of_shards,
        "number_of_replicas": number_of_replicas,
        "refresh_interval": "1s",
    }

    if enable_nori:
        settings["analysis"] = {
            "analyzer": {
                "nori_analyzer": {
                    "type": "custom",
                    "tokenizer": "nori_tokenizer",
                    "filter": [
                        "nori_readingform",  # í•œì â†’ í•œê¸€ ë³€í™˜
                        "lowercase",
                        "nori_part_of_speech",  # í’ˆì‚¬ í•„í„° (ì¡°ì‚¬/ì–´ë¯¸ ì œê±°)
                    ],
                }
            },
            "filter": {
                "nori_part_of_speech": {
                    "type": "nori_part_of_speech",
                    # ì œê±°í•  í’ˆì‚¬ íƒœê·¸ (ì¡°ì‚¬, ì–´ë¯¸, ì ‘ë¯¸ì‚¬ ë“±)
                    "stoptags": [
                        "E",    # ì–´ë¯¸
                        "IC",   # ê°íƒ„ì‚¬
                        "J",    # ì¡°ì‚¬
                        "MAG",  # ì¼ë°˜ ë¶€ì‚¬
                        "MM",   # ê´€í˜•ì‚¬
                        "SP",   # ì‰¼í‘œ, ë§ˆì¹¨í‘œ
                        "SSC",  # ë‹«ëŠ” ê´„í˜¸
                        "SSO",  # ì—¬ëŠ” ê´„í˜¸
                        "SC",   # êµ¬ë¶„ì
                        "SE",   # ì¤„ì„í‘œ
                        "XPN",  # ì ‘ë‘ì‚¬
                        "XSA",  # í˜•ìš©ì‚¬ íŒŒìƒ ì ‘ë¯¸ì‚¬
                        "XSN",  # ëª…ì‚¬ íŒŒìƒ ì ‘ë¯¸ì‚¬
                        "XSV",  # ë™ì‚¬ íŒŒìƒ ì ‘ë¯¸ì‚¬
                        "UNA",  # ì•Œ ìˆ˜ ì—†ìŒ
                        "NA",   # ë¶„ì„ ë¶ˆëŠ¥
                        "VSV",  # ë™ì‚¬
                    ],
                }
            },
        }

    return settings
```

**Step 2-2: í…ìŠ¤íŠ¸ í•„ë“œì— nori analyzer ì ìš©**

**ë³€ê²½í•  í•„ë“œë“¤**:

```python
# Line 44-48: content í•„ë“œ
"content": {
    "type": "text",
    "analyzer": "nori_analyzer",  # standard â†’ nori_analyzer
},

# Line 50-55: search_text í•„ë“œ
"search_text": {
    "type": "text",
    "analyzer": "nori_analyzer",  # standard â†’ nori_analyzer
},

# Line 117-121: chunk_summary í•„ë“œ
"chunk_summary": {
    "type": "text",
    "index": True,
    "analyzer": "nori_analyzer",  # ì¶”ê°€
},

# Line 122-132: chunk_keywords í•„ë“œ
"chunk_keywords": {
    "type": "keyword",
    "doc_values": True,
    "fields": {
        "text": {
            "type": "text",
            "analyzer": "nori_analyzer",  # standard â†’ nori_analyzer
        },
    },
},
```

**Step 2-3: get_rag_chunks_mapping() íŒŒë¼ë¯¸í„° ì¶”ê°€**

```python
def get_rag_chunks_mapping(dims: int = 768, use_nori: bool = True) -> dict[str, Any]:
    """Get RAG chunks index mapping with specified embedding dimensions.

    Args:
        dims: Embedding vector dimensions (default: 768)
        use_nori: Use Nori analyzer for Korean text (default: True)

    Returns:
        Elasticsearch mapping definition
    """
    analyzer = "nori_analyzer" if use_nori else "standard"

    return {
        "properties": {
            # ... (ìœ„ì—ì„œ ìˆ˜ì •í•œ í•„ë“œë“¤ì— analyzer ë³€ìˆ˜ ì‚¬ìš©)
            "content": {
                "type": "text",
                "analyzer": analyzer,
            },
            # ...
        }
    }
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] `get_index_settings()` í•¨ìˆ˜ ìˆ˜ì • (enable_nori íŒŒë¼ë¯¸í„° ì¶”ê°€)
- [ ] Nori analyzer/filter ì„¤ì • ì¶”ê°€
- [ ] `get_rag_chunks_mapping()` í•¨ìˆ˜ ìˆ˜ì • (use_nori íŒŒë¼ë¯¸í„° ì¶”ê°€)
- [ ] content, search_text, chunk_summary, chunk_keywords.text í•„ë“œ analyzer ë³€ê²½
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ìˆë‹¤ë©´)

---

#### **Phase 3: ë¦¬ì¸ë±ì‹± ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±**

**íŒŒì¼**: `scripts/reindex_with_nori.py` (ì‹ ê·œ)

**ì£¼ìš” ê¸°ëŠ¥**:
1. í˜„ì¬ ì¸ë±ìŠ¤ ìƒíƒœ í™•ì¸ (ë²„ì „, ë¬¸ì„œ ìˆ˜, dims)
2. Nori í¬í•¨ ì‹ ê·œ ì¸ë±ìŠ¤ ìƒì„± (v2)
3. ë°ì´í„° reindex (ë°±ê·¸ë¼ìš´ë“œ ë˜ëŠ” ë™ê¸°)
4. Alias ì „í™˜ (atomic operation)
5. ê²€ì¦ (ë¬¸ì„œ ìˆ˜, ìƒ˜í”Œ ì¿¼ë¦¬)
6. ë¡¤ë°± ê¸°ëŠ¥

**ìŠ¤í¬ë¦½íŠ¸ êµ¬ì¡°**:
```python
#!/usr/bin/env python3
"""Reindex existing data with Nori analyzer.

This script creates a new index version with Nori analyzer enabled,
reindexes all data, and switches the alias atomically.

Usage:
    # Dry run (preview changes)
    python scripts/reindex_with_nori.py --dry-run

    # Execute reindexing
    python scripts/reindex_with_nori.py

    # Rollback to previous version
    python scripts/reindex_with_nori.py --rollback

    # Custom version numbers
    python scripts/reindex_with_nori.py --from-version 1 --to-version 2
"""

import argparse
import logging
import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from elasticsearch import Elasticsearch

from backend.config.settings import search_settings
from backend.llm_infrastructure.elasticsearch import EsIndexManager
from backend.llm_infrastructure.elasticsearch.mappings import (
    get_rag_chunks_mapping,
    get_index_settings,
)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


def reindex_with_nori(
    es_host: str,
    env: str,
    from_version: int,
    to_version: int,
    index_prefix: str = "rag_chunks",
    dry_run: bool = False,
) -> bool:
    """Reindex data with Nori analyzer.

    Args:
        es_host: Elasticsearch host URL
        env: Environment name (dev, staging, prod)
        from_version: Source index version
        to_version: Target index version (with Nori)
        index_prefix: Index prefix (default: rag_chunks)
        dry_run: If True, only preview changes

    Returns:
        True if successful, False otherwise
    """
    logger.info("=" * 80)
    logger.info("Nori Reindexing Strategy")
    logger.info("=" * 80)
    logger.info(f"ES Host: {es_host}")
    logger.info(f"Environment: {env}")
    logger.info(f"From version: v{from_version}")
    logger.info(f"To version: v{to_version} (with Nori)")
    logger.info(f"Dry run: {dry_run}")
    logger.info("=" * 80)

    # Initialize ES client and manager
    es_client = Elasticsearch([es_host], verify_certs=False)
    manager = EsIndexManager(
        es_client=es_client,
        env=env,
        index_prefix=index_prefix,
    )

    # Step 1: Check source index
    logger.info(f"\n[Step 1] Checking source index v{from_version}...")
    source_index = manager.get_index_name(from_version)

    if not manager.index_exists(from_version):
        logger.error(f"Source index {source_index} does not exist!")
        return False

    # Get source index stats
    stats = es_client.indices.stats(index=source_index)
    doc_count = stats["indices"][source_index]["total"]["docs"]["count"]
    size_bytes = stats["indices"][source_index]["total"]["store"]["size_in_bytes"]
    size_gb = size_bytes / (1024**3)

    logger.info(f"  âœ“ Source index: {source_index}")
    logger.info(f"  âœ“ Documents: {doc_count:,}")
    logger.info(f"  âœ“ Size: {size_gb:.2f} GB")

    # Get current dims
    current_dims = manager.get_index_dims(version=from_version)
    logger.info(f"  âœ“ Embedding dims: {current_dims}")

    # Step 2: Create target index with Nori
    logger.info(f"\n[Step 2] Creating target index v{to_version} with Nori...")
    target_index = manager.get_index_name(to_version)

    if manager.index_exists(to_version):
        logger.warning(f"  âš  Target index {target_index} already exists!")
        if not dry_run:
            response = input(f"Delete and recreate {target_index}? [y/N]: ")
            if response.lower() != "y":
                logger.info("Aborted by user")
                return False
            manager.delete_index(to_version)

    if dry_run:
        logger.info(f"  [DRY RUN] Would create index: {target_index}")
        logger.info(f"  [DRY RUN] With Nori analyzer enabled")
    else:
        try:
            # Create index with Nori
            body = {
                "settings": get_index_settings(
                    number_of_shards=1,
                    number_of_replicas=0,
                    enable_nori=True,  # â† Nori í™œì„±í™”
                ),
                "mappings": get_rag_chunks_mapping(
                    dims=current_dims or 768,
                    use_nori=True,  # â† Nori í™œì„±í™”
                ),
            }
            es_client.indices.create(index=target_index, body=body)
            logger.info(f"  âœ“ Created index: {target_index}")
        except Exception as e:
            logger.error(f"  âœ— Failed to create index: {e}")
            return False

    # Step 3: Reindex data
    logger.info(f"\n[Step 3] Reindexing {doc_count:,} documents...")

    if dry_run:
        logger.info(f"  [DRY RUN] Would reindex from {source_index} to {target_index}")
        estimated_time = doc_count / 10000  # ~10k docs/sec
        logger.info(f"  [DRY RUN] Estimated time: ~{estimated_time:.1f} seconds")
    else:
        try:
            logger.info("  Starting reindex (this may take several minutes)...")
            result = es_client.reindex(
                body={
                    "source": {"index": source_index},
                    "dest": {"index": target_index},
                },
                wait_for_completion=True,
                refresh=True,
            )

            created = result.get("created", 0)
            logger.info(f"  âœ“ Reindexed {created:,} documents")

            if created != doc_count:
                logger.warning(
                    f"  âš  Document count mismatch: expected {doc_count:,}, got {created:,}"
                )
        except Exception as e:
            logger.error(f"  âœ— Reindex failed: {e}")
            return False

    # Step 4: Test Nori analyzer
    logger.info("\n[Step 4] Testing Nori analyzer...")

    test_cases = [
        ("ì¥ë¹„ë¥¼ ê°€ë™í–ˆë‹¤", ["ì¥ë¹„", "ê°€ë™"]),
        ("ì„¼ì„œë¥¼ êµì²´í–ˆìŠµë‹ˆë‹¤", ["ì„¼ì„œ", "êµì²´"]),
        ("ì±”ë²„ ì²­ì†Œ ì ˆì°¨", ["ì±”ë²„", "ì²­ì†Œ", "ì ˆì°¨"]),
    ]

    if dry_run:
        logger.info("  [DRY RUN] Would test analyzer with sample queries")
    else:
        try:
            for text, expected_tokens in test_cases:
                result = es_client.indices.analyze(
                    index=target_index,
                    body={"analyzer": "nori_analyzer", "text": text},
                )
                tokens = [t["token"] for t in result["tokens"]]
                logger.info(f"  '{text}' â†’ {tokens}")

                # Check if expected tokens are present
                missing = set(expected_tokens) - set(tokens)
                if missing:
                    logger.warning(f"    âš  Missing tokens: {missing}")
                else:
                    logger.info(f"    âœ“ All expected tokens found")
        except Exception as e:
            logger.warning(f"  âš  Analyzer test failed: {e}")

    # Step 5: Switch alias
    alias_name = manager.get_alias_name()
    logger.info(f"\n[Step 5] Switching alias {alias_name} â†’ v{to_version}...")

    if dry_run:
        logger.info(f"  [DRY RUN] Would switch alias to {target_index}")
    else:
        try:
            manager.switch_alias(version=to_version)
            logger.info(f"  âœ“ Alias switched: {alias_name} â†’ {target_index}")
        except Exception as e:
            logger.error(f"  âœ— Failed to switch alias: {e}")
            return False

    # Step 6: Verification
    logger.info("\n[Step 6] Verification...")

    if dry_run:
        logger.info("  [DRY RUN] Verification skipped")
    else:
        try:
            # Verify alias
            alias_resp = es_client.indices.get_alias(name=alias_name)
            if target_index in alias_resp:
                logger.info(f"  âœ“ Alias verified: {alias_name} â†’ {target_index}")
            else:
                logger.error("  âœ— Alias verification failed!")
                return False

            # Verify document count
            new_stats = es_client.indices.stats(index=alias_name)
            new_doc_count = new_stats["indices"][target_index]["total"]["docs"]["count"]

            if new_doc_count == doc_count:
                logger.info(f"  âœ“ Document count verified: {new_doc_count:,}")
            else:
                logger.warning(
                    f"  âš  Count mismatch: expected {doc_count:,}, got {new_doc_count:,}"
                )
        except Exception as e:
            logger.error(f"  âœ— Verification failed: {e}")
            return False

    # Success!
    logger.info("\n" + "=" * 80)
    if dry_run:
        logger.info("âœ“ DRY RUN COMPLETE")
        logger.info("  Run without --dry-run to execute reindexing")
    else:
        logger.info("âœ“ REINDEXING COMPLETE")
        logger.info(f"  Source: {source_index} (v{from_version})")
        logger.info(f"  Target: {target_index} (v{to_version}, Nori enabled)")
        logger.info(f"  Alias: {alias_name} â†’ {target_index}")
        logger.info("\n  Next steps:")
        logger.info("  1. Test search with Korean queries")
        logger.info("  2. Compare Recall@10 vs previous version")
        logger.info("  3. Monitor for issues")
        logger.info(f"  4. Rollback if needed: python scripts/reindex_with_nori.py --rollback")
        logger.info(f"  5. Delete old index after validation: es_client.indices.delete('{source_index}')")
    logger.info("=" * 80)

    return True


def rollback(es_host: str, env: str, to_version: int, index_prefix: str = "rag_chunks") -> bool:
    """Rollback alias to previous version.

    Args:
        es_host: Elasticsearch host URL
        env: Environment name
        to_version: Version to rollback to
        index_prefix: Index prefix

    Returns:
        True if successful
    """
    logger.info("=" * 80)
    logger.info("ROLLBACK: Switching alias back to previous version")
    logger.info("=" * 80)

    es_client = Elasticsearch([es_host], verify_certs=False)
    manager = EsIndexManager(es_client=es_client, env=env, index_prefix=index_prefix)

    if not manager.index_exists(to_version):
        logger.error(f"Target version v{to_version} does not exist!")
        return False

    try:
        manager.switch_alias(version=to_version)
        alias_name = manager.get_alias_name()
        target_index = manager.get_index_name(to_version)
        logger.info(f"âœ“ Rolled back: {alias_name} â†’ {target_index}")
        return True
    except Exception as e:
        logger.error(f"âœ— Rollback failed: {e}")
        return False


def main():
    parser = argparse.ArgumentParser(
        description="Reindex with Nori analyzer",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__,
    )
    parser.add_argument(
        "--es-host",
        default=search_settings.es_host,
        help=f"Elasticsearch host (default: {search_settings.es_host})",
    )
    parser.add_argument(
        "--env",
        default=search_settings.es_env,
        help=f"Environment name (default: {search_settings.es_env})",
    )
    parser.add_argument(
        "--index-prefix",
        default=search_settings.es_index_prefix,
        help=f"Index prefix (default: {search_settings.es_index_prefix})",
    )
    parser.add_argument(
        "--from-version",
        type=int,
        default=1,
        help="Source index version (default: 1)",
    )
    parser.add_argument(
        "--to-version",
        type=int,
        default=2,
        help="Target index version (default: 2)",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Preview changes without executing",
    )
    parser.add_argument(
        "--rollback",
        action="store_true",
        help="Rollback to previous version",
    )

    args = parser.parse_args()

    if args.rollback:
        success = rollback(
            es_host=args.es_host,
            env=args.env,
            to_version=args.from_version,
            index_prefix=args.index_prefix,
        )
    else:
        success = reindex_with_nori(
            es_host=args.es_host,
            env=args.env,
            from_version=args.from_version,
            to_version=args.to_version,
            index_prefix=args.index_prefix,
            dry_run=args.dry_run,
        )

    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] `scripts/reindex_with_nori.py` ì‘ì„±
- [ ] Dry-run í…ŒìŠ¤íŠ¸: `python scripts/reindex_with_nori.py --dry-run`
- [ ] ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬: `chmod +x scripts/reindex_with_nori.py`

---

#### **Phase 4: ê²€ì¦ ë° ë¡¤ë°± ì „ëµ**

**ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸**:

1. **ê¸°ìˆ ì  ê²€ì¦**:
   - [ ] ë¬¸ì„œ ìˆ˜ ì¼ì¹˜ í™•ì¸
   - [ ] Embedding ì°¨ì› ì¼ì¹˜ í™•ì¸
   - [ ] Nori analyzer ë™ì‘ í™•ì¸ (analyze API)
   - [ ] Alias ì „í™˜ í™•ì¸

2. **ê¸°ëŠ¥ ê²€ì¦ (ìƒ˜í”Œ ì¿¼ë¦¬)**:
   ```python
   test_queries = [
       # (ì¿¼ë¦¬, ì˜ˆìƒ ë§¤ì¹­ í‚¤ì›Œë“œ)
       ("ì¥ë¹„ë¥¼ ê°€ë™í–ˆë‹¤", ["ì¥ë¹„ ê°€ë™", "ì¥ë¹„ ì‹œë™"]),
       ("ì„¼ì„œë¥¼ êµì²´í–ˆìŠµë‹ˆë‹¤", ["ì„¼ì„œ êµì²´", "ì„¼ì„œ ì¥ì°©"]),
       ("ì±”ë²„ ì²­ì†Œ ì ˆì°¨", ["ì±”ë²„ í´ë¦¬ë‹", "ì±”ë²„ë¥¼ ì²­ì†Œ"]),
       ("íŒí”„ê°€ ì‘ë™í•˜ì§€ ì•Šì•„ìš”", ["íŒí”„ ì‘ë™ ë¶ˆëŸ‰", "íŒí”„ ê³ ì¥"]),
       ("EFEM ì„¤ì¹˜ ë°©ë²•", ["EFEM ì¥ì°©", "EFEM ì„¤ì¹˜"]),
   ]
   ```
   - [ ] ê° ì¿¼ë¦¬ì˜ Recall@10 ì¸¡ì •
   - [ ] v1 vs v2 ë¹„êµ (ì˜ˆìƒ: +15% Recall)

3. **ì„±ëŠ¥ ê²€ì¦**:
   - [ ] ê²€ìƒ‰ ì‘ë‹µ ì‹œê°„ (<500ms ëª©í‘œ)
   - [ ] ì¸ë±ìŠ¤ í¬ê¸° ë¹„êµ
   - [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§

**ë¡¤ë°± ì ˆì°¨**:

```bash
# 1. ë¬¸ì œ ë°œê²¬ ì‹œ ì¦‰ì‹œ ë¡¤ë°±
python scripts/reindex_with_nori.py --rollback

# ë˜ëŠ” ì§ì ‘ alias ì „í™˜
python -c "
from backend.llm_infrastructure.elasticsearch import EsIndexManager
manager = EsIndexManager(es_host='http://localhost:8002', env='dev')
manager.switch_alias(version=1)  # v1ë¡œ ë¡¤ë°±
print('Rolled back to v1')
"

# 2. ê²€ì¦
curl http://localhost:8002/rag_chunks_dev_current/_cat/aliases

# 3. v2 ì¸ë±ìŠ¤ëŠ” ì¦‰ì‹œ ì‚­ì œí•˜ì§€ ë§ê³  ë³´ê´€ (ì¬ì‹œë„ ê°€ëŠ¥)
```

**ë¡¤ë°± ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] ë¡¤ë°± ìŠ¤í¬ë¦½íŠ¸ í…ŒìŠ¤íŠ¸
- [ ] ë¡¤ë°± í›„ ê¸°ëŠ¥ ê²€ì¦
- [ ] v2 ì¸ë±ìŠ¤ ë³´ê´€ ê¸°ê°„ ê²°ì • (ì˜ˆ: 1ì£¼ì¼)

---

### ğŸ“Š ì˜ˆìƒ íš¨ê³¼

| ì§€í‘œ | Before (v1) | After (v2) | ê°œì„ ìœ¨ |
|------|-------------|------------|--------|
| **Recall@10 (í•œêµ­ì–´)** | ê¸°ì¤€ | ì˜ˆìƒ +15% | +15% |
| **í˜•íƒœì†Œ ë§¤ì¹­** | âŒ ì‹¤íŒ¨ | âœ… ì„±ê³µ | - |
| **ê²€ìƒ‰ ì†ë„** | ê¸°ì¤€ | ìœ ì‚¬ (Â±5%) | 0% |
| **ì¸ë±ìŠ¤ í¬ê¸°** | ê¸°ì¤€ | ì˜ˆìƒ +10% | +10% |

**ì˜ˆì‹œ**:
- **Before**: "ì¥ë¹„ë¥¼ ê°€ë™í–ˆë‹¤" â†’ ë§¤ì¹­ ì‹¤íŒ¨ (exact matchë§Œ ê°€ëŠ¥)
- **After**: "ì¥ë¹„ë¥¼ ê°€ë™í–ˆë‹¤" â†’ "ì¥ë¹„", "ê°€ë™" í† í°ìœ¼ë¡œ ë¶„ë¦¬ â†’ "ì¥ë¹„ ê°€ë™" ë¬¸ì„œ ë§¤ì¹­ âœ“

---

### ğŸš€ ì‹¤í–‰ ìˆœì„œ

```bash
# Phase 1: Docker ì„¤ì •
vi docker-compose.yml  # entrypoint ì¶”ê°€
docker compose down elasticsearch
docker compose up -d elasticsearch
docker exec -it rag-elasticsearch bin/elasticsearch-plugin list

# Phase 2: ì½”ë“œ ìˆ˜ì •
vi backend/llm_infrastructure/elasticsearch/mappings.py
# - get_index_settings() ìˆ˜ì •
# - get_rag_chunks_mapping() ìˆ˜ì •

# Phase 3: ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
vi scripts/reindex_with_nori.py
chmod +x scripts/reindex_with_nori.py

# Dry-run í…ŒìŠ¤íŠ¸
python scripts/reindex_with_nori.py --dry-run

# ì‹¤ì œ ì‹¤í–‰
python scripts/reindex_with_nori.py

# Phase 4: ê²€ì¦
python scripts/test_nori_search.py  # ë³„ë„ ì‘ì„± í•„ìš”
```

---

### âš ï¸ ì£¼ì˜ì‚¬í•­

1. **ë‹¤ìš´íƒ€ì„ ìµœì†Œí™”**:
   - ReindexëŠ” ë°±ê·¸ë¼ìš´ë“œë¡œ ì‹¤í–‰ ê°€ëŠ¥ (wait_for_completion=False)
   - Alias ì „í™˜ì€ atomic operation (ë¬´ì¤‘ë‹¨)

2. **ë””ìŠ¤í¬ ìš©ëŸ‰**:
   - v1ê³¼ v2ê°€ ë™ì‹œ ì¡´ì¬ (ì¼ì‹œì ìœ¼ë¡œ 2ë°° ìš©ëŸ‰ í•„ìš”)
   - í˜„ì¬ 5.5GB â†’ ê²€ì¦ í›„ v1 ì‚­ì œ

3. **ì„ë² ë”©ì€ ì¬ìƒì„± ì•ˆ í•¨**:
   - ReindexëŠ” ë¬¸ì„œë§Œ ë³µì‚¬ (embedding ê·¸ëŒ€ë¡œ ìœ ì§€)
   - NoriëŠ” í…ìŠ¤íŠ¸ ì¸ë±ì‹±(BM25)ì—ë§Œ ì˜í–¥

4. **ì„¤ì • ë°±ì—…**:
   - í˜„ì¬ v1 mapping/settingsëŠ” ì´ë¯¸ ë°±ì—…ë¨ (`docs/es_mapping_snapshot_2026-01-02.json`)

---

### ğŸ“ ê´€ë ¨ ë¬¸ì„œ

- **TODO í•­ëª©**: P1 â€” í•œêµ­ì–´ analyzer(Nori) í™œì„±í™” ê³„íš ìˆ˜ë¦½ (line 82-86)
- **ê¸°ì¡´ ìŠ¤ëƒ…ìƒ·**: `docs/es_mapping_snapshot_2026-01-02.json`
- **ë§ˆì´ê·¸ë ˆì´ì…˜ ì°¸ê³ **: `scripts/migrate_to_alias_strategy.py`
- **ES Nori ê³µì‹ ë¬¸ì„œ**: https://www.elastic.co/guide/en/elasticsearch/plugins/current/analysis-nori.html

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2026-01-02
**ë‹¤ìŒ ë¦¬ë·°**: Phase 1 ì™„ë£Œ í›„ (2ì£¼ í›„)
