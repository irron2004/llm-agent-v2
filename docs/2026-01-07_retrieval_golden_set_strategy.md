# Retrieval Golden Set êµ¬ì¶• ì „ëµ

**ì‘ì„±ì¼**: 2026-01-07
**ëª©ì **: ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€ë¥¼ ìœ„í•œ Golden Set(qrels) êµ¬ì¶• ë°©ë²•ë¡  ì •ì˜

---

## ëª©ì°¨

1. [ê°œìš”](#1-ê°œìš”)
2. [ì „ëµì˜ í•µì‹¬ ì›ì¹™](#2-ì „ëµì˜-í•µì‹¬-ì›ì¹™)
3. [Pooling ë°©ë²•ë¡ ](#3-pooling-ë°©ë²•ë¡ )
4. [Graded Relevance](#4-graded-relevance)
5. [Annotation í”„ë¡œì„¸ìŠ¤](#5-annotation-í”„ë¡œì„¸ìŠ¤)
6. [í’ˆì§ˆ ê´€ë¦¬](#6-í’ˆì§ˆ-ê´€ë¦¬)
7. [í”„ë¡œì íŠ¸ íŠ¹í™” ì‚¬í•­](#7-í”„ë¡œì íŠ¸-íŠ¹í™”-ì‚¬í•­)
8. [ì‹¤í–‰ ê³„íš](#8-ì‹¤í–‰-ê³„íš)
9. [ë¹„ìš© ë° ì‹œê°„ ì¶”ì •](#9-ë¹„ìš©-ë°-ì‹œê°„-ì¶”ì •)
10. [ì‹¤ë¬´ í•¨ì • ë° ë³´ì™„](#10-ì‹¤ë¬´-í•¨ì •-ë°-ë³´ì™„)
11. [Answer Quality í‰ê°€ í™•ì¥](#11-answer-quality-í‰ê°€-í™•ì¥)
12. [ë¶€ë¡](#12-ë¶€ë¡)

---

## 1. ê°œìš”

### 1.1 ë°°ê²½

í˜„ì¬ RAG ì‹œìŠ¤í…œì€ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰(BM25 + Dense Vector)ì„ ì‚¬ìš©í•˜ê³  ìˆìœ¼ë‚˜, ê²€ìƒ‰ í’ˆì§ˆì„ ì •ëŸ‰ì ìœ¼ë¡œ í‰ê°€í•  ìˆ˜ë‹¨ì´ ë¶€ì¡±í•¨. ë‹¤ìŒ ê°œì„  ì‚¬í•­ë“¤ì„ í‰ê°€í•˜ê¸° ìœ„í•´ Golden Setì´ í•„ìš”:

- Nori analyzer íš¨ê³¼ ì¸¡ì •
- í•˜ì´ë¸Œë¦¬ë“œ ê°€ì¤‘ì¹˜ íŠœë‹ (í˜„ì¬ dense:sparse = 0.7:0.3)
- Multi-query expansion íš¨ê³¼
- Cross-encoder re-ranking íš¨ê³¼
- Multi-hop retrieval ê°œì„ 

### 1.2 Golden Setì´ë€?

**ì •ì˜**: ì§ˆë¬¸-ë¬¸ì„œ ê´€ë ¨ì„±(relevance)ì„ ì‚¬ëŒì´ ë¼ë²¨ë§í•œ ì •ë‹µ ë°ì´í„°ì…‹

```
Golden Set = {
  "ì§ˆë¬¸": "RFID ì„¼ì„œ êµì²´ ì ˆì°¨ëŠ”?",
  "ì •ë‹µ ë¬¸ì„œ": [
    {"doc_id": "sop_rfid#12", "relevance": 3},  # í•„ìˆ˜
    {"doc_id": "ts_guide_rfid#5", "relevance": 2},  # í•„ìš”
    {"doc_id": "log_40001#3", "relevance": 1},  # ì°¸ê³ 
  ]
}
```

**ìš©ë„**:
- Recall@k, nDCG@k, MRR ê°™ì€ ê²€ìƒ‰ í’ˆì§ˆ ì§€í‘œ ê³„ì‚°
- ê²€ìƒ‰ ì‹œìŠ¤í…œ ë³€ê²½ ì „/í›„ ì„±ëŠ¥ ë¹„êµ
- A/B í…ŒìŠ¤íŠ¸ ê¸°ì¤€

### 1.3 ì™œ Golden Set êµ¬ì¶•ì´ ì–´ë ¤ìš´ê°€?

**ë¬¸ì œì **:
1. **ì „ìˆ˜ ì¡°ì‚¬ ë¶ˆê°€ëŠ¥**: 338K+ chunks ì „ì²´ë¥¼ ì‚¬ëŒì´ ë³¼ ìˆ˜ ì—†ìŒ
2. **ì£¼ê´€ì„±**: ì‚¬ëŒë§ˆë‹¤ "ê´€ë ¨ ìˆë‹¤"ì˜ ê¸°ì¤€ì´ ë‹¤ë¦„
3. **ë¹„ìš©**: ì§ˆë¬¸ 40ê°œ Ã— ë¬¸ì„œ 100ê°œ/ì§ˆë¬¸ Ã— 30ì´ˆ = 33ì‹œê°„ ì†Œìš”
4. **í¸í–¥**: í•œ ê²€ìƒ‰ ë°©ì‹ë§Œ ì‚¬ìš©í•˜ë©´ ê·¸ ë°©ì‹ì— ìœ ë¦¬í•œ Golden Setì´ ë¨

**í•´ê²°ì±…**: ì •ë³´ê²€ìƒ‰ ì»¤ë®¤ë‹ˆí‹°ì˜ í‘œì¤€ ë°©ë²•ë¡  ì ìš©
- **Pooling**: ì—¬ëŸ¬ ê²€ìƒ‰ ë°©ì‹ì˜ ìƒìœ„ ê²°ê³¼ë§Œ í‰ê°€
- **Graded Relevance**: 0/1ì´ ì•„ë‹Œ 3~4ë‹¨ê³„ë¡œ êµ¬ë¶„
- **Dev/Test Split**: í‰ê°€ìš© ë°ì´í„° ê³ ì •
- **í’ˆì§ˆ ê´€ë¦¬**: 2ì¸ ë¼ë²¨ë§ìœ¼ë¡œ ì¼ì¹˜ë„ í™•ì¸

---

## 2. ì „ëµì˜ í•µì‹¬ ì›ì¹™

### 2.1 Multi-Relevant ì¸ì •

**ì›ì¹™**: í•˜ë‚˜ì˜ ì§ˆë¬¸ì— ì—¬ëŸ¬ ë¬¸ì„œê°€ ê´€ë ¨ë  ìˆ˜ ìˆìŒ

```
ì§ˆë¬¸: "RFID ì„¼ì„œ ì—ëŸ¬ í•´ê²° ë°©ë²•"

ê´€ë ¨ ë¬¸ì„œ:
- maintenance_log: ê³¼ê±° í•´ê²° ì‚¬ë¡€ (ì°¸ê³ )
- ts_guide: ì§„ë‹¨ ì ˆì°¨ (í•„ìš”)
- sop: êµì²´ ì ˆì°¨ (í•„ìˆ˜)
- setup_manual: ì„¼ì„œ ì‚¬ì–‘ (ë°°ê²½)

â†’ ëª¨ë‘ relevantí•˜ì§€ë§Œ "ì¤‘ìš”ë„"ê°€ ë‹¤ë¦„
```

### 2.2 Poolingìœ¼ë¡œ í›„ë³´ ì¶•ì†Œ

**ë¬¸ì œ**: 338K chunksë¥¼ ëª¨ë‘ ë³¼ ìˆ˜ ì—†ìŒ

**í•´ê²°**: ì—¬ëŸ¬ ê²€ìƒ‰ ë°©ì‹ì˜ Top-N í•©ì§‘í•©ë§Œ í‰ê°€

```python
# ì§ˆë¬¸ë‹¹ í›„ë³´ pool ìƒì„±
pool = []
pool += bm25_search(query, top_k=50)           # í¬ì†Œ ê²€ìƒ‰
pool += dense_search(query, top_k=50)          # ë°€ì§‘ ê²€ìƒ‰
pool += hybrid_search(query, top_k=50)         # í•˜ì´ë¸Œë¦¬ë“œ
pool += stratified_search(query, top_k=80)     # doc_typeë³„
pool += tag_based_search(query, top_k=30)      # tag í™•ì¥

pool = deduplicate(pool)[:150]  # ìµœì¢… 150ê°œ
```

**íš¨ê³¼**:
- 338K â†’ 150ê°œë¡œ ì¶•ì†Œ (99.96% ê°ì†Œ)
- ë‹¤ì–‘í•œ ë°©ì‹ í¬í•¨ â†’ í¸í–¥ ê°ì†Œ
- ë†“ì¹œ relevant ìµœì†Œí™”

### 2.3 Dev/Test ë¶„ë¦¬

**ì›ì¹™**: í‰ê°€ìš© ë°ì´í„°ëŠ” ê³ ì •, ê°œë°œìš© ë°ì´í„°ëŠ” í™•ì¥ ê°€ëŠ¥

```
ì „ì²´ 40ê°œ ì§ˆë¬¸
â”œâ”€ Dev Set (30ê°œ): ê°œë°œ/íŠœë‹ìš©, ê³„ì† í™•ì¥ ê°€ëŠ¥
â””â”€ Test Set (10ê°œ): í‰ê°€ ì „ìš©, v1ë¡œ freeze

ëª¨ë¸ ë³€ê²½ ì‹œ:
- Devë¡œ íŠœë‹ (ê°€ì¤‘ì¹˜ ì¡°ì •, íŒŒë¼ë¯¸í„° ì‹¤í—˜)
- Testë¡œ ìµœì¢… í‰ê°€ (ê³µì • ë¹„êµ)
```

**ë²„ì „ ê´€ë¦¬**:
- Test v1 (2026-01-15): ì´ˆê¸° ë²„ì „, freeze
- Test v2 (2026-04-01): ë¶„ê¸°ë³„ ë¦¬í”„ë ˆì‹œ (ë¬¸ì„œ ëŒ€ê·œëª¨ ë³€ê²½ ì‹œ)
- Dev: ì§€ì† í™•ì¥ (ìƒˆ query/document ì¶”ê°€)

### 2.4 Chunk vs Document ë‹¨ìœ„ ë¶„ë¦¬ âš ï¸ ì¤‘ìš”

**ë¬¸ì œ**: í˜„ì¬ ì‹œìŠ¤í…œì€ "chunk ë‹¨ìœ„" ì¸ë±ì‹±ì´ì§€ë§Œ, ì •ë‹µì€ "ë¬¸ì„œ/ì„¹ì…˜ ë‹¨ìœ„"ì¸ ê²½ìš°ê°€ ë§ìŒ

```
ì˜ˆ: "RFID ì„¼ì„œ êµì²´ SOP"
- ë¬¸ì„œ: global_sop_supra_xp_all_pm_rfid
- Chunk: #0012, #0013, #0014 (3ê°œë¡œ ë¶„í• ë¨)

ë¼ë²¨ë§ ì‹œ:
- "ì´ ë¬¸ì„œê°€ relevant" (ë¬¸ì„œ ì „ì²´)
- í•˜ì§€ë§Œ qrelsëŠ” chunk ë‹¨ìœ„ë¡œ ì €ì¥í•´ì•¼ í•¨
```

**í•´ê²°ì±…: ì•ˆì •ì ì¸ ë©”íƒ€ë°ì´í„° ì¶”ê°€**

qrelsì— ë‹¤ìŒ í•„ë“œë¥¼ **ë°˜ë“œì‹œ** í¬í•¨:

```json
{
  "chunk_id": "global_sop_supra_xp_all_pm_rfid#0012",  // í‰ê°€ ë‹¨ìœ„
  "parent_doc_id": "global_sop_supra_xp_all_pm_rfid",  // ë¬¸ì„œ ë‹¨ìœ„
  "section_id": "3. RFID ì„¼ì„œ êµì²´ ì ˆì°¨",               // ì„¹ì…˜ ë‹¨ìœ„
  "doc_type": "sop",
  "device_name": "SUPRA XP",
  "module": "PM",
  "chapter": "RFID Sensor",
  "version": "Rev. 1.2",
  "last_updated": "2024-08-15"
}
```

**íš¨ê³¼**:
- Chunk ì¬ë¶„í•  ì‹œ parent_doc_idë¡œ ë§¤í•‘ ê°€ëŠ¥
- ë¬¸ì„œ ë²„ì „ ë³€ê²½ ì¶”ì 
- Doc-level aggregation ê°€ëŠ¥ (chunk í‰ê°€ â†’ doc í‰ê°€)

**Stable ID ì „ëµ** (Optional, ê³ ê¸‰):

```
í˜„ì¬: chunk_id = f"{doc_id}#{sequential_number}"
      â†’ ì¬ì¸ë±ì‹± ì‹œ ë²ˆí˜¸ ë°”ë€œ (ë¶ˆì•ˆì •)

ê¶Œì¥: chunk_id = f"{doc_id}#{section_anchor}#{content_hash[:8]}"
      â†’ ì„¹ì…˜ ê¸°ì¤€ + ë‚´ìš© í•´ì‹œ (ì¬ì¸ë±ì‹±ì— ê°•ê±´)

ì˜ˆ: global_sop_rfid#section3_replacement#a3f8b2d1
```

---

## 3. Pooling ë°©ë²•ë¡ 

### 3.1 ê¸°ë³¸ Pooling (5ê°€ì§€ ë°©ì‹)

```python
def create_pool(query: str, retriever, top_k=50) -> list:
    """ì§ˆë¬¸ë‹¹ í›„ë³´ pool ìƒì„±"""
    pools = []

    # 1. BM25 (í¬ì†Œ ê²€ìƒ‰)
    pools.append(retriever.search(
        query,
        method="bm25",
        top_k=top_k
    ))

    # 2. Dense (ë°€ì§‘ ê²€ìƒ‰)
    pools.append(retriever.search(
        query,
        method="dense",
        top_k=top_k
    ))

    # 3. Hybrid (í˜„ì¬ ìš´ì˜ ë°©ì‹)
    pools.append(retriever.search(
        query,
        method="hybrid",
        dense_weight=0.7,
        sparse_weight=0.3,
        top_k=top_k
    ))

    # 4. Query í™•ì¥ (ë™ì˜ì–´/ì•½ì–´)
    expanded_query = expand_query(query)  # ì˜ˆ: "ì ê²€" â†’ "ì ê²€ ì •ê¸°ì ê²€ check"
    pools.append(retriever.search(
        expanded_query,
        method="bm25",
        top_k=top_k
    ))

    # 5. Multi-query (LLM ìƒì„±)
    if enable_multi_query:
        variants = llm_generate_query_variants(query, n=2)
        for variant in variants:
            pools.append(retriever.search(variant, top_k=top_k//2))

    # Deduplicate
    return deduplicate(flatten(pools))
```

### 3.2 Stratified Pooling (Doc-type ë‹¤ì–‘ì„± ë³´ì¥)

**ë¬¸ì œ**: ë‹¨ìˆœ top-Nì€ dominant typeì— í¸í–¥ë¨

```
ì˜ˆ: "RFID ì„¼ì„œ êµì²´"
- Setup manual: 200ê°œ (ë¬¸ì„œ ë§ìŒ)
- SOP: 50ê°œ
- TS-guide: 30ê°œ
- Log: 10ê°œ

â†’ Poolì— setup manualë§Œ ê°€ë“ ì°¸
```

**í•´ê²° A: ê³ ì • Quota ë°©ì‹ (ê¸°ë³¸)**

```python
def stratified_pool(query: str, retriever, type_quotas: dict) -> list:
    """Doc-typeë³„ ë‹¤ì–‘ì„± ë³´ì¥"""
    pool = []

    # Typeë³„ë¡œ ê²€ìƒ‰
    for doc_type, quota in type_quotas.items():
        results = retriever.search(
            query,
            filters={"doc_type": doc_type},
            top_k=quota
        )
        pool.extend(results)

    return pool

# ì‚¬ìš© ì˜ˆì‹œ
type_quotas = {
    "sop": 30,           # SOP 30ê°œ
    "ts_guide": 25,      # TS-guide 25ê°œ
    "maintenance_log": 20,     # ì •ë¹„ë¡œê·¸ 20ê°œ
    "setup": 20,         # Setup manual 20ê°œ
}
pool = stratified_pool(query, retriever, type_quotas)  # ì´ 95ê°œ
```

**í•´ê²° B: Min Quota ë°©ì‹ (ê¶Œì¥, ë” ìœ ì—°)** â­

ë¬¸ì œ: ì–´ë–¤ ì§ˆë¬¸ì€ íŠ¹ì • doc_typeì´ ê±°ì˜ ë¬´ê´€í•  ìˆ˜ ìˆìŒ (ê³ ì • quotaëŠ” ë¹„íš¨ìœ¨)

```python
def stratified_pool_min_quota(
    query: str,
    retriever,
    total_pool_size: int = 150,
    min_per_type: int = 10
) -> list:
    """ì „ì²´ poolì—ì„œ typeë³„ ìµœì†Œ ë³´ì¥ + ë‚˜ë¨¸ì§€ëŠ” ì „ì²´ ë­í‚¹"""

    # Step 1: ì „ì²´ ê²€ìƒ‰ (hybrid)
    all_results = retriever.search(query, top_k=total_pool_size)

    # Step 2: Typeë³„ ì¹´ìš´íŠ¸
    type_counts = defaultdict(int)
    for doc in all_results:
        type_counts[doc.metadata["doc_type"]] += 1

    # Step 3: ë¶€ì¡±í•œ typeë§Œ ì¶”ê°€ ê²€ìƒ‰
    additional = []
    for doc_type in ["sop", "ts_guide", "maintenance_log", "setup"]:
        if type_counts[doc_type] < min_per_type:
            needed = min_per_type - type_counts[doc_type]
            extra = retriever.search(
                query,
                filters={"doc_type": doc_type},
                top_k=needed + 5  # ì•½ê°„ ì—¬ìœ 
            )
            # ì¤‘ë³µ ì œì™¸í•˜ê³  ì¶”ê°€
            for doc in extra:
                if doc.chunk_id not in {d.chunk_id for d in all_results}:
                    additional.append(doc)
                    if len(additional) >= needed:
                        break

    # Step 4: ë³‘í•© (ì „ì²´ ë­í‚¹ ìš°ì„  + ì¶”ê°€)
    final_pool = all_results + additional
    return final_pool[:total_pool_size]

# íš¨ê³¼:
# - ìì—°ìŠ¤ëŸ¬ìš´ ë­í‚¹ ë³´ì¡´ (ì „ì²´ hybrid ìš°ì„ )
# - Typeë³„ ìµœì†Œ ë³´ì¥ (í¸í–¥ ë°©ì§€)
# - ë¹„íš¨ìœ¨ ê°ì†Œ (ë¬´ê´€í•œ type ê°•ì œ ì•ˆ í•¨)
```

### 3.3 Tag ê¸°ë°˜ í™•ì¥

**ëª©ì **: Seed relevant ë¬¸ì„œì˜ tagë¡œ ì¶”ê°€ í›„ë³´ ë°œêµ´

```python
def expand_by_tags(seed_docs: list, retriever, max_per_tag=20) -> list:
    """Tag ê¸°ë°˜ í™•ì¥ (ì—°ì‡„ í™•ì¥ ë°©ì§€)"""

    # Step 1: Seed ë¬¸ì„œì—ì„œ tag ì¶”ì¶œ
    all_tags = []
    for doc in seed_docs:
        all_tags.extend(doc.metadata.get("chunk_keywords", []))
        all_tags.extend(doc.metadata.get("tags", []))

    # Step 2: Tag í•„í„°ë§ (ë„ë©”ì¸ íŠ¹í™”)
    filtered_tags = [
        tag for tag in all_tags
        if len(tag) <= 20                          # ë‹¨ì–´ ìˆ˜ì¤€ë§Œ
        and tag not in STOPWORD_TAGS               # "batch", "manual" ì œì™¸
        and (is_equipment(tag) or is_part(tag))    # ì¥ë¹„/ë¶€í’ˆëª… ìš°ì„ 
    ]

    # Step 3: ë¹ˆë„ ê¸°ë°˜ ìƒìœ„ ì„ íƒ
    tag_counts = Counter(filtered_tags)
    top_tags = [tag for tag, _ in tag_counts.most_common(5)]  # ìµœëŒ€ 5ê°œ

    # Step 4: Tagë‹¹ ê²€ìƒ‰ (ì—°ì‡„ í™•ì¥ ì•ˆ í•¨!)
    expanded = []
    for tag in top_tags:
        results = retriever.search(
            query=tag,
            filters={"chunk_keywords": tag},
            top_k=max_per_tag
        )
        expanded.extend(results)

    return expanded

# ì‚¬ìš© ì˜ˆì‹œ
seed = hybrid_search(query, top_k=5)  # ìƒìœ„ 5ê°œ
expanded = expand_by_tags(seed, retriever, max_per_tag=20)
```

**ì£¼ì˜ì‚¬í•­**:
- âŒ ì—°ì‡„ í™•ì¥ ê¸ˆì§€: Tag A â†’ Doc B â†’ Tag C â†’ Doc D... (ë²”ìœ„ í­ë°œ)
- âœ… 1ë‹¨ê³„ë§Œ: Seed docsì˜ tag â†’ ì¶”ê°€ docs (ì—¬ê¸°ì„œ ë©ˆì¶¤)
- âœ… Tag í’ˆì§ˆ ê´€ë¦¬: ë„ˆë¬´ ì¼ë°˜ì /ë„ˆë¬´ ê¸´ tag ì œì™¸

### 3.4 Hard Negative Mining

**ëª©ì **: "ë¹„ìŠ·í•´ ë³´ì´ì§€ë§Œ ë‹µì€ ì•„ë‹Œ" ë¬¸ì„œ í¬í•¨ (í‰ê°€ ì—„ê²©í™”)

```python
def mine_hard_negatives(query: str, relevant_docs: list, retriever, n=10) -> list:
    """ê°™ì€ tag/ì¥ë¹„ì¸ë° ê´€ë ¨ ì—†ëŠ” ë¬¸ì„œ"""

    # Relevant ë¬¸ì„œì˜ tag ì¶”ì¶œ
    tags = extract_tags(relevant_docs)
    device = relevant_docs[0].metadata.get("device_name")

    # TagëŠ” ê°™ì€ë° relevant ì•„ë‹Œ ê²ƒ
    candidates = retriever.search_by_tags(tags, top_k=100)

    hard_negatives = [
        doc for doc in candidates
        if doc.chunk_id not in {d.chunk_id for d in relevant_docs}
        and doc.metadata.get("device_name") == device  # ê°™ì€ ì¥ë¹„
    ][:n]

    return hard_negatives

# ì˜ˆì‹œ
query = "RFID ì„¼ì„œ êµì²´ ì ˆì°¨"
relevant = [sop_rfid_replacement]  # êµì²´ SOP
hard_neg = mine_hard_negatives(query, relevant, retriever)
# â†’ sop_rfid_cleaning (ì²­ì†Œ SOP, êµì²´ ì•„ë‹˜)
# â†’ sop_rfid_calibration (êµì • SOP, êµì²´ ì•„ë‹˜)
```

### 3.5 Near-Duplicate ì œê±° âš ï¸ ì¤‘ìš” (íŠ¹íˆ Log ë¬¸ì„œ)

**ë¬¸ì œ**: ì •ë¹„ ë¡œê·¸(maintenance_log)ëŠ” ì¤‘ë³µ/ìœ ì‚¬ chunkê°€ ë§ì•„ poolì´ ë„ë°°ë¨

```
ì˜ˆ: "RFID ì„¼ì„œ ì—ëŸ¬" ê²€ìƒ‰
â†’ Pool 150ê°œ ì¤‘ 100ê°œê°€ ê±°ì˜ ë™ì¼í•œ ë¡œê·¸
â†’ ë¼ë²¨ë§ ì‹œê°„ ë‚­ë¹„ + ë‹¤ì–‘ì„± ì €í•˜
```

**í•´ê²°: Pool ìƒì„± ë‹¨ê³„ì—ì„œ near-duplicate clustering**

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def deduplicate_pool_with_clustering(
    pool: list,
    similarity_threshold: float = 0.85,
    doc_type_specific: dict = None
) -> list:
    """Near-duplicateë¥¼ clusteringí•´ì„œ ëŒ€í‘œ 1ê°œë§Œ ë‚¨ê¹€"""

    if doc_type_specific is None:
        doc_type_specific = {
            "maintenance_log": 0.90,  # ë¡œê·¸ëŠ” ë” ì—„ê²©
            "sop": 0.75,              # SOPëŠ” ëœ ì—„ê²©
            "ts_guide": 0.75,
            "setup": 0.70
        }

    # Doc-typeë³„ë¡œ ë¶„ë¦¬
    by_type = defaultdict(list)
    for doc in pool:
        doc_type = doc.metadata.get("doc_type", "other")
        by_type[doc_type].append(doc)

    deduplicated = []

    for doc_type, docs in by_type.items():
        if len(docs) <= 1:
            deduplicated.extend(docs)
            continue

        # TF-IDFë¡œ ìœ ì‚¬ë„ ê³„ì‚°
        texts = [d.content for d in docs]
        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform(texts)
        sim_matrix = cosine_similarity(tfidf_matrix)

        # Clustering (greedy)
        threshold = doc_type_specific.get(doc_type, similarity_threshold)
        used = set()
        clusters = []

        for i in range(len(docs)):
            if i in used:
                continue

            cluster = [i]
            for j in range(i+1, len(docs)):
                if j not in used and sim_matrix[i, j] >= threshold:
                    cluster.append(j)
                    used.add(j)

            clusters.append(cluster)

        # ê° clusterì—ì„œ ëŒ€í‘œ 1ê°œ ì„ íƒ (ë­í‚¹ ìš°ì„ )
        for cluster in clusters:
            representative = docs[cluster[0]]  # ì²« ë²ˆì§¸ = ë­í‚¹ ë†’ì€ ê²ƒ
            deduplicated.append(representative)

    return deduplicated

# ì‚¬ìš© ì˜ˆì‹œ
pool_raw = create_pool_for_query(query, retriever, top_k=200)
pool_final = deduplicate_pool_with_clustering(pool_raw)[:150]
# â†’ ë¡œê·¸ ì¤‘ë³µ ì œê±° í›„ 150ê°œ ìœ ì§€
```

**Alternative: Simhash/MinHash (ë” ë¹ ë¦„, ëŒ€ìš©ëŸ‰ìš©)**

```python
from simhash import Simhash

def quick_deduplicate(pool: list, hamming_threshold: int = 3) -> list:
    """Simhashë¡œ ë¹ ë¥´ê²Œ ì¤‘ë³µ ì œê±°"""

    hashes = {}
    deduplicated = []

    for doc in pool:
        h = Simhash(doc.content)

        # ê¸°ì¡´ hashì™€ ë¹„êµ
        is_duplicate = False
        for existing_hash in hashes.keys():
            if h.distance(existing_hash) <= hamming_threshold:
                is_duplicate = True
                break

        if not is_duplicate:
            hashes[h] = doc
            deduplicated.append(doc)

    return deduplicated
```

**íš¨ê³¼**:
- ë¼ë²¨ë§ ì‹œê°„ 30~50% ì ˆì•½
- Pool ë‹¤ì–‘ì„± ì¦ê°€
- Annotator í”¼ë¡œë„ ê°ì†Œ

### 3.6 ìµœì¢… Pool í¬ê¸° ê¶Œì¥

```
ì§ˆë¬¸ ë‚œì´ë„ë³„ pool í¬ê¸°:

Easy (í‚¤ì›Œë“œ ëª…í™•):    80~100ê°œ
Medium (ì¼ë°˜):         120~150ê°œ
Hard (Multi-hop):      150~200ê°œ

í‰ê· : 150ê°œ ê¶Œì¥
```

**ì´ìœ **:
- ë„ˆë¬´ ì‘ìœ¼ë©´ (<50): Relevant ë†“ì¹  ìœ„í—˜
- ë„ˆë¬´ í¬ë©´ (>200): ë¼ë²¨ë§ ë¹„ìš© ì¦ê°€
- 150ê°œ Ã— 30ì´ˆ = 75ë¶„/ì§ˆë¬¸ (ì ë‹¹)

---

## 4. Graded Relevance

### 4.1 ì™œ 0/1ì´ ì•„ë‹Œ ë‹¤ë‹¨ê³„ì¸ê°€?

**ë¬¸ì œ: Binary relevance (0/1)**

```
ì§ˆë¬¸: "RFID ì„¼ì„œ êµì²´ ì ˆì°¨"

ë¬¸ì„œ A: SOP - êµì²´ ì ˆì°¨ ì§ì ‘ ê¸°ìˆ 
ë¬¸ì„œ B: TS-guide - êµì²´ ì „ ì§„ë‹¨ (ì‚¬ì „ ì‘ì—…)

Binaryë¡œëŠ”:
- A = 1 (relevant)
- B = 1 (relevant)

â†’ Aì™€ Bê°€ ë™ë“±? ì•„ë‹ˆì•¼!
```

**í•´ê²°: Graded relevance (0/1/2/3)**

```
ë¬¸ì„œ A: Grade 3 (Must-have)
ë¬¸ì„œ B: Grade 2 (Should-have)

â†’ nDCG@k ê³„ì‚° ì‹œ Aê°€ Bë³´ë‹¤ ë†’ì€ ì ìˆ˜
â†’ "í•„ìˆ˜ ë¬¸ì„œë¥¼ ìƒìœ„ì— ì˜¬ë¦¬ëŠ” ê²€ìƒ‰"ì„ ì •í™•íˆ í‰ê°€
```

### 4.2 4-Level Grading ê¸°ì¤€ (í”„ë¡œì íŠ¸ íŠ¹í™”)

#### Grade 3: Must-have (í•„ìˆ˜)

**ì •ì˜**: ì´ ë¬¸ì„œë§Œ ìˆìœ¼ë©´ ì§ˆë¬¸ í•´ê²° ê°€ëŠ¥

**ê¸°ì¤€**:
- ì§ˆë¬¸ì˜ ì§ì ‘ ë‹µ(ì ˆì°¨, í•´ê²°ì±…, ì •í™•í•œ ì§„ë‹¨) í¬í•¨
- í•µì‹¬ ì •ë³´ê°€ ëª…í™•íˆ ê¸°ìˆ ë¨
- ì¶”ê°€ ë¬¸ì„œ ì—†ì´ë„ ì´í•´ ê°€ëŠ¥

**ì˜ˆì‹œ**:

```yaml
ì§ˆë¬¸: "RFID ì„¼ì„œ êµì²´ ì ˆì°¨ëŠ”?"

Grade 3 ë¬¸ì„œ:
- global_sop_supra_xp_all_pm_rfid#0012
  ë‚´ìš©: "3. RFID ì„¼ì„œ êµì²´ ì ˆì°¨
         3-1. ì„¼ì„œ íƒˆê±° (ë‚˜ì‚¬ 4ê°œ ì œê±°)
         3-2. ìƒˆ ì„¼ì„œ ì¥ì°©
         3-3. í†µì‹  í…ŒìŠ¤íŠ¸"
  ì´ìœ : êµì²´ ì ˆì°¨ê°€ ë‹¨ê³„ë³„ë¡œ ëª…ì‹œë¨
```

#### Grade 2: Should-have (í•„ìš”)

**ì •ì˜**: ë¬¸ì œ í•´ê²°ì— í•„ìš”í•œ ë°°ê²½ì§€ì‹/ì‚¬ì „ ì‘ì—…

**ê¸°ì¤€**:
- ì§ˆë¬¸ ë‹µì„ ì´í•´í•˜ëŠ” ë° í•„ìš”í•œ ì •ë³´
- ì‚¬ì „ í™•ì¸/ì¤€ë¹„ ì‚¬í•­
- ê´€ë ¨ ê°œë…/ì›ë¦¬ ì„¤ëª…

**ì˜ˆì‹œ**:

```yaml
ì§ˆë¬¸: "RFID ì„¼ì„œ êµì²´ ì ˆì°¨ëŠ”?"

Grade 2 ë¬¸ì„œ:
- supra_xp_ts_guide_rfid_abnormal#0008
  ë‚´ìš©: "RFID ì„¼ì„œ ê³ ì¥ ì§„ë‹¨
         - í†µì‹  í™•ì¸ ë°©ë²•
         - ì „ì› ì²´í¬
         - êµì²´ í•„ìš” ì—¬ë¶€ íŒë‹¨"
  ì´ìœ : êµì²´ ì „ ê³ ì¥ í™•ì¸ í•„ìš” (ì‚¬ì „ ì‘ì—…)

- global_sop_safety_esd#0003
  ë‚´ìš©: "ì •ì „ê¸° ë°©ì§€ ì ˆì°¨
         - ì ‘ì§€ í™•ì¸
         - ESD ë°´ë“œ ì°©ìš©"
  ì´ìœ : ì„¼ì„œ êµì²´ ì‹œ ì•ˆì „ ìˆ˜ì¹™ (í•„ìˆ˜ ë°°ê²½)
```

#### Grade 1: Nice-to-have (ì°¸ê³ )

**ì •ì˜**: ê´€ë ¨ ì‚¬ë¡€/ì¶”ê°€ ì •ë³´

**ê¸°ì¤€**:
- ìœ ì‚¬ ì‘ì—… ì‚¬ë¡€
- ê´€ë ¨ ë¶€í’ˆ/ì¥ë¹„ ì •ë³´
- ìˆìœ¼ë©´ ë„ì›€ë˜ì§€ë§Œ í•„ìˆ˜ ì•„ë‹˜

**ì˜ˆì‹œ**:

```yaml
ì§ˆë¬¸: "RFID ì„¼ì„œ êµì²´ ì ˆì°¨ëŠ”?"

Grade 1 ë¬¸ì„œ:
- maintenance_log_40001648#0002
  ë‚´ìš©: "EPAH54 RFID ì„¼ì„œ êµì²´ ì‘ì—…
         - ì‘ì—…ì: ê¹€OO
         - ì†Œìš”ì‹œê°„: 30ë¶„
         - íŠ¹ì´ì‚¬í•­: ì¼€ì´ë¸” ë‹¨ì ë¶€ì‹ ë°œê²¬"
  ì´ìœ : ê³¼ê±° êµì²´ ì‚¬ë¡€ (ì°¸ê³ ìš©)

- global_sop_integer_rfid#0005
  ë‚´ìš©: "Integer Plus RFID êµì²´ ì ˆì°¨"
  ì´ìœ : ë‹¤ë¥¸ ì¥ë¹„ì§€ë§Œ ì ˆì°¨ ìœ ì‚¬ (ì°¸ê³ ìš©)
```

#### Grade 0: Not relevant (ë¬´ê´€)

**ì •ì˜**: í‚¤ì›Œë“œë§Œ ê²¹ì¹˜ê³  ì‹¤ì§ˆì  ë„ì›€ ì•ˆ ë¨

**ê¸°ì¤€**:
- ë‹¤ë¥¸ ì‘ì—…/ë‹¤ë¥¸ ë¬¸ì œ
- í‚¤ì›Œë“œ ìš°ì—°íˆ í¬í•¨
- ì§ˆë¬¸ í•´ê²°ì— ê¸°ì—¬ ì•ˆ í•¨

**ì˜ˆì‹œ**:

```yaml
ì§ˆë¬¸: "RFID ì„¼ì„œ êµì²´ ì ˆì°¨ëŠ”?"

Grade 0 ë¬¸ì„œ:
- global_sop_supra_xp_all_pm_rfid_cleaning#0007
  ë‚´ìš©: "RFID ì„¼ì„œ ì²­ì†Œ ì ˆì°¨"
  ì´ìœ : ì²­ì†ŒëŠ” êµì²´ê°€ ì•„ë‹˜ (ë‹¤ë¥¸ ì‘ì—…)

- setup_manual_geneva_xp#0234
  ë‚´ìš©: "Geneva XP RFID ì„¼ì„œ ì‚¬ì–‘
         - í†µì‹ : RS-485
         - ì „ì••: 24V"
  ì´ìœ : ì‚¬ì–‘ ì •ë³´ë§Œ, êµì²´ ì ˆì°¨ ì—†ìŒ
```

### 4.3 Edge Cases

#### Case 1: ë²„ì „/ì¥ë¹„ ì°¨ì´

```yaml
ì§ˆë¬¸: "SUPRA XP RFID ì„¼ì„œ êµì²´"
ë¬¸ì„œ: "SUPRA N RFID ì„¼ì„œ êµì²´" (ë‹¤ë¥¸ ì‹œë¦¬ì¦ˆ)

íŒë‹¨:
- ì ˆì°¨ê°€ ê±°ì˜ ë™ì¼ â†’ Grade 1 (ì°¸ê³ ìš©)
- ì ˆì°¨ê°€ í¬ê²Œ ë‹¤ë¦„ â†’ Grade 0 (ë¬´ê´€)
```

#### Case 2: ë¶€ë¶„ Overlap

```yaml
ì§ˆë¬¸: "ì±”ë²„ ì˜¨ë„ê°€ ì•ˆ ì˜¬ë¼ê°€ëŠ” ì´ìœ "
ë¬¸ì„œ: "ì±”ë²„ íˆí„° êµì • ì ˆì°¨"

íŒë‹¨:
- "ì›ì¸" í¬í•¨ â†’ Grade 2~3
- "í•´ê²°ì±…ë§Œ" â†’ Grade 1
- "êµì •ë§Œ" (ì§„ë‹¨ ì—†ìŒ) â†’ Grade 0~1
```

#### Case 3: Multi-document ì¡°í•©

```yaml
ì§ˆë¬¸: "ì—ëŸ¬ E1234 í•´ê²° ë°©ë²•"

í•„ìš” ë¬¸ì„œ:
- Log: E1234 ë°œìƒ ì‚¬ë¡€ â†’ Grade 3 (í˜„ìƒ í™•ì¸)
- TS-guide: E1234 ì§„ë‹¨ â†’ Grade 3 (ì›ì¸ íŒŒì•…)
- SOP: E1234 ì¡°ì¹˜ â†’ Grade 3 (í•´ê²°ì±…)

â†’ ê° ë‹¨ê³„ì—ì„œ í•„ìˆ˜ì´ë¯€ë¡œ ëª¨ë‘ Grade 3
```

---

## 5. Annotation í”„ë¡œì„¸ìŠ¤

### 5.1 Annotator (ë¼ë²¨ ì‘ì—…ì)

**ëˆ„ê°€ í•˜ë‚˜?**

1ìˆœìœ„: **ì •ë¹„ ì—”ì§€ë‹ˆì–´** (ë„ë©”ì¸ ì „ë¬¸ê°€)
2ìˆœìœ„: ë¬¸ì„œ ê´€ë¦¬ì (ì–´ë–¤ ë¬¸ì„œê°€ ê´€ë ¨ìˆëŠ”ì§€ ì•„ëŠ” ì‚¬ëŒ)
3ìˆœìœ„: ë°ì´í„° ê³¼í•™ì (ë„ë©”ì¸ ì§€ì‹ í•™ìŠµ í›„)

**í•„ìš” ì¸ì›**:
- ì£¼ annotator: 1ëª…
- ê²€ì¦ annotator: 1ëª… (20% ì¤‘ë³µ ë¼ë²¨ë§)

### 5.2 Annotation ë„êµ¬

#### Option A: CLI ë„êµ¬ (ê°„ë‹¨, ë¹ ë¦„)

```bash
# ì‹¤í–‰
python scripts/golden_set/annotate.py \
  --queries data/golden_set/queries.jsonl \
  --pools data/golden_set/pools/ \
  --output data/golden_set/qrels_dev.jsonl \
  --mode interactive
```

**í™”ë©´ ì˜ˆì‹œ**:

```
Query [1/40]: RFID ì„¼ì„œ êµì²´ ì ˆì°¨ëŠ”?

Document [1/147]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ID: global_sop_supra_xp_all_pm_rfid#0012
Type: sop
Device: SUPRA XP

Content:
  3. RFID ì„¼ì„œ êµì²´ ì ˆì°¨
  3-1. ì„¼ì„œ íƒˆê±°
    - ì „ì› ì°¨ë‹¨ í™•ì¸
    - ì„¼ì„œ ê³ ì • ë‚˜ì‚¬ 4ê°œ ì œê±°
  ...

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Grade (0/1/2/3/skip): 3
Rationale (optional): êµì²´ ì ˆì°¨ ì§ì ‘ ê¸°ìˆ 
Snippet (optional): 3-1. ì„¼ì„œ íƒˆê±°...

âœ“ Saved! [1/147 done, 75 min remaining]
```

#### Option B: Web UI (ê³ ê¸‰, í¸ë¦¬)

```bash
# ì„œë²„ ì‹¤í–‰
python scripts/golden_set/annotation_server.py

# ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:5000 ì ‘ì†
```

**ê¸°ëŠ¥**:
- ë¬¸ì„œ full-text ê²€ìƒ‰
- ì´ì „/ë‹¤ìŒ íƒìƒ‰
- ì§„í–‰ë¥  tracking
- ì—¬ëŸ¬ annotator ë™ì‹œ ì‘ì—…

### 5.3 LLM ë³´ì¡° (Optional)

**ëª©ì **: ë¼ë²¨ë§ ì‹œê°„ ë‹¨ì¶• (33ì‹œê°„ â†’ 15ì‹œê°„)

```python
def llm_assisted_annotation(query, document, llm):
    """LLMì´ ì´ˆì•ˆ ìƒì„± â†’ ì‚¬ëŒì´ ê²€í† """

    # LLMì—ê²Œ draft grade ìš”ì²­
    prompt = f"""
ì§ˆë¬¸: {query}

ë¬¸ì„œ:
{document.content[:500]}

ì´ ë¬¸ì„œê°€ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ë° ë„ì›€ì´ ë˜ë‚˜ìš”?
0: ë„ì›€ ì•ˆ ë¨
1: ì°¸ê³  ì •ë„
2: í•„ìš”í•œ ë°°ê²½ì§€ì‹
3: ì§ì ‘ ë‹µ í¬í•¨

JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€:
{{"grade": 0~3, "reason": "ê°„ë‹¨í•œ ì´ìœ "}}
"""

    response = llm.generate(prompt, max_tokens=100)
    draft = json.loads(response)

    # ì‚¬ëŒì—ê²Œ ë³´ì—¬ì£¼ê¸°
    print(f"ğŸ¤– LLM ì œì•ˆ: Grade {draft['grade']}")
    print(f"   ì´ìœ : {draft['reason']}")
    print("\nğŸ‘¤ LLM íŒë‹¨ì´ ë§ë‚˜ìš”?")

    user_input = input("  [y] ë§ìŒ  [n] í‹€ë¦¼  [v] ë¬¸ì„œ ì „ì²´ ë³´ê¸°: ")

    if user_input == 'y':
        final_grade = draft['grade']  # LLM ì œì•ˆ ìŠ¹ì¸
    elif user_input == 'v':
        print(document.content)  # ì „ì²´ ë³´ê¸°
        final_grade = input("  Grade (0/1/2/3): ")
    else:
        final_grade = input("  ì •í™•í•œ grade: ")

    return {
        "grade": int(final_grade),
        "llm_draft": draft['grade'],
        "llm_agreed": (final_grade == draft['grade'])
    }
```

**íš¨ê³¼**:
- LLMì´ 80% ë§ì¶”ë©´ â†’ ì‹œê°„ 55% ì ˆì•½ (33h â†’ 15h)
- ë¹„ìš©: GPT-4o-mini ê¸°ì¤€ $0.30 (400ì›)

**ì£¼ì˜**:
- âš ï¸ LLM íŒë‹¨ì„ ê·¸ëŒ€ë¡œ ì“°ë©´ ì•ˆ ë¨ (ì‚¬ëŒì´ ìµœì¢… ê²°ì •)
- âœ… "ì´ˆì•ˆ" ë˜ëŠ” "ë‘ ë²ˆì§¸ ì˜ê²¬"ìœ¼ë¡œë§Œ ì‚¬ìš©

### 5.4 ì €ì¥ í¬ë§·

```jsonl
// data/golden_set/qrels_dev.jsonl (JSONL format, 1 query per line)
{
  "query_id": "q001",
  "query_text": "RFID ì„¼ì„œ êµì²´ ì ˆì°¨ëŠ”?",
  "query_type": "procedure",
  "device_name": "SUPRA XP",
  "difficulty": "easy",

  "relevance_judgments": [
    {
      "chunk_id": "global_sop_supra_xp_all_pm_rfid#0012",
      "doc_id": "global_sop_supra_xp_all_pm_rfid",
      "grade": 3,
      "rationale": "êµì²´ ì ˆì°¨ ì§ì ‘ ê¸°ìˆ ",
      "supporting_snippet": "3-1. ì„¼ì„œ íƒˆê±° - ê³ ì • ë‚˜ì‚¬ 4ê°œ ì œê±°...",
      "annotator": "engineer_kim",
      "llm_draft": 3,
      "llm_agreed": true,
      "annotated_at": "2026-01-07T10:30:00"
    },
    {
      "chunk_id": "supra_xp_ts_guide_rfid#0008",
      "doc_id": "supra_xp_ts_guide_rfid",
      "grade": 2,
      "rationale": "êµì²´ ì „ ì§„ë‹¨ ì ˆì°¨ (ì‚¬ì „ ì‘ì—…)",
      "supporting_snippet": "RFID í†µì‹  í™•ì¸...",
      "annotator": "engineer_kim",
      "llm_draft": 2,
      "llm_agreed": true,
      "annotated_at": "2026-01-07T10:32:00"
    }
  ],

  "pool_info": {
    "pool_size": 147,
    "methods": ["bm25", "dense", "hybrid", "stratified", "tag-based"],
    "annotated_count": 8
  }
}
```

**TREC format ë³€í™˜**:

```bash
# Standard TREC qrels formatìœ¼ë¡œ export
python scripts/golden_set/export_trec.py \
  --input data/golden_set/qrels_dev.jsonl \
  --output data/golden_set/qrels_dev.trec

# Output: qrels_dev.trec
# query_id 0 doc_id relevance
q001 0 global_sop_supra_xp_all_pm_rfid#0012 3
q001 0 supra_xp_ts_guide_rfid#0008 2
q001 0 maintenance_log_40001#3 1
...
```

---

## 6. í’ˆì§ˆ ê´€ë¦¬

### 6.1 Annotation Guidelines ë¬¸ì„œí™”

**ëª©ì **: ì‚¬ëŒë§ˆë‹¤ íŒë‹¨ ê¸°ì¤€ì´ ë‹¬ë¼ì§€ëŠ” ê²ƒ ë°©ì§€

**í•„ìˆ˜ ë‚´ì—­**:

```markdown
# Annotation Guidelines

## 1. Relevance ì •ì˜

"ë¬¸ì„œê°€ ì§ˆë¬¸ í•´ê²°ì— ì‹¤ì§ˆì ìœ¼ë¡œ ê¸°ì—¬í•˜ëŠ”ê°€?"

- í‚¤ì›Œë“œë§Œ ê²¹ì¹˜ëŠ” ê²ƒ â‰  relevant
- ì‹¤ì œë¡œ ì—”ì§€ë‹ˆì–´ê°€ ì½ê³  ë„ì›€ë°›ëŠ”ê°€?

## 2. Gradeë³„ ê¸°ì¤€

[ìœ„ì˜ 4.2ì ˆ ë‚´ìš© ë³µì‚¬]

## 3. íŒë‹¨ ì›ì¹™

- ë¬¸ì„œ ì „ì²´ë¥¼ ì½ê³  íŒë‹¨ (ì œëª©/ì²« ë¬¸ì¥ë§Œ ë³´ê³  ê²°ì • ê¸ˆì§€)
- ì• ë§¤í•˜ë©´ ë‚®ì€ ë“±ê¸‰ ì„ íƒ (precision ìš°ì„ )
- ì¥ë¹„/ë²„ì „ì´ ë‹¤ë¥´ë©´ 1ë“±ê¸‰ ë‚®ì¶¤

## 4. ìì£¼ ë¬»ëŠ” ì§ˆë¬¸

Q: ì ˆì°¨ëŠ” ì—†ê³  ì‚¬ì–‘ë§Œ ìˆìœ¼ë©´?
A: Grade 0~1 (ì§ˆë¬¸ì´ "ì‚¬ì–‘"ì„ ë¬¼ìœ¼ë©´ 3, "ì ˆì°¨"ë¥¼ ë¬¼ìœ¼ë©´ 0)

Q: ë‹¤ë¥¸ ì¥ë¹„ì§€ë§Œ ì ˆì°¨ê°€ ìœ ì‚¬í•˜ë©´?
A: Grade 1 (ì°¸ê³ ìš©)

...
```

### 6.2 Inter-Annotator Agreement (2ì¸ ë¼ë²¨ë§)

**ëª©ì **: ê°€ì´ë“œë¼ì¸ì´ ëª…í™•í•œì§€ ê²€ì¦

**ë°©ë²•**: ì „ì²´ 40ê°œ ì¤‘ 8ê°œ(20%)ë¥¼ 2ëª…ì´ ë…ë¦½ì ìœ¼ë¡œ ë¼ë²¨ë§

```python
# ê°™ì€ ì§ˆë¬¸/ë¬¸ì„œë¥¼ 2ëª…ì´ ê°ê° ë¼ë²¨ë§
annotator_A = {
    "doc1": 3,
    "doc2": 2,
    "doc3": 1,
    "doc4": 0,
    "doc5": 0,
}

annotator_B = {
    "doc1": 3,  # âœ“ ì¼ì¹˜
    "doc2": 2,  # âœ“ ì¼ì¹˜
    "doc3": 0,  # âœ— ë¶ˆì¼ì¹˜ (A=1, B=0)
    "doc4": 0,  # âœ“ ì¼ì¹˜
    "doc5": 1,  # âœ— ë¶ˆì¼ì¹˜ (A=0, B=1)
}

# ì¼ì¹˜ë„ ê³„ì‚°
from sklearn.metrics import cohen_kappa_score

grades_A = [3, 2, 1, 0, 0]
grades_B = [3, 2, 0, 0, 1]

kappa = cohen_kappa_score(grades_A, grades_B, weights="linear")
# kappa = 0.75
```

**í•´ì„**:

```
Îº = 1.0      : ì™„ë²½í•œ ì¼ì¹˜
Îº = 0.8~1.0  : ë§¤ìš° ì¢‹ìŒ (ê°€ì´ë“œë¼ì¸ ìš°ìˆ˜)
Îº = 0.6~0.8  : ì¢‹ìŒ (ì¼ë¶€ ì¡°ì • í•„ìš”)
Îº = 0.4~0.6  : ë³´í†µ (ê°€ì´ë“œë¼ì¸ ê°œì„  í•„ìš”)
Îº < 0.4      : ë‚˜ì¨ (ê°€ì´ë“œë¼ì¸ ì¬ì‘ì„±)
```

**ì¡°ì¹˜**:

```python
if kappa < 0.6:
    # ë¶ˆì¼ì¹˜ ì¼€ì´ìŠ¤ ë¶„ì„
    disagreements = find_large_disagreements(A, B, threshold=2)

    for case in disagreements:
        print(f"Doc: {case['doc_id']}")
        print(f"  A: {case['grade_A']} (ì´ìœ : {case['reason_A']})")
        print(f"  B: {case['grade_B']} (ì´ìœ : {case['reason_B']})")

        # í† ë¡  í›„ ê°€ì´ë“œë¼ì¸ ì—…ë°ì´íŠ¸
        guideline_update = discuss(case)

    # ì¬ë¼ë²¨ë§
    re_annotate_with_updated_guidelines()
```

### 6.3 Quality Check Metrics

```python
# 1. Coverage check
python scripts/golden_set/check_coverage.py \
  --qrels data/golden_set/qrels_dev.jsonl

# Output:
# Doc type distribution:
#   sop: 42% (target: 30-40%)
#   ts_guide: 28% (target: 25-30%)
#   maintenance_log: 18% (target: 15-20%)
#   setup: 12% (target: 10-15%)
#   âœ“ Balanced!
#
# Device distribution:
#   SUPRA: 52% (corpus: 50%)
#   Integer: 22% (corpus: 20%)
#   ...
#   âœ“ Representative!
#
# Grade distribution:
#   0: 65% (most are not relevant)
#   1: 12%
#   2: 13%
#   3: 10%
#   âœ“ Reasonable!
```

```python
# 2. Pool utilization
# Poolì—ì„œ ì‹¤ì œë¡œ ëª‡ ê°œê°€ relevantë¡œ ë¼ë²¨ë§ë˜ì—ˆë‚˜?

total_docs_in_pool = 40 queries Ã— 150 docs = 6,000
relevant_docs = sum(grade >= 1) = 600

recall_estimate = 600 / 6000 = 10%

# 10%ê°€ ì ë‹¹ (5~15% ê¶Œì¥)
# - ë„ˆë¬´ ë†’ìœ¼ë©´ (>20%): Pool ë„ˆë¬´ ì‘ìŒ, relevant ë†“ì¹  ìœ„í—˜
# - ë„ˆë¬´ ë‚®ìœ¼ë©´ (<5%): Pool ë„ˆë¬´ í¼, ë¹„íš¨ìœ¨
```

---

## 7. í”„ë¡œì íŠ¸ íŠ¹í™” ì‚¬í•­

### 7.1 ì§ˆë¬¸ 40ê°œ êµ¬ì„± ì „ëµ

**ìœ í˜•ë³„ ë¶„í¬**:

```yaml
1. ì ˆì°¨ ì§ˆë¬¸ (How-to): 15ê°œ
   ì˜ˆ: "RFID ì„¼ì„œ êµì²´ ì ˆì°¨ëŠ”?"
   Relevant: SOP (grade 3) + TS-guide (grade 2)

2. ì§„ë‹¨ ì§ˆë¬¸ (Why/What): 10ê°œ
   ì˜ˆ: "ì±”ë²„ ì˜¨ë„ê°€ ì•ˆ ì˜¬ë¼ê°€ëŠ” ì´ìœ ëŠ”?"
   Relevant: TS-guide (grade 3) + Log (grade 2)

3. ì‚¬ë¡€ ì§ˆë¬¸ (Past issues): 5ê°œ
   ì˜ˆ: "EFEM ë¡œë´‡ ì—ëŸ¬ E1234 í•´ê²° ì‚¬ë¡€"
   Relevant: Log (grade 3) + SOP (grade 2)

4. ë¶€í’ˆ ì •ë³´ (Spec/Location): 5ê°œ
   ì˜ˆ: "Pirani Gauge êµì²´ ì£¼ê¸°ëŠ”?"
   Relevant: Setup manual (grade 3)

5. Multi-hop ì§ˆë¬¸: 5ê°œ
   ì˜ˆ: "ë¡œê·¸ #40001648ì˜ ì—ëŸ¬ë¥¼ í•´ê²°í•˜ë ¤ë©´?"
   Relevant: Log (step 1) + SOP (step 2)
```

**ì¥ë¹„ë³„ ë¶„í¬** (ë¬¸ì„œ corpus ë¹„ìœ¨ ë°˜ì˜):

```yaml
SUPRA ì‹œë¦¬ì¦ˆ: 50% (20ê°œ)
Integer Plus: 20% (8ê°œ)
Precia: 15% (6ê°œ)
Geneva: 10% (4ê°œ)
ê¸°íƒ€: 5% (2ê°œ)
```

**ë‚œì´ë„ ë¶„í¬**:

```yaml
Easy (í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ í•´ê²°): 10ê°œ
  ì˜ˆ: "Pirani Gauge SOP"

Medium (ì˜ë¯¸ ì´í•´ í•„ìš”): 20ê°œ
  ì˜ˆ: "ì§„ê³µì´ ë–¨ì–´ì§€ëŠ” ì´ìœ "

Hard (ë©€í‹°í™‰/ë¬¸ë§¥ í•„ìš”): 10ê°œ
  ì˜ˆ: "ë¡œê·¸ì˜ ì—ëŸ¬ì½”ë“œë¡œ SOP ì°¾ê¸°"
```

### 7.2 Multi-hop ì§ˆë¬¸ ë¼ë²¨ë§

**íŠ¹ìˆ˜ í˜•ì‹**: ë‹¨ê³„ë³„ relevant ë¼ë²¨ë§

```json
{
  "query_id": "q036",
  "query_text": "ë¡œê·¸ #40001648ì˜ ì—ëŸ¬ë¥¼ í•´ê²°í•˜ë ¤ë©´ ì–´ë–¤ SOPë¥¼ ì°¸ê³ í•´ì•¼ í•˜ë‚˜?",
  "query_type": "multihop",

  "hops": [
    {
      "hop": 1,
      "sub_task": "ë¡œê·¸ì—ì„œ ì—ëŸ¬ì½”ë“œ/ì¦ìƒ ì¶”ì¶œ",
      "relevant": [
        {
          "chunk_id": "40001648#0001",
          "grade": 3,
          "rationale": "ì—ëŸ¬ ë°œìƒ ë¡œê·¸"
        }
      ]
    },
    {
      "hop": 2,
      "sub_task": "ì—ëŸ¬ì½”ë“œë¡œ SOP ê²€ìƒ‰",
      "relevant": [
        {
          "chunk_id": "global_sop_supra_vplus_pm_controller#0023",
          "grade": 3,
          "rationale": "Controller PM SOP"
        }
      ]
    }
  ],

  "relevance_judgments": [
    // Hop 1+2 í†µí•© (end-to-end í‰ê°€ìš©)
    {"chunk_id": "40001648#0001", "grade": 3},
    {"chunk_id": "global_sop_supra_vplus_pm_controller#0023", "grade": 3}
  ]
}
```

**í‰ê°€ ë°©ì‹**:

```python
# Option 1: End-to-end í‰ê°€ (ì „ì²´ relevantë¡œ í‰ê°€)
recall_e2e = len(retrieved âˆ© all_hops_relevant) / len(all_hops_relevant)

# Option 2: Hop-by-hop í‰ê°€ (ë‹¨ê³„ë³„ë¡œ í‰ê°€)
for hop in hops:
    recall_hop = len(retrieved âˆ© hop.relevant) / len(hop.relevant)
```

### 7.3 í•œêµ­ì–´ í˜•íƒœì†Œ ë³€í˜• í…ŒìŠ¤íŠ¸

**ëª©ì **: Nori analyzer íš¨ê³¼ ì¸¡ì •

```json
{
  "query_id": "q001",
  "query_text": "ì ê²€ ì ˆì°¨",
  "query_variants": [
    "ì ê²€ ì ˆì°¨",        // ì›í˜•
    "ì ê²€í•˜ëŠ” ì ˆì°¨",    // ë™ì‚¬í˜•
    "ì ê²€ì¤‘ì¸ ì ˆì°¨",    // ì§„í–‰í˜•
    "ì •ê¸°ì ê²€ ì ˆì°¨",    // ë³µí•©ì–´
    "ì ê²€í•˜ë‹¤"          // ë™ì‚¬ ì›í˜•
  ],

  "relevance_judgments": [
    // ë™ì¼í•œ qrels ì‚¬ìš©
  ]
}
```

**í‰ê°€**:

```python
# ê° variantì˜ recall ë¹„êµ
for variant in query_variants:
    results = retriever.search(variant, top_k=10)
    recall = calculate_recall(results, qrels)
    print(f"{variant}: Recall@10 = {recall:.2f}")

# Nori ìˆì„ ë•Œ:
# "ì ê²€ ì ˆì°¨": 0.90
# "ì ê²€í•˜ëŠ” ì ˆì°¨": 0.88 (slightly lower, but still good)
# "ì ê²€ì¤‘ì¸ ì ˆì°¨": 0.85
# â†’ Noriê°€ í˜•íƒœì†Œ ì •ê·œí™” ì˜ í•¨

# Nori ì—†ì„ ë•Œ (standard analyzer):
# "ì ê²€ ì ˆì°¨": 0.90
# "ì ê²€í•˜ëŠ” ì ˆì°¨": 0.45 (much lower!)
# "ì ê²€ì¤‘ì¸ ì ˆì°¨": 0.30
# â†’ í˜•íƒœì†Œ ë³€í˜•ì— ì·¨ì•½
```

### 7.4 Hybrid Weight ë¶„ì„

**ëª©ì **: Queryë³„ë¡œ BM25/Dense ì¤‘ ì–´ëŠ ìª½ì´ ë” ì¢‹ì€ì§€ íŒŒì•…

```python
def analyze_method_preference(qrels, retriever):
    """Queryë³„ë¡œ ì–´ë–¤ methodê°€ ë” ì˜ ì°¾ì•˜ëŠ”ì§€"""

    for query_id, judgments in qrels.items():
        query = get_query_text(query_id)
        relevant_ids = {j["chunk_id"] for j in judgments if j["grade"] >= 2}

        # BM25 vs Dense ë¹„êµ
        bm25_results = retriever.search(query, method="bm25", top_k=10)
        dense_results = retriever.search(query, method="dense", top_k=10)

        bm25_recall = len(relevant_ids & set(bm25_results)) / len(relevant_ids)
        dense_recall = len(relevant_ids & set(dense_results)) / len(relevant_ids)

        if bm25_recall > dense_recall + 0.1:
            print(f"{query_id}: BM25-favored (keyword-heavy)")
            # â†’ sparse_weight ì˜¬ë ¤ì•¼ í•¨
        elif dense_recall > bm25_recall + 0.1:
            print(f"{query_id}: Dense-favored (semantic)")
            # â†’ dense_weight ì˜¬ë ¤ì•¼ í•¨
        else:
            print(f"{query_id}: Balanced")

# ê²°ê³¼ ì˜ˆì‹œ:
# q001 (RFID ì„¼ì„œ êµì²´): BM25-favored (specific term)
# q005 (ì˜¨ë„ê°€ ì•ˆ ì˜¬ë¼ê°€ëŠ” ì´ìœ ): Dense-favored (semantic)
# q010 (ì—ëŸ¬ E1234): BM25-favored (error code)
#
# â†’ Dynamic weighting ì „ëµ ìˆ˜ë¦½ ê°€ëŠ¥
#   - Error code í¬í•¨ â†’ sparse_weight 0.5
#   - Why/How ì§ˆë¬¸ â†’ dense_weight 0.8
```

---

## 8. ì‹¤í–‰ ê³„íš

### Phase 0: ì¤€ë¹„ (1ì¼)

**ì‘ì—… ë‚´ìš©**:

```bash
# 1. ì§ˆë¬¸ 40ê°œ ì¤€ë¹„
python scripts/golden_set/prepare_queries.py \
  --source user_logs \
  --output data/golden_set/queries.jsonl \
  --distribution "procedure:15,diagnosis:10,case:5,spec:5,multihop:5"

# 2. Annotation ê°€ì´ë“œë¼ì¸ ì‘ì„±
# â†’ docs/golden_set_annotation_guidelines.md

# 3. ë„êµ¬ ì„¤ì¹˜
pip install sentence-transformers scikit-learn
```

**ì‚°ì¶œë¬¼**:
- `data/golden_set/queries.jsonl`: 40ê°œ ì§ˆë¬¸
- `docs/golden_set_annotation_guidelines.md`: ë¼ë²¨ë§ ê°€ì´ë“œ
- Annotation ìŠ¤í¬ë¦½íŠ¸ ì¤€ë¹„

---

### Phase 1: Pooling (2ì¼)

**ì‘ì—… ë‚´ìš©**:

```bash
# ê° queryì— ëŒ€í•´ pool ìƒì„±
python scripts/golden_set/create_pools.py \
  --queries data/golden_set/queries.jsonl \
  --output data/golden_set/pools/ \
  --methods "bm25,dense,hybrid,stratified,tag-based" \
  --pool-size 150

# ì‹¤í–‰ ì˜ˆì‹œ
# Query q001: "RFID ì„¼ì„œ êµì²´ ì ˆì°¨"
#   BM25: 50 docs
#   Dense: 50 docs
#   Hybrid: 50 docs
#   Stratified: 95 docs (sop:30, ts:25, log:20, setup:20)
#   Tag-based: 40 docs (tags: RFID, sensor, PM)
#   â†’ Dedup: 147 docs
#
# Progress: [40/40] 100% | ETA: 0s
# Saved pools to data/golden_set/pools/*.jsonl
```

**ì‚°ì¶œë¬¼**:
- `data/golden_set/pools/q001.jsonl` ~ `q040.jsonl`: ê° ì§ˆë¬¸ë‹¹ pool
- `data/golden_set/pool_stats.json`: Pool í†µê³„

---

### Phase 2: Annotation (1ì£¼)

#### 2.1 ì£¼ Annotator ì‘ì—… (5ì¼)

**ë°©ë²• A: CLI ë„êµ¬**

```bash
python scripts/golden_set/annotate.py \
  --queries data/golden_set/queries.jsonl \
  --pools data/golden_set/pools/ \
  --output data/golden_set/qrels_annotator_A.jsonl \
  --annotator engineer_kim \
  --mode interactive

# ì§„í–‰ ìƒí™©
# [Query 1/40] RFID ì„¼ì„œ êµì²´ ì ˆì°¨
#   [Doc 1/147] ... Grade: 3 âœ“
#   [Doc 2/147] ... Grade: 0 âœ“
#   ...
#   Progress: 8/147 (5.4%) | Time: 3m | ETA: 52m
```

**ë°©ë²• B: LLM ë³´ì¡° (ê¶Œì¥)**

```bash
python scripts/golden_set/annotate.py \
  --queries data/golden_set/queries.jsonl \
  --pools data/golden_set/pools/ \
  --output data/golden_set/qrels_annotator_A.jsonl \
  --annotator engineer_kim \
  --mode llm-assisted \
  --llm-endpoint http://localhost:8003/v1

# LLMì´ ë¨¼ì € draft ìƒì„± (1ì‹œê°„)
# â†’ Annotatorê°€ ê²€í†  (15ì‹œê°„)
# â†’ ì´ 16ì‹œê°„ (vs. ìˆœìˆ˜ ì‚¬ëŒ: 33ì‹œê°„)
```

#### 2.2 ê²€ì¦ Annotator ì‘ì—… (2ì¼)

```bash
# 8ê°œ ì§ˆë¬¸(20%)ë§Œ ì¤‘ë³µ ë¼ë²¨ë§
python scripts/golden_set/annotate.py \
  --queries data/golden_set/queries_overlap.jsonl \
  --pools data/golden_set/pools/ \
  --output data/golden_set/qrels_annotator_B.jsonl \
  --annotator engineer_park \
  --mode interactive
```

**ì‚°ì¶œë¬¼**:
- `data/golden_set/qrels_annotator_A.jsonl`: 40ê°œ ì „ì²´
- `data/golden_set/qrels_annotator_B.jsonl`: 8ê°œ ì¤‘ë³µ

---

### Phase 3: Quality Check (2ì¼)

```bash
# 1. Inter-annotator agreement
python scripts/golden_set/check_agreement.py \
  --annotator-a data/golden_set/qrels_annotator_A.jsonl \
  --annotator-b data/golden_set/qrels_annotator_B.jsonl \
  --output data/golden_set/agreement_report.json

# Output:
# Cohen's Kappa: 0.78 (Good!)
# Disagreements: 12 cases
# - q003, doc#45: A=2, B=1 (minor)
# - q007, doc#89: A=3, B=0 (major!) â† Review needed
```

```bash
# 2. Coverage check
python scripts/golden_set/check_coverage.py \
  --qrels data/golden_set/qrels_annotator_A.jsonl \
  --output data/golden_set/coverage_report.json

# Output:
# Doc type distribution:
#   sop: 40% âœ“
#   ts_guide: 28% âœ“
#   maintenance_log: 18% âœ“
#   setup: 14% âœ“
#
# Grade distribution:
#   Grade 3: 10%
#   Grade 2: 13%
#   Grade 1: 12%
#   Grade 0: 65%
#   âœ“ Reasonable!
```

```bash
# 3. ë¶ˆì¼ì¹˜ í•´ê²°
python scripts/golden_set/resolve_disagreements.py \
  --disagreements data/golden_set/agreement_report.json \
  --output data/golden_set/qrels_resolved.jsonl

# Interactive mode:
# Case 1: q007, doc#89
#   Annotator A: Grade 3 (ì´ìœ : "ì ˆì°¨ í¬í•¨")
#   Annotator B: Grade 0 (ì´ìœ : "ë‹¤ë¥¸ ì¥ë¹„")
#
# ğŸ‘¤ Expert decision:
#   [a] Aê°€ ë§ìŒ
#   [b] Bê°€ ë§ìŒ
#   [c] ë‘˜ ë‹¤ ì•„ë‹˜ (ìƒˆë¡œ íŒë‹¨)
#
# ì…ë ¥> b  âœ“ Saved!
```

**ì‚°ì¶œë¬¼**:
- `data/golden_set/qrels_final.jsonl`: ë¶ˆì¼ì¹˜ í•´ê²°ëœ ìµœì¢… qrels
- `data/golden_set/agreement_report.json`: í’ˆì§ˆ ë³´ê³ ì„œ

---

### Phase 4: Dev/Test Split (0.5ì¼)

```bash
# Stratified split (query_type ë¹„ìœ¨ ìœ ì§€)
python scripts/golden_set/split_dev_test.py \
  --qrels data/golden_set/qrels_final.jsonl \
  --dev data/golden_set/qrels_dev.jsonl \
  --test data/golden_set/qrels_test_v1.jsonl \
  --split 0.75 \
  --stratify query_type \
  --seed 42

# Output:
# Dev set: 30 queries
#   - procedure: 12
#   - diagnosis: 8
#   - case: 4
#   - spec: 4
#   - multihop: 2
#
# Test set: 10 queries (FROZEN!)
#   - procedure: 3
#   - diagnosis: 2
#   - case: 1
#   - spec: 1
#   - multihop: 3
```

**ë²„ì „ ê´€ë¦¬**:

```bash
# TestëŠ” gitìœ¼ë¡œ ë²„ì „ ê´€ë¦¬
git add data/golden_set/qrels_test_v1.jsonl
git commit -m "feat: Add test golden set v1 (frozen)"
git tag golden-set-test-v1

# DevëŠ” ê³„ì† í™•ì¥ ê°€ëŠ¥
# (ìƒˆ query/document ì¶”ê°€)
```

**ì‚°ì¶œë¬¼**:
- `data/golden_set/qrels_dev.jsonl`: 30 queries (í™•ì¥ ê°€ëŠ¥)
- `data/golden_set/qrels_test_v1.jsonl`: 10 queries (frozen)

---

### Phase 5: í‰ê°€ ì‹¤í–‰ (ë°˜ë³µ)

```bash
# Baseline í‰ê°€
python scripts/golden_set/evaluate.py \
  --qrels data/golden_set/qrels_test_v1.jsonl \
  --retriever-config configs/retrieval/baseline.yaml \
  --output results/baseline_test_v1.json

# Output: results/baseline_test_v1.json
{
  "config": {
    "method": "hybrid",
    "dense_weight": 0.7,
    "sparse_weight": 0.3,
    "re_ranker": null
  },

  "metrics": {
    "recall@5": 0.72,
    "recall@10": 0.85,
    "recall@20": 0.92,
    "ndcg@5": 0.68,
    "ndcg@10": 0.71,
    "mrr": 0.65
  },

  "by_query_type": {
    "procedure": {
      "recall@5": 0.78,
      "ndcg@5": 0.72
    },
    "diagnosis": {
      "recall@5": 0.65,
      "ndcg@5": 0.60
    },
    "multihop": {
      "recall@5": 0.55,
      "ndcg@5": 0.52
    }
  },

  "by_doc_type": {
    "sop": {"precision@5": 0.82},
    "ts_guide": {"precision@5": 0.71},
    "maintenance_log": {"precision@5": 0.58}
  }
}
```

**ê°œì„  ì‹¤í—˜ ì˜ˆì‹œ**:

```bash
# ì‹¤í—˜ 1: Dense weight ì˜¬ë¦¬ê¸°
python scripts/golden_set/evaluate.py \
  --qrels data/golden_set/qrels_test_v1.jsonl \
  --retriever-config configs/retrieval/dense_heavy.yaml \
  --output results/dense_heavy_test_v1.json

# ì‹¤í—˜ 2: Re-ranker ì¶”ê°€
python scripts/golden_set/evaluate.py \
  --qrels data/golden_set/qrels_test_v1.jsonl \
  --retriever-config configs/retrieval/with_reranker.yaml \
  --output results/with_reranker_test_v1.json

# ë¹„êµ
python scripts/golden_set/compare_results.py \
  --baseline results/baseline_test_v1.json \
  --experiments results/dense_heavy_test_v1.json \
                results/with_reranker_test_v1.json \
  --output results/comparison.md
```

**ë¹„êµ ê²°ê³¼**:

| Config | Recall@10 | nDCG@10 | ë³€í™” |
|--------|-----------|---------|------|
| Baseline (0.7/0.3) | 0.85 | 0.71 | - |
| Dense heavy (0.8/0.2) | 0.87 | 0.73 | +2.4% âœ“ |
| With re-ranker | 0.88 | 0.78 | +9.9% âœ“âœ“ |

---

## 9. ë¹„ìš© ë° ì‹œê°„ ì¶”ì •

### 9.1 ì¸ë ¥ íˆ¬ì…

| Phase | ì‘ì—… | ì¸ì› | ê¸°ê°„ | Person-days |
|-------|------|------|------|-------------|
| 0. ì¤€ë¹„ | ì§ˆë¬¸/ê°€ì´ë“œë¼ì¸ | 1 | 1ì¼ | 1 |
| 1. Pooling | Pool ìƒì„± | 1 | 2ì¼ | 2 |
| 2. Annotation | ì£¼ annotator | 1 | 5ì¼ | 5 |
| | ê²€ì¦ annotator | 1 | 2ì¼ | 2 |
| 3. Quality | Agreement/Coverage | 1 | 2ì¼ | 2 |
| 4. Split | Dev/Test ë¶„ë¦¬ | 1 | 0.5ì¼ | 0.5 |
| **í•©ê³„** | | | | **12.5** |

**ì‹¤ì œ ì†Œìš” ê¸°ê°„**: 2ì£¼ (2ì¸ ë³‘ë ¬ ì‘ì—…)

### 9.2 ë¹„ìš©

#### Option A: ìˆœìˆ˜ ì‚¬ëŒ

```
ì£¼ annotator: 33ì‹œê°„ Ã— 5ë§Œì›/ì‹œê°„ = 165ë§Œì›
ê²€ì¦ annotator: 7ì‹œê°„ Ã— 5ë§Œì›/ì‹œê°„ = 35ë§Œì›

í•©ê³„: 200ë§Œì›
```

#### Option B: LLM ë³´ì¡° (ê¶Œì¥)

```
LLM ë¹„ìš©:
- 4,000 docs Ã— 500 tokens = 2M input tokens
- 4,000 grades = 4K output tokens
- GPT-4o-mini: $0.15/1M in + $0.60/1M out
- ì´: $0.30 + $0.003 = $0.30 (400ì›)

ì‚¬ëŒ ë¹„ìš©:
- ì£¼ annotator: 16ì‹œê°„ Ã— 5ë§Œì› = 80ë§Œì› (55% ì ˆì•½!)
- ê²€ì¦ annotator: 7ì‹œê°„ Ã— 5ë§Œì› = 35ë§Œì›

í•©ê³„: 115ë§Œì› + 400ì› = ì•½ 115ë§Œì›

ì ˆì•½: 85ë§Œì› (42%)
```

### 9.3 ì»´í“¨íŒ… ë¹„ìš©

```
Pooling (ES query): ë¬´ì‹œ ê°€ëŠ¥ (<$1)
Dense embedding: 40 queries Ã— 768 dim = ë¬´ì‹œ ê°€ëŠ¥
í‰ê°€ (ranx/pytrec_eval): Local, ë¬´ë£Œ

ì´: <$1
```

### 9.4 ì´ ë¹„ìš©

**ì¶”ì²œ ë°©ì‹ (LLM ë³´ì¡°)**: 115ë§Œì› + ì¸í”„ë¼ <$1 = **ì•½ 115ë§Œì›**

---

## 10. ì‹¤ë¬´ í•¨ì • ë° ë³´ì™„ âš ï¸

ì´ ì„¹ì…˜ì€ IR í‰ê°€ ì‹¤ë¬´ì—ì„œ í”íˆ ê²ªëŠ” í•¨ì •ë“¤ê³¼ ëŒ€ì‘ ë°©ë²•ì„ ì •ë¦¬í•¨.

### 10.1 Incomplete Qrels (ë¶ˆì™„ì „í•œ ì •ë‹µ ë°ì´í„°) ë¬¸ì œ

**ë¬¸ì œ**: Pooling ë°©ì‹ì€ êµ¬ì¡°ì ìœ¼ë¡œ "unjudged documents"ë¥¼ ë§Œë“¦

```
Poolì— í¬í•¨ ì•ˆ ëœ ë¬¸ì„œ = ë¼ë²¨ë§ ì•ˆ ë¨
â†’ í‰ê°€ ì‹œ "0ì (not relevant)"ìœ¼ë¡œ ê°„ì£¼ë¨
â†’ ì‹¤ì œë¡œëŠ” relevantì¼ ìˆ˜ ìˆìŒ (false negative)
```

**ëŒ€ì‘ A: Judged@k ì§€í‘œë¥¼ í•­ìƒ í•¨ê»˜ ë¦¬í¬íŠ¸ (í•„ìˆ˜)**

```python
def calculate_judged_at_k(results: list, qrels: dict, k: int) -> float:
    """ìƒìœ„ kê°œ ì¤‘ ë¼ë²¨ë§ëœ ë¹„ìœ¨"""
    top_k_docs = results[:k]
    judged_count = sum(1 for doc in top_k_docs if doc.id in qrels)
    return judged_count / k

# í‰ê°€ ë¦¬í¬íŠ¸ ì˜ˆì‹œ
{
  "recall@10": 0.85,
  "judged@10": 0.90,  # â† ìƒìœ„ 10ê°œ ì¤‘ 90%ë§Œ ë¼ë²¨ë§ë¨
  "unjudged@10": 0.10  # 10%ëŠ” íŒë‹¨ ë¶ˆê°€
}
```

**í•´ì„**:
- `judged@10` < 0.7: Pool ë„ˆë¬´ ì‘ìŒ, recall ì‹ ë¢° ë¶ˆê°€
- `judged@10` > 0.9: ì ì ˆí•¨

**ëŒ€ì‘ B: Unjudged-robust ì§€í‘œ ë³‘í–‰ (ê¶Œì¥)**

```python
# Bpref: unjudgedì— ëœ ë¯¼ê°í•œ ì§€í‘œ
from ranx import Qrels, Run, evaluate

qrels = Qrels(qrels_dict)
run = Run(results_dict)

metrics = evaluate(qrels, run, ["bpref", "ndcg@10", "recall@10"])
# bpref: unjudgedë¥¼ ë¬´ì‹œí•˜ê³  relevant/non-relevantë§Œ ë¹„êµ
```

**ë„êµ¬**:
- `pytrec_eval`: bpref ì§€ì›
- `ranx`: ë¹ ë¥¸ IR í‰ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬
- `ir_measures`: 50+ ì§€í‘œ ì§€ì›

### 10.2 Graded Relevance ì •ì˜ ì¶©ëŒ ë°©ì§€

**ì¶©ëŒ A: Grade 3(í•„ìˆ˜) ë‚¨ë°œ**

```
Multi-hopì—ì„œ:
- Log: ì—ëŸ¬ í™•ì¸ (hop 1 í•„ìˆ˜)
- TS-guide: ì§„ë‹¨ (hop 2 í•„ìˆ˜)
- SOP: ì¡°ì¹˜ (hop 3 í•„ìˆ˜)

â†’ ëª¨ë‘ Grade 3? nDCG ë³€ë³„ë ¥ ì €í•˜!
```

**í•´ê²°ì±…**:

```yaml
ì›ì¹™: Grade 3ì€ "ìµœì¢… ì¡°ì¹˜/ì •ë‹µ"ë§Œ

Multi-hop:
  - Hop 1,2 (ì¤‘ê°„ ë‹¨ê³„): Grade 2
  - Hop 3 (ìµœì¢… í•´ê²°): Grade 3

ë‹¨ì¼ ì§ˆë¬¸:
  - ì§ì ‘ ë‹µ: Grade 3
  - ì‚¬ì „ ì‘ì—…/ë°°ê²½: Grade 2
  - ì°¸ê³  ì‚¬ë¡€: Grade 1
```

**ì¶©ëŒ B: ì¥ë¹„/ë²„ì „ ì°¨ì´ ì²˜ë¦¬**

```
ì§ˆë¬¸: "SUPRA XP RFID êµì²´"
ë¬¸ì„œ: "SUPRA N RFID êµì²´"

ì ˆì°¨ ë™ì¼ â†’ Grade?
ì ˆì°¨ ìœ ì‚¬ â†’ Grade?
ì ˆì°¨ ë‹¤ë¦„ â†’ Grade?
```

**í•´ê²°ì±… (ì•ˆì „ ë¦¬ìŠ¤í¬ ê³ ë ¤)**:

```yaml
ì €ìœ„í—˜ ì‘ì—… (ì²­ì†Œ/ì ê²€):
  - ë™ì¼ ì‹œë¦¬ì¦ˆ: Grade 3
  - ë‹¤ë¥¸ ì‹œë¦¬ì¦ˆ (ì ˆì°¨ ë™ì¼): Grade 2
  - ë‹¤ë¥¸ ì‹œë¦¬ì¦ˆ (ì ˆì°¨ ìœ ì‚¬): Grade 1

ê³ ìœ„í—˜ ì‘ì—… (ì „ì›/ê°€ìŠ¤/ì§„ê³µ):
  - ë™ì¼ ì‹œë¦¬ì¦ˆë§Œ: Grade 3
  - ë‹¤ë¥¸ ì‹œë¦¬ì¦ˆ: Grade 0~1 (ë³´ìˆ˜ì )
  â†’ ì•ˆì „ ì‚¬ê³  ì˜ˆë°© ìš°ì„ 
```

### 10.3 í‰ê°€ ë„êµ¬ ì„ íƒ

**ë¬¸ì œ**: `sentence-transformers.InformationRetrievalEvaluator`ëŠ” í•˜ì´ë¸Œë¦¬ë“œ í‰ê°€ì— ë¶ˆí¸

```python
# sentence-transformersëŠ” embedding ì¤‘ì‹¬
evaluator = InformationRetrievalEvaluator(...)
# â†’ BM25, hybrid, re-ranking í‰ê°€ê°€ ì–´ìƒ‰í•¨
```

**ê¶Œì¥: Run-based í‰ê°€ (TREC ë°©ì‹)**

```python
# Step 1: Retriever ê²°ê³¼ë¥¼ "run" íŒŒì¼ë¡œ ì €ì¥
run = {}
for query_id, query_text in queries.items():
    results = retriever.search(query_text, top_k=100)
    run[query_id] = {doc.id: score for doc, score in results}

# Step 2: Qrels + Runìœ¼ë¡œ í‰ê°€
from ranx import Qrels, Run, evaluate

qrels = Qrels.from_file("qrels_test_v1.trec")
run = Run(run)

metrics = evaluate(
    qrels, run,
    ["ndcg@10", "recall@10", "mrr", "bpref", "judged@10"]
)
```

**íš¨ê³¼**:
- BM25/Dense/Hybrid/Re-rank ëª¨ë‘ ë™ì¼ í”„ë ˆì„ì›Œí¬
- ì‹¤í—˜ ìë™í™” ì‰¬ì›€
- TREC í‘œì¤€ ì¤€ìˆ˜

**ë„êµ¬ ë¹„êµ**:

| ë„êµ¬ | ì¥ì  | ë‹¨ì  |
|------|------|------|
| sentence-transformers | ê°„ë‹¨, embedding í†µí•© | Hybrid í‰ê°€ ë¶ˆí¸ |
| pytrec_eval | í‘œì¤€, bpref ì§€ì› | Python ë°”ì¸ë”© ëŠë¦¼ |
| ranx | ë¹ ë¦„, í˜„ëŒ€ì  API | ì‹ ê·œ ë¼ì´ë¸ŒëŸ¬ë¦¬ |
| ir_measures | 50+ ì§€í‘œ | ì˜ì¡´ì„± ë§ìŒ |

**ê¶Œì¥**: `ranx` (ë¹ ë¥´ê³  ì‚¬ìš© ì‰¬ì›€)

### 10.4 íŒŒì¼ëŸ¿ ìš°ì„  ì ‘ê·¼ (ì‹¤íŒ¨ í™•ë¥  ìµœì†Œí™”)

**ë¬¸ì œ**: 40ê°œ ì „ì²´ ë¼ë²¨ë§ í›„ "ê°€ì´ë“œë¼ì¸ ì˜ëª»ëìŒ" ë°œê²¬ â†’ ì¬ì‘ì—…

**í•´ê²°: 6~8ê°œ íŒŒì¼ëŸ¿ ë¨¼ì €**

```
Phase 0: íŒŒì¼ëŸ¿ (3ì¼)
  - 6~8ê°œ ì§ˆë¬¸ ì„ íƒ (ìœ í˜•ë³„ ê³¨ê³ ë£¨)
  - Pool ìƒì„± â†’ ë¼ë²¨ë§ â†’ Agreement í™•ì¸
  - Disagreement ë¶„ì„ â†’ ê°€ì´ë“œë¼ì¸ ìˆ˜ì •

Phase 1: ì „ì²´ í™•ì¥ (1ì£¼)
  - ìˆ˜ì •ëœ ê°€ì´ë“œë¼ì¸ìœ¼ë¡œ 32ê°œ ì¶”ê°€
  - Dev/Test split

íš¨ê³¼:
  - ì¬ì‘ì—… ë¦¬ìŠ¤í¬ 80% ê°ì†Œ
  - ê°€ì´ë“œë¼ì¸ í’ˆì§ˆ í™•ë³´
  - ë„êµ¬/í”„ë¡œì„¸ìŠ¤ ê²€ì¦
```

**íŒŒì¼ëŸ¿ ì§ˆë¬¸ ì„ íƒ ê¸°ì¤€**:

```yaml
í•„ìˆ˜ í¬í•¨:
  - Procedure: 2ê°œ (ì‰¬ì›€/ì–´ë ¤ì›€ ê° 1)
  - Diagnosis: 2ê°œ
  - Multi-hop: 1ê°œ
  - Log search: 1ê°œ

ì¥ë¹„ ë¶„í¬:
  - SUPRA: 3ê°œ
  - Integer: 2ê°œ
  - ê¸°íƒ€: 1ê°œ

ë‚œì´ë„:
  - Easy: 2ê°œ
  - Medium: 3ê°œ
  - Hard: 1ê°œ
```

### 10.5 Doc-type ë„¤ì´ë° í†µì¼ âš ï¸

**ë¬¸ì œ ë°œê²¬**: ë¬¸ì„œì—ì„œ `myservice` / `maintenance_log` í˜¼ìš©

```python
# ES ì‹¤ì œ í•„ë“œê°’
doc_type = "maintenance_log"  # â† ì‹¤ì œ ê°’

# ë¬¸ì„œ ì˜ˆì‹œì—ì„œ
"myservice": 20,  # â† ë‹¤ë¥¸ ì´ë¦„

â†’ ì½”ë“œ ì‘ì„± ì‹œ í˜¼ë€!
```

**í•´ê²°**:

```python
# í˜„ì¬ í”„ë¡œì íŠ¸ ì‹¤ì œ doc_type í™•ì¸
es.search(
    index="rag_chunks_dev_current",
    body={
        "size": 0,
        "aggs": {"types": {"terms": {"field": "doc_type.keyword"}}}
    }
)

# ê²°ê³¼ì— ë”°ë¼ ë¬¸ì„œ í†µì¼
# ì˜ˆ: "maintenance_log"ë¡œ í†µì¼
```

**ê¶Œì¥ ë„¤ì´ë° (í™•ì¸ í›„ ê²°ì •)**:

```python
DOC_TYPES = {
    "sop": "SOP (Standard Operating Procedure)",
    "ts_guide": "Troubleshooting Guide",
    "maintenance_log": "Maintenance Log",
    "setup": "Setup Manual"
}
```

---

## 11. Answer Quality í‰ê°€ í™•ì¥

Retrieval í‰ê°€ë§Œìœ¼ë¡œëŠ” ë¶ˆì¶©ë¶„. ìµœì¢… ë‹µë³€ í’ˆì§ˆë„ í‰ê°€ í•„ìš”.

### 11.1 Answer Rubric ì¶”ê°€

**ë°©ë²•**: qrelsì— "ë‹µë³€ì´ í¬í•¨í•´ì•¼ í•  ë‚´ìš©" ì²´í¬ë¦¬ìŠ¤íŠ¸ ì¶”ê°€

```json
{
  "query_id": "q001",
  "query_text": "RFID ì„¼ì„œ êµì²´ ì ˆì°¨ëŠ”?",

  "relevance_judgments": [...],  // ê¸°ì¡´ retrieval qrels

  "answer_rubric": {
    "must_have": [
      "ì „ì› ì°¨ë‹¨ ë° LOTO í™•ì¸",
      "ì„¼ì„œ íƒˆê±° ì ˆì°¨ (ë‚˜ì‚¬ ì œê±°, ì¼€ì´ë¸” ë¶„ë¦¬)",
      "ìƒˆ ì„¼ì„œ ì¥ì°© ì ˆì°¨",
      "ë™ì‘ í™•ì¸ ë° ìº˜ë¦¬ë¸Œë ˆì´ì…˜"
    ],
    "must_not_have": [
      "ì „ì› ON ìƒíƒœì—ì„œ ì‘ì—… ì§€ì‹œ",
      "ë‹¤ë¥¸ ì¥ë¹„ ì‹œë¦¬ì¦ˆ ì ˆì°¨ í˜¼ìš©"
    ],
    "quality_criteria": {
      "safety_check": "í•„ìˆ˜",
      "step_by_step": "í•„ìˆ˜",
      "verification": "í•„ìˆ˜"
    }
  }
}
```

### 11.2 Oracle vs Retrieved 2íŠ¸ë™ í‰ê°€

```python
# Track A: Oracle (ê²€ìƒ‰ í’ˆì§ˆ ì˜í–¥ ì œê±°)
oracle_context = get_docs_by_ids(qrels[query_id].relevant_docs)
oracle_answer = llm.generate(query, context=oracle_context)

# Track B: Retrieved (ì‹¤ì œ E2E)
retrieved_context = retriever.search(query, top_k=5)
retrieved_answer = llm.generate(query, context=retrieved_context)

# í‰ê°€
oracle_score = evaluate_answer(oracle_answer, rubric)
retrieved_score = evaluate_answer(retrieved_answer, rubric)

# ë¶„ì„
if retrieved_score < oracle_score:
    print("ê²€ìƒ‰ì´ ë¬¸ì œ")
else:
    print("ìƒì„± ëª¨ë¸/í”„ë¡¬í”„íŠ¸ ê°œì„  í•„ìš”")
```

### 11.3 í‰ê°€ ì§€í‘œ

```python
# Rubric-based í‰ê°€
def evaluate_answer(answer: str, rubric: dict) -> dict:
    scores = {}

    # Must-have check
    must_have_count = sum(
        1 for item in rubric["must_have"]
        if item.lower() in answer.lower()
    )
    scores["must_have_coverage"] = must_have_count / len(rubric["must_have"])

    # Must-not-have check
    must_not_violations = sum(
        1 for item in rubric["must_not_have"]
        if item.lower() in answer.lower()
    )
    scores["safety_violations"] = must_not_violations

    # LLM-as-judge (optional)
    scores["llm_judge"] = llm_judge_quality(answer, rubric)

    return scores
```

**Test Rubricë„ freeze**:

```
Dev rubric: ê³„ì† í™•ì¥ ê°€ëŠ¥
Test rubric: v1 freeze (retrieval testì™€ ë™ì¼)
```

---

## 12. ë¶€ë¡

### 12.1 ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
llm-agent-v2/
â”œâ”€ data/
â”‚  â””â”€ golden_set/
â”‚     â”œâ”€ queries.jsonl                  # 40ê°œ ì§ˆë¬¸
â”‚     â”œâ”€ queries_overlap.jsonl          # ì¤‘ë³µ ë¼ë²¨ë§ìš© 8ê°œ
â”‚     â”œâ”€ pools/
â”‚     â”‚  â”œâ”€ q001.jsonl                  # Queryë³„ pool
â”‚     â”‚  â”œâ”€ q002.jsonl
â”‚     â”‚  â””â”€ ...
â”‚     â”œâ”€ qrels_dev.jsonl                # Dev set (30ê°œ)
â”‚     â”œâ”€ qrels_test_v1.jsonl            # Test set v1 (10ê°œ, frozen)
â”‚     â”œâ”€ qrels_test_v1.trec             # TREC format
â”‚     â””â”€ reports/
â”‚        â”œâ”€ agreement_report.json
â”‚        â””â”€ coverage_report.json
â”‚
â”œâ”€ scripts/
â”‚  â””â”€ golden_set/
â”‚     â”œâ”€ prepare_queries.py             # Phase 0
â”‚     â”œâ”€ create_pools.py                # Phase 1
â”‚     â”œâ”€ annotate.py                    # Phase 2
â”‚     â”œâ”€ check_agreement.py             # Phase 3
â”‚     â”œâ”€ check_coverage.py              # Phase 3
â”‚     â”œâ”€ resolve_disagreements.py       # Phase 3
â”‚     â”œâ”€ split_dev_test.py              # Phase 4
â”‚     â”œâ”€ evaluate.py                    # Phase 5
â”‚     â”œâ”€ compare_results.py             # Phase 5
â”‚     â””â”€ export_trec.py                 # ë³€í™˜ ë„êµ¬
â”‚
â”œâ”€ docs/
â”‚  â”œâ”€ golden_set_annotation_guidelines.md   # Annotation ê°€ì´ë“œ
â”‚  â””â”€ 2026-01-07_retrieval_golden_set_strategy.md  # ì´ ë¬¸ì„œ
â”‚
â””â”€ results/
   â”œâ”€ baseline_test_v1.json
   â”œâ”€ dense_heavy_test_v1.json
   â””â”€ comparison.md
```

### 12.2 ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬

```bash
# í•„ìˆ˜ (í‰ê°€)
pip install ranx                   # Run-based IR í‰ê°€ (ê¶Œì¥, 10.3ì ˆ ì°¸ê³ )
pip install scikit-learn           # cohen_kappa_score (Inter-annotator agreement)

# í•„ìˆ˜ (ê²€ìƒ‰)
pip install sentence-transformers  # Dense embedding

# Optional (LLM ë³´ì¡°)
pip install openai                 # LLM draft annotation

# Optional (ì¶”ê°€ í‰ê°€ ì§€í‘œ)
pip install pytrec_eval            # bpref ë“± TREC í‘œì¤€ ì§€í‘œ
pip install ir_measures            # 50+ IR ì§€í‘œ

# í‰ê°€ ì˜ˆì‹œ (ê¶Œì¥)
from ranx import Qrels, Run, evaluate
metrics = evaluate(qrels, run, ["ndcg@10", "recall@10", "bpref"])
```

### 12.3 í‰ê°€ ì§€í‘œ ì •ì˜

#### Recall@k

```
Recall@k = (ìƒìœ„ kê°œì— í¬í•¨ëœ relevant ìˆ˜) / (ì „ì²´ relevant ìˆ˜)

ì˜ˆ:
- ì „ì²´ relevant: 5ê°œ
- ìƒìœ„ 10ê°œì— í¬í•¨: 4ê°œ
- Recall@10 = 4/5 = 0.8
```

#### nDCG@k (Normalized Discounted Cumulative Gain)

```
DCG@k = Î£(relevance_i / log2(position_i + 1))

ì˜ˆ:
ìœ„ì¹˜ 1: grade 3 â†’ 3 / log2(2) = 3.0
ìœ„ì¹˜ 2: grade 2 â†’ 2 / log2(3) = 1.26
ìœ„ì¹˜ 3: grade 0 â†’ 0 / log2(4) = 0
...

nDCG@k = DCG@k / Ideal_DCG@k
```

**íŠ¹ì§•**: Graded relevance ë°˜ì˜, ìƒìœ„ ë­í‚¹ ì¤‘ìš”

#### MRR (Mean Reciprocal Rank)

```
RR = 1 / (ì²« relevant ë¬¸ì„œ ìœ„ì¹˜)

ì˜ˆ:
- ì²« relevantê°€ 3ìœ„ â†’ RR = 1/3 = 0.33
- 40ê°œ ì§ˆë¬¸ í‰ê·  â†’ MRR
```

### 12.4 ì°¸ê³  ìë£Œ

**í•™ìˆ  ìë£Œ**:
- TREC (Text REtrieval Conference) methodology
- MS MARCO dataset êµ¬ì¶• ë°©ë²•ë¡ 
- BEIR benchmark annotation guidelines

**ë„êµ¬**:
- ranx (ê¶Œì¥): https://github.com/AmenRa/ranx
- pytrec_eval: https://github.com/cvangysel/pytrec_eval
- sentence-transformers: https://www.sbert.net/
- TREC format: https://trec.nist.gov/data/qrels_eng/

**í”„ë¡œì íŠ¸ ë¬¸ì„œ**:
- `docs/2026-01-02_retrieval review.md`: í˜„ì¬ retrieval ì•„í‚¤í…ì²˜
- `docs/2026-01-07_rlm_paper_review.md`: Multi-hop retrieval ê°œì„ 

---

## ë‹¤ìŒ ë‹¨ê³„

1. **ì§ˆë¬¸ 40ê°œ ì´ˆì•ˆ ì‘ì„±** (user logs ë¶„ì„)
2. **Annotation guidelines ë¬¸ì„œ ì‘ì„±**
3. **Pooling ìŠ¤í¬ë¦½íŠ¸ êµ¬í˜„** (ê¸°ì¡´ es_search_service í™•ì¥)
4. **Annotation ë„êµ¬ ì„ íƒ** (CLI vs Web UI)
5. **Phase 0~5 ì‹¤í–‰**

**ë¬¸ì˜**: ê° ë‹¨ê³„ êµ¬í˜„ ì‹œ ì¶”ê°€ ê°€ì´ë“œ í•„ìš” ì‹œ ìš”ì²­ ë°”ëŒ.
