# 리트리벌 아키텍처 진단 및 개선

## 현재 RAG 리트리벌 구성

### 환경 및 인덱스 구성

**Elasticsearch 구성:** 시스템은 희소(sparse) 및 밀집(dense) 리트리벌 모두에 Elasticsearch(아마 8.x)를 사용합니다. 개발 환경에서는 단일 노드 클러스터로 동작합니다(스냅샷 기준 `shards=1`, `replicas=1`). 인덱스 alias 전략(버전 인덱스 + current alias)은 코드상 준비되어 있으나, 실제로 alias가 버전 인덱스를 가리키는지 여부는 `_cat/aliases`로 별도 확인이 필요합니다.

**인덱스와 필드:** 주요 인덱스(= alias가 가리키는 실제 인덱스)는 문서 chunk를 저장합니다. 주요 속성은 다음과 같습니다.

* **Dense Vector 필드:** `embedding`(타입 `dense_vector`)은 임베딩 모델 차원(예: 768 또는 1024)과 일치하며, KNN 검색에서 **cosine similarity**를 사용합니다. (스냅샷 기준 `dims=768`, `index_options=int8_hnsw`)

* **텍스트 필드:** `content`(chunk의 전체 텍스트)와 결합 필드 `search_text`가 있습니다. 둘 다 기본적으로 `standard analyzer`를 사용합니다. `search_text`는 BM25 키워드 매칭을 위한 올인원(catch-all) 텍스트로 쓰기 위해 `content`, `title`, `tags`를 결합한 값입니다.

* **메타데이터 필드:** `device_name`(장비명), `doc_type`(SOP, 정비 로그 등 문서 카테고리), `chapter`(섹션/챕터 제목) 등은 필터링/컨텍스트 제공을 위한 메타데이터로 저장됩니다. 스냅샷 기준으로는 문자열 필드가 `text` + `.keyword` 멀티필드 형태이므로, 정확 매칭 필터는 `term`을 `.keyword`에 거는 것을 권장합니다(예: `device_name.keyword`).

* **LLM 요약/키워드:** 각 chunk는 LLM이 생성한 요약(`chunk_summary`)과 키워드를 가질 수 있습니다. 스냅샷 기준으로 `chunk_summary`/`doc_description`/`chunk_keywords`는 `text`로 검색 가능합니다. 키워드는 `tags` 또는 `chunk_keywords` 등으로 저장될 수 있으며, 실제 검색 기여는 “어떤 필드를 쿼리에 포함하는지(필드명/부스트/분석기)”에 좌우됩니다.

**인덱스 설정:** 현재 인덱스는 한국어 텍스트에 대해 기본 `standard analyzer`를 사용합니다. 코드상으로는 설정에서 한국어 형태소 분석을 위한 커스텀 **Nori analyzer**를 제공할 수 있도록 준비되어 있으나, 현재는 주석 처리되어 있습니다. 즉, 시스템이 기본 상태에서 한국어 전용 토크나이징을 활용하지 못하고 있습니다. 동의어(synonym) 필터나 커스텀 토크나이징 설정도 정의되어 있지 않습니다(매핑/설정 JSON에서 확인되지 않음).

---

## 쿼리 및 하이브리드 검색 전략

**Dense Vector 검색:** 의미 기반(semantic) 매칭을 위해, 시스템은 질의 임베딩(예: KoE5 또는 BGE 같은 임베딩 모델 사용)을 생성하고 `embedding` 필드에 대해 `kNN` 검색을 수행합니다. 인덱스는 효율적인 근사 kNN을 위해 내부적으로 HNSW(ES 8.x에서 `dense_vector`의 기본 방식)를 사용할 가능성이 큽니다. 일반적으로 top `k` 결과를 가져오고, re-scoring을 허용하기 위해 `num_candidates`를 더 크게 설정합니다(예: `2× top_k`). 질의 벡터는 검색 전에 L2 정규화를 수행하여 cosine similarity를 dot product로 계산할 수 있게 합니다.

**Sparse BM25 검색:** 동시에 텍스트 필드에 대해 키워드 검색을 수행합니다. 구현은 결합 필드 `search_text`에 대해 BM25 검색(`match` 또는 `multi_match` 사용)입니다. 원래 설계는 `content^2`, `search_text`, `doc_description`, `chunk_summary` 등에 서로 다른 boost를 주어 `multi_match`를 구성하는 것을 고려했었습니다(예: `content`에 더 높은 가중치). 스냅샷 기준으로 `chunk_summary`/`doc_description`는 검색 가능하므로, 품질(노이즈) 확인 후 낮은 weight로 포함하는 전략이 가능합니다. 또한 `chunk_keywords.text` 같은 서브필드는 스냅샷에 없으므로, 실제 매핑과 쿼리 필드명이 일치하도록 관리해야 합니다.

**하이브리드 스코어 결합:** 시스템은 dense와 sparse 결과를 결합하기 위해 하이브리드 검색을 사용합니다. 기본적으로 ES의 `script_score`를 사용해 BM25 점수와 cosine similarity 점수를 선형 결합합니다. 공식은 다음과 같습니다:

`score = dense_weight * cosineSimilarity(query_vector, 'embedding') + sparse_weight * _score + 1.0`

(`+1.0`은 BM25 점수가 낮거나 음수가 될 수 있을 때 전체 점수가 음수가 되는 것을 방지합니다.) 설정 기본값은 `dense_weight=0.7`, `sparse_weight=0.3`이며, 이는 키워드 매칭보다 의미 매칭이 약 두 배 정도 더 큰 영향력을 갖도록 한 것입니다. 또한 ES 8.x에서 활성화할 수 있는 대체 결합 방법으로 **Reciprocal Rank Fusion (RRF)**도 지원하지만, 기본값은 가중치 `script_score` 방식입니다.

**결과 형식:** 하이브리드 스코어 적용 후 top K 히트(예: top 10)를 반환합니다. 각 히트에는 chunk의 content(또는 snippet), 점수, 메타데이터가 포함됩니다. 예를 들어 API는 snippet(내용 앞부분 약 150자 + “…”)과 신뢰도 점수(0–100%로 스케일된 값일 수 있음)를 `device_name`, `chapter` 같은 필드와 함께 반환하여 사용자가 결과를 식별하기 쉽게 합니다. (현재 구현은 이러한 결과를 반환하는 `/search` API를 제공하며, 답변 생성을 위한 LLM과의 완전한 통합은 향후 단계로 계획되어 있습니다.)

---

## 문서 인제스천 및 Chunk 저장

**Chunking 전략:** 시스템은 섹션 기반 chunking(“Section = Chunk”) 접근을 사용합니다. 고정 크기 chunk 대신 문서의 논리적 섹션에 의존합니다.

* **PDF 매뉴얼(예: SOP, 셋업 가이드):** VLM(예: Qwen-VL-30B)이 PDF를 파싱하여 섹션 단위(페이지 또는 헤딩 기준일 가능성 높음)로 내용을 추출합니다. 각 섹션은 Elasticsearch에서 하나의 chunk가 됩니다. 장비명(`device_name`), 간단한 문서 설명, 섹션 제목(`chapter`) 같은 메타데이터를(종종 첫 페이지나 섹션 헤딩에서) 추출해 chunk와 함께 저장합니다.

* **텍스트 기반 정비 로그:** 커스텀 파서(`parse_maintenance_txt`)가 로그를 `Status`, `Action taken`, `Cause`, `Result` 같은 사전 정의된 섹션으로 분리합니다. 각 구간은 chunk(문서 단위)로 취급됩니다. 이 구조화 파싱은 보통 로그 당 4개의 chunk를 생성하며, 각 chunk는 트러블슈팅 기록의 서로 다른 측면을 나타냅니다.

**LLM 생성 요약 및 키워드:** chunking 후 파이프라인은 LLM으로 각 chunk에 대한 간결한 요약(`chunk_summary`)을 만들고 중요한 키워드를 추출합니다. 정비 로그의 경우 프롬프트 템플릿(예: `maintenance_summary_v1`)이 이슈 요약과(부품명/에러 타입 같은) 핵심 용어 생성을 유도합니다. 이 결과는 chunk의 메타데이터로 저장됩니다(요약은 `chunk_summary` 필드, 키워드는 `tags` 리스트 또는 유사 필드). PDF 섹션의 경우에도 설정에 따라 요약이 생성될 수 있습니다(코드에는 chunk-level 요약을 위한 `enable_summarization` 옵션이 있으나, PDF 인제스천에서는 기본적으로 꺼져 있을 수 있습니다).

**문서 스키마 예시:** ES에 저장되는 각 chunk는 코드의 dataclass `EsChunkDocument`에 대응합니다. 주요 필드는 다음과 같습니다.

* `doc_id` – chunk를 원본 문서에 연결하는 식별자(예: 같은 매뉴얼의 모든 chunk는 같은 `doc_id`를 공유).
* `chunk_id` – chunk의 고유 ID(보통 `doc_id#0001` 같은 형식).
* `page` – 해당 chunk의 페이지 번호(또는 시작 페이지) (해당되는 경우).
* `content` – chunk의 원본 텍스트.
* `search_text` – 검색용 결합 텍스트(`title + content + tags`).
* `embedding` – 의미 기반 검색을 위한 chunk 텍스트의 벡터 임베딩.
* `device_name`, `doc_type`, `chapter`, `chunk_summary`(요약), `tags`(키워드) 등 메타데이터 필드(있는 경우).
* chunk의 타임스탬프와 `content_hash`도 저장됩니다. `content_hash`는 중복 chunk를 감지하여 동일한 내용이 두 번 인덱싱되는 것을 막기 위해 사용하는 텍스트의 짧은 해시입니다.

**중복 제거 및 버저닝:** 인제스천 과정에서 각 chunk의 content에 대한 해시를 실시간으로 계산합니다. 이는 중복 chunk 인덱싱을 건너뛰는 데 사용할 수 있습니다(예: 문서의 여러 버전에 같은 섹션이 포함되거나, 인제스천이 재시도되는 경우). 인덱스 버저닝은 앞서 언급한 alias 메커니즘을 통해 보장됩니다. 즉, 다운타임 없이 새 인덱스(예: `..._v2`)에 데이터를 인덱싱한 뒤 alias를 전환해 라이브로 만들 수 있고, 이는 지식 베이스에 대한 일종의 버전 관리/롤백 수단을 제공합니다.

---

## 질의 전처리 및 추가 기능

**질의 전처리:** 질의 임베딩 생성 전에 사용자 질의는 정규화 전처리기(normalization preprocessor)를 거칩니다(L3 normalization 레벨로 설정됨). 이는 아마 소문자화(lowercasing), 불필요한 구두점/공백 제거, 그리고 임베딩 모델 입력을 위한 기본 토큰 정규화 정도를 포함할 것입니다. 이 단계에서 형태소 분석이나 맞춤법 교정 같은 고급 NLP 처리는 적용된 증거가 없으며, 비교적 단순한 정규화 파이프라인으로 보입니다.

**멀티 쿼리 확장(Multi-query Expansion):** 시스템에는 LLM을 사용한 질의 확장 기능이 준비되어 있으나, 현재 설정상 비활성화되어 있습니다. 활성화되면 LLM이 사용자 질문을 여러 관련 질의로 재표현/확장합니다(예: 다른 표현이나 다른 측면을 담은 2~3개의 변형 질의 생성). 각 질의로 개별 검색을 수행한 뒤 결과를 병합하여 recall을 높일 수 있습니다. 이는 사용자 질의가 짧거나 모호할 때 특히 도움이 됩니다(LLM이 동의어나 관련 용어를 추가하여 더 많은 문서를 포착). 하지만 현재는 꺼져 있으므로 원본 질의만 사용합니다.

**결과 재정렬(Re-ranking):** 마찬가지로 cross-encoder 리랭커 모델(예: MiniLM 또는 MS MARCO 기반 cross-encoder)을 이용해 상위 결과를 재정렬하는 기능도 지원하지만, 기본적으로 꺼져 있습니다. 켜면 하이브리드 검색으로 상위 10개를 가져온 뒤, cross-encoder(질의-문서 관련성에 파인튜닝된 BERT 계열 모델)가 각 chunk를 질의와 함께 재평가하여 새로운 관련성 점수를 부여하고, 정말 관련 있는 chunk가 상단에 오도록 정렬하여 precision을 높일 수 있습니다. 현재는 이 단계 없이 ES 점수를 그대로 반환합니다.

**한국어 처리:** 한국어 전용 언어 처리(linguistic processing)가 활성화되어 있지 않다는 점이 두드러집니다. Nori analyzer는 사용 가능하지만 인덱싱에 사용되지 않으며, 질의 파싱에 사용하는 언급도 없습니다. 따라서 질의는 평문으로 처리됩니다. 이는 한국어의 형태 변화(어미/조사 등)가 정규화되지 않기 때문에 recall에 악영향을 줄 수 있습니다.

---

## 리트리벌 이후 단계 및 (계획된) LLM 통합

**검색 결과 전달:** 현재 구현은 리트리벌 결과를 반환하는 검색 API 엔드포인트를 제공합니다. 응답에는 일반적으로 chunk 히트 목록(snippet, 메타데이터, 점수 포함)이 들어 있습니다. 예시로 결과는 다음처럼 보일 수 있습니다:
“Device: AlphaEtcher3000, Section: 5.2 Maintenance Procedure – Snippet: ...the chamber was disassembled and the O-ring replaced...”.
점수는 퍼센트 또는 사용자 친화적인 신뢰도 지표로 변환될 수도 있습니다.

**LLM 프롬프트 구성(향후):** 제공된 코드에서는 아직 구현되어 있지 않지만, 의도는 상위 k개의 리트리브된 chunk를 LLM(“agent”)에 입력하여 사용자의 질문에 대한 최종 답을 생성하는 것입니다. 이때 베스트 프랙티스로는 매우 유사한 chunk들을 중복 제거(dedup)하고, 필요하면 클러스터링하거나 다양한 정보를 선택하여, 관련 컨텍스트를 포함한 프롬프트를 구성하는 것이 포함됩니다. 문서 제목이나 섹션 헤딩 같은 메타데이터를 프롬프트에 포함해 LLM이 각 chunk의 출처 맥락을 이해하도록 도울 수 있습니다. 민감정보가 있다면 필요 시 마스킹할 수 있습니다. 또한 top-k를 적절히 선택하는 것이 중요합니다(너무 크면 프롬프트 오버플로우 위험). 예를 들어 많은 RAG 세팅에서는 최종 답변 생성에 3~5개 chunk를 사용하곤 하지만, 이는 chunk 크기와 LLM 컨텍스트 윈도우에 따라 달라집니다.

**페이지네이션/UI:** 프런트엔드는 필요 시 검색 결과를 페이지네이션합니다. 백엔드는 기본적으로 한 질의에 대해 상위 10개(또는 설정 가능한 `top_k`) 결과를 가져오고, UI가 5개씩 보여주면 클라이언트 측에서 해당 결과를 슬라이싱합니다. 이는 검색 API가 QA agent 용도뿐 아니라 엔지니어가 지식 베이스를 직접 탐색하는 인터랙티브 사용도 고려해 설계되었음을 시사합니다.

---

## 식별된 이슈 및 개선 제안

현재 아키텍처를 바탕으로 한 잠재 이슈 진단 및 개선 제안은 다음과 같습니다.

1. **한국어 텍스트 분석기 사용:** 현재 검색은 `standard analyzer`를 사용하고 있는데, 이는 한국어에 최적이 아닙니다. 한국어는 교착어이므로 recall이 떨어질 수 있습니다. 예를 들어 질의 “장비를 가동했다”와 문서 내 “장비 가동”은 적절한 토크나이징이 없으면 매칭이 잘 되지 않을 수 있습니다. 한국어 텍스트 필드에 Nori analyzer를 활성화하면 형태소 토크나이징과 스테밍(예: “장착했다” → “장착”)이 가능해져 매칭이 개선됩니다. 이를 위해 인덱스 설정에서 Nori analyzer 주석을 해제/활성화하고, 질의와 문서가 모두 한국어 형태소 기반으로 처리되도록 리인덱싱해야 합니다. Nori를 사용할 경우, 반도체 장비 도메인 용어에 특화된 동의어 또는 기술 용어 사전이 있다면 이를 함께 적용해 서로 다른 전문 용어/약어를 정규화할 수도 있습니다.

2. **임베딩 차원 일관성 보장:** 임베딩 차원 설정에 불일치 가능성이 있습니다. 코드의 기본 매핑은 벡터 차원을 1024(다국어 E5 같은 모델에 적합)로 기대하지만, 환경에서는 768을 사용할 수 있습니다(예: BGE 또는 더 작은 모델). 인제스천 파이프라인에는 생성된 임베딩 차원이 인덱스 매핑과 다르면 에러를 발생시키는 체크가 있습니다. 문제를 피하려면 인덱스 생성 시 `dims` 파라미터가 올바른지 재확인하세요. 임베딩 모델을 1024→768로 변경했다면, 인덱스 매핑을 그에 맞게 업데이트하거나(보통 새 인덱스 재생성이 필요) 인덱스 매니저를 통해 새 차원으로 인덱스를 다시 만들어야 합니다. `SEARCH_ES_EMBEDDING_DIMS` 환경변수와 매핑을 동기화하는 것이 핵심입니다.

3. **Recall 향상을 위한 Multi-Query Expansion(MQE) 활성화:** 현재 비활성화된 multi-query 접근은 복잡하거나 모호한 질의에서 도움이 될 수 있습니다. MQE를 켜면 LLM이 질의를 다양한 방식(동의어/관련 용어 포함)으로 재구성하여 recall을 높입니다. 예를 들어 “RFID 센서 오류”는 “RFID 센서 에러”, “RFID 리더기 문제” 등으로 확장될 수 있어, 서로 다른 용어를 쓰는 문서도 잡을 수 있습니다. 활성화 시 여러 번의 검색 및 LLM 호출로 인해 지연과 비용이 늘어나므로, recall이 특히 중요한 경우에만 쓰거나, 모호한 질의로 감지된 경우에만 트리거하는 것이 좋습니다. 소수(예: 2~3개 확장)부터 시작해, 실제로 top 결과에 더 관련 문서가 들어오는지 평가하세요.

4. **정밀도(Precision)를 위한 Cross-Encoder Re-ranking 적용:** 인덱스에 콘텐츠가 많아질수록, 초기 하이브리드 랭킹이 항상 가장 관련 있는 chunk를 1위에 올리지 못할 수 있습니다(특히 dense와 sparse 신호가 충돌할 때). 설정된 `cross-encoder/ms-marco-MiniLM-L-6-v2` 같은 cross-encoder 리랭커를 상위 N개 결과에 적용하면 정밀도가 크게 개선될 수 있습니다. cross-encoder는 각 후보 chunk와 질의를 함께 받아 더 깊은 의미 이해 기반으로 관련성 점수를 산출합니다(예: 키워드는 모두 맞지만 문맥상 관련 없는 chunk를 걸러냄). LLM에 넘기기 전에 상위 5~10개만 재랭킹해도 최종 답변 정확도가 개선될 수 있습니다. 다만 모델 추론 단계가 추가되므로 성능 영향(지연/비용)을 모니터링해야 합니다.

5. **필드 Boost 튜닝 및 인덱싱 필드 최적화:** 현재는 결합 필드 `search_text` 중심으로 검색합니다. 만약 쿼리에 여러 필드를 포함하고 싶다면(예: content vs summary vs keywords), 각 필드의 중요도에 맞게 boost를 튜닝해야 합니다. 예를 들어 수동으로 잘 큐레이션된 키워드(tags)를 신뢰한다면 raw content보다 더 크게 boost할 수 있습니다. 반면 LLM 생성 요약(`chunk_summary`)은 동의어나 간결한 설명으로 도움이 될 수 있지만 노이즈가 있을 수 있어 낮은 weight가 적절할 수 있습니다.
   중요: 검색 쿼리에 포함하는 모든 필드는 실제로 **인덱싱되어 있어야** 합니다. 스냅샷 기준으로는 `chunk_summary`/`doc_description`가 `text`로 검색 가능하므로, “인덱싱을 위해서”는 reindex가 필수는 아닙니다. 다만 분석기 변경(Nori), 멀티필드 추가, 필드 타입 정리(예: text→keyword 분리) 같은 변경은 신규 인덱스(vNext) + 재색인으로 접근하는 것이 안전합니다. 전형적인 질의에 대해 다양한 boost/필드 조합으로 A/B 테스트를 수행해 최적 구성을 찾으세요.

6. **쿼리에 메타데이터 필터 적극 활용:** 각 chunk에 `device_name`, `doc_type` 등의 풍부한 메타데이터가 있습니다. 이를 검색 UI의 필터로 노출하거나 쿼리 파라미터로 제공하세요. 예를 들어 엔지니어가 특정 장비 모델 또는 로그 타입으로 좁히고 싶다면, 해당 정보를 기반으로 `term` 필터를 추가해 그 범위 내에서만 검색하도록 할 수 있습니다. 이는 관련 없는 결과(예: 다른 장비의 트러블슈팅)를 제거해 precision을 크게 향상시킵니다. 프런트엔드 또는 API가 이러한 필터를 지정할 수 있게 하세요. 사용자 입력이 없어도, 질의에 특정 장비/문서 타입이 언급되면(“[장비명] 오류…”) 자동으로 메타데이터 필터를 적용할 수도 있습니다. 이를 위해 장비명 별칭 매핑을 유지하거나, 콘텐츠/메타데이터에서 장비명을 일관되게 관리할 필요가 있습니다.

7. **동적 하이브리드 가중치(Dynamic Hybrid Weighting):** dense와 sparse 스코어의 최적 비율은 질의에 따라 달라질 수 있습니다. 예를 들어 매우 짧은 키워드 질의(“EFEM 알람 코드 1234”)는 특정 코드/용어가 중요하므로 BM25 비중(`sparse_weight > 0.3`)이 더 유리할 수 있습니다. 반대로 긴 자연어 질문(“왜 챔버 온도가 설정값에 도달하지 못할까요?”)은 의미 매칭 비중(`dense_weight` 증가)이 더 좋을 수 있습니다. 질의 길이/희귀 토큰 존재 여부 등 질의 특성에 따라 `dense_weight`와 `sparse_weight`를 조정하는 간단한 휴리스틱을 구현해보세요. 예:

* 질의가 2단어 이하이거나 특정 에러 코드/ID처럼 보이면 BM25 쪽으로(예: `dense_weight 0.4`, `sparse_weight 0.6`)
* 질의가 문장형 또는 복잡 질문이면 반대로(예: `dense_weight 0.8`, `sparse_weight 0.2`)

또는 분류기를 통해 동적으로 학습하거나, 사용자에게 “fuzzy vs exact” 검색 토글을 제공할 수도 있습니다. Elasticsearch 8에서는 **RRF**를 사용할 수도 있는데, 이는 수동 가중치 튜닝 없이도 두 모달리티의 결과를 균형있게 섞어, 단순 가중합에서 놓치는 문서를 끌어올릴 수 있습니다.

8. **인덱스 업데이트 및 유지보수:** 문서를 더 많이 인제스트하거나 문서가 업데이트될 때, stale 데이터가 서비스되지 않도록 alias 버저닝 전략을 준수하세요. 설명된 프로세스(버전 인덱스 생성 후 alias 전환)는 좋은 프랙티스입니다. 또한 `pipeline_version`과 인덱스의 `_meta`(예: `get_index_meta`로 세팅)가 있다면 어떤 설정으로 인덱스를 만들었는지 추적하는 데 활용하세요. 이는 향후 임베딩 모델/청킹 방식 등이 바뀔 때 문제 진단에 도움이 됩니다. 제공된 스크립트(예: `es_query.py stats`)를 정기적으로 실행해 인덱스 상태와 문서 수를 점검하면, 인제스천 실패로 chunk 수가 비정상적으로 낮아지는 같은 문제를 조기에 발견할 수 있습니다.

---

위 사항들을 적용하면 RAG 시스템의 리트리벌 구성요소가 더 견고하고 효과적으로 동작할 것입니다. 요약하면, 핵심 다음 단계는 **한국어 텍스트 분석 활성화**, **(multi-query 확장, 리랭킹, 필드/가중치 튜닝을 통한) 검색 전략 정교화**, 그리고 **메타데이터를 활용한 타게팅 검색**입니다. 이는 더 관련 있는 트러블슈팅 근거 chunk를 리트리브하게 만들고, 결과적으로 LLM이 엔지니어에게 더 정확하고 유용한 가이드를 제공하도록 도울 것입니다.
